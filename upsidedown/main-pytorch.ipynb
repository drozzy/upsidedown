{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from experiment import rollout_random, ReplayBuffer, Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-1. Initialize replay buffer with warm-up episodes using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 22.1\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(500, 100)\n",
    "avg_reward = rollout_random(num_episodes=500, env=env, replay_buffer=rb, render=False)\n",
    "\n",
    "print(f\"Average Episode Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-2 Initialize a behavior function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate\n",
    "    linearly for the first ``warmup_steps`` training steps, and decreasing it thereafter proportionally\n",
    "    to the inverse square root of the step number, scaled by the inverse square root of the\n",
    "    dimensionality of the model. Time will tell if this is just madness or it's actually important.\n",
    "    Parameters\n",
    "    ----------\n",
    "    warmup_steps: ``int``, required.\n",
    "        The number of steps to linearly increase the learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        last_epoch = max(1, self.last_epoch)\n",
    "        scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n",
    "        return [base_lr * scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.fc3 = nn.Linear(512,512)\n",
    "        self.fc4 = nn.Linear(512,512)\n",
    "        self.fc5 = nn.Linear(512,num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.relu(self.fc1(x))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = F.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "d = env.observation_space.shape[0]\n",
    "model = Behavior(input_shape=d+2, num_actions=1).to(device) # env.action_space.n\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "#lr_scheduler = NoamLR(optimizer, 5000)\n",
    "\n",
    "loss_object = torch.nn.BCEWithLogitsLoss().to(device) #CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A1-3: while stopping criteria is not reached do:\n",
    "### A1-4:   Improve the behavior function by training on replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sum = 0\n",
    "loss_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_training(s, dr, dh):\n",
    "    l = s.tolist()\n",
    "    l.append(dr)\n",
    "    l.append(dh)\n",
    "    return l\n",
    "\n",
    "def segments_to_training(segments):\n",
    "    x = []\n",
    "    y = []\n",
    "    for (s, dr, dh), action in segments:\n",
    "        l = to_training(s, dr, dh)\n",
    "        x.append(l)\n",
    "        y.append(action)\n",
    "        \n",
    "    x = torch.tensor(x).float().to(device)\n",
    "    y = torch.tensor(y).float().to(device)\n",
    "    \n",
    "    return x, y\n",
    "        \n",
    "# accuracy_m = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    #print(predictions, targets)\n",
    "    \n",
    "    loss = loss_object(predictions, targets.unsqueeze(1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def generate_episode(cmd):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    \n",
    "    t = Trajectory()\n",
    "    while not done:\n",
    "        (dh, dr) = cmd\n",
    "        inputs = torch.tensor([to_training(s, dr, dh)]).float().to(device)\n",
    "        \n",
    "        action_probs = model(inputs)\n",
    "        action_probs = F.sigmoid(action_probs) #, dim=-1)\n",
    "        \n",
    "        m = torch.distributions.bernoulli.Bernoulli(probs=action_probs) #categorical.Categorical(probs=action_probs)\n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "        \n",
    "        # env.render()\n",
    "        s_old = s\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        t.add(s_old, action, reward, s)\n",
    "        \n",
    "        ep_reward += reward\n",
    "        dh = dh - 1\n",
    "        dr = dr - reward\n",
    "        cmd = (dh, dr)\n",
    "    \n",
    "    # print(f'Episode reward: {ep_reward}')\n",
    "    rb.add(t)\n",
    "    return ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 12.84\n",
      "Average Episode Reward: 42.36\n",
      "i: 200, Loss: 0.6799904704093933\n",
      "Average Episode Reward: 41.29\n",
      "Average Episode Reward: 27.37\n",
      "i: 400, Loss: 0.6696462631225586\n",
      "Average Episode Reward: 27.85\n",
      "Average Episode Reward: 38.56\n",
      "i: 600, Loss: 0.6629749536514282\n",
      "Average Episode Reward: 49.75\n",
      "Average Episode Reward: 49.94\n",
      "i: 800, Loss: 0.6577674150466919\n",
      "Average Episode Reward: 66.81\n",
      "Average Episode Reward: 36.6\n",
      "i: 1000, Loss: 0.6537543535232544\n",
      "Average Episode Reward: 38.87\n",
      "Average Episode Reward: 60.49\n",
      "i: 1200, Loss: 0.6502493023872375\n",
      "Average Episode Reward: 49.2\n",
      "Average Episode Reward: 70.0\n",
      "i: 1400, Loss: 0.6467254757881165\n",
      "Average Episode Reward: 69.01\n",
      "Average Episode Reward: 70.05\n",
      "i: 1600, Loss: 0.6434688568115234\n",
      "Average Episode Reward: 73.83\n",
      "Average Episode Reward: 65.46\n",
      "i: 1800, Loss: 0.6404635310173035\n",
      "Average Episode Reward: 64.6\n",
      "Average Episode Reward: 76.73\n",
      "i: 2000, Loss: 0.6378771066665649\n",
      "Average Episode Reward: 49.24\n",
      "Average Episode Reward: 66.57\n",
      "i: 2200, Loss: 0.6356172561645508\n",
      "Average Episode Reward: 46.67\n",
      "Average Episode Reward: 83.4\n",
      "i: 2400, Loss: 0.6338470578193665\n",
      "Average Episode Reward: 80.98\n",
      "Average Episode Reward: 59.31\n",
      "i: 2600, Loss: 0.6321471333503723\n",
      "Average Episode Reward: 82.66\n",
      "Average Episode Reward: 54.05\n",
      "i: 2800, Loss: 0.6306791305541992\n",
      "Average Episode Reward: 81.21\n",
      "Average Episode Reward: 67.39\n",
      "i: 3000, Loss: 0.6293849349021912\n",
      "Average Episode Reward: 109.17\n",
      "Average Episode Reward: 86.41\n",
      "i: 3200, Loss: 0.6282123923301697\n",
      "Average Episode Reward: 70.15\n",
      "Average Episode Reward: 43.91\n",
      "i: 3400, Loss: 0.6271167397499084\n",
      "Average Episode Reward: 75.7\n",
      "Average Episode Reward: 109.81\n",
      "i: 3600, Loss: 0.6260021924972534\n",
      "Average Episode Reward: 90.96\n",
      "Average Episode Reward: 127.95\n",
      "i: 3800, Loss: 0.6250572204589844\n",
      "Average Episode Reward: 81.97\n",
      "Average Episode Reward: 56.42\n",
      "i: 4000, Loss: 0.6242003440856934\n",
      "Average Episode Reward: 60.55\n",
      "Average Episode Reward: 47.14\n",
      "i: 4200, Loss: 0.6233367323875427\n",
      "Average Episode Reward: 33.41\n",
      "Average Episode Reward: 38.38\n",
      "i: 4400, Loss: 0.6225429773330688\n",
      "Average Episode Reward: 102.24\n",
      "Average Episode Reward: 84.87\n",
      "i: 4600, Loss: 0.6219058632850647\n",
      "Average Episode Reward: 56.07\n",
      "Average Episode Reward: 32.97\n",
      "i: 4800, Loss: 0.6212677955627441\n",
      "Average Episode Reward: 84.05\n",
      "Average Episode Reward: 91.68\n",
      "i: 5000, Loss: 0.6207320690155029\n",
      "Average Episode Reward: 24.46\n",
      "Average Episode Reward: 93.74\n",
      "i: 5200, Loss: 0.6201292872428894\n",
      "Average Episode Reward: 107.07\n",
      "Average Episode Reward: 118.28\n",
      "i: 5400, Loss: 0.6195638179779053\n",
      "Average Episode Reward: 127.96\n",
      "Average Episode Reward: 39.75\n",
      "i: 5600, Loss: 0.6190419793128967\n",
      "Average Episode Reward: 117.31\n",
      "Average Episode Reward: 107.68\n",
      "i: 5800, Loss: 0.618584156036377\n",
      "Average Episode Reward: 82.92\n",
      "Average Episode Reward: 129.89\n",
      "i: 6000, Loss: 0.6180800795555115\n",
      "Average Episode Reward: 90.34\n",
      "Average Episode Reward: 133.7\n",
      "i: 6200, Loss: 0.6177107095718384\n",
      "Average Episode Reward: 116.1\n",
      "Average Episode Reward: 127.65\n",
      "i: 6400, Loss: 0.6173389554023743\n",
      "Average Episode Reward: 88.53\n",
      "Average Episode Reward: 51.51\n",
      "i: 6600, Loss: 0.6170526146888733\n",
      "Average Episode Reward: 135.39\n",
      "Average Episode Reward: 145.76\n",
      "i: 6800, Loss: 0.6167171597480774\n",
      "Average Episode Reward: 80.24\n",
      "Average Episode Reward: 125.37\n",
      "i: 7000, Loss: 0.6163755655288696\n",
      "Average Episode Reward: 155.55\n",
      "Average Episode Reward: 54.35\n",
      "i: 7200, Loss: 0.6161217093467712\n",
      "Average Episode Reward: 135.72\n",
      "Average Episode Reward: 163.11\n",
      "i: 7400, Loss: 0.6157991886138916\n",
      "Average Episode Reward: 129.11\n",
      "Average Episode Reward: 167.48\n",
      "i: 7600, Loss: 0.6154996752738953\n",
      "Average Episode Reward: 95.51\n",
      "Average Episode Reward: 119.94\n",
      "i: 7800, Loss: 0.6152696013450623\n",
      "Average Episode Reward: 56.13\n",
      "Average Episode Reward: 122.25\n",
      "i: 8000, Loss: 0.6150084137916565\n",
      "Average Episode Reward: 148.74\n",
      "Average Episode Reward: 141.48\n",
      "i: 8200, Loss: 0.6147284507751465\n",
      "Average Episode Reward: 147.47\n",
      "Average Episode Reward: 102.43\n",
      "i: 8400, Loss: 0.6144684553146362\n",
      "Average Episode Reward: 109.78\n",
      "Average Episode Reward: 189.83\n",
      "i: 8600, Loss: 0.6142095923423767\n",
      "Average Episode Reward: 160.78\n",
      "Average Episode Reward: 135.86\n",
      "i: 8800, Loss: 0.6139858365058899\n",
      "Average Episode Reward: 221.59\n",
      "Average Episode Reward: 41.91\n",
      "i: 9000, Loss: 0.6137581467628479\n",
      "Average Episode Reward: 201.45\n",
      "Average Episode Reward: 159.37\n",
      "i: 9200, Loss: 0.6135555505752563\n",
      "Average Episode Reward: 145.15\n",
      "Average Episode Reward: 172.38\n",
      "i: 9400, Loss: 0.6133314371109009\n",
      "Average Episode Reward: 148.88\n",
      "Average Episode Reward: 186.18\n",
      "i: 9600, Loss: 0.6131520867347717\n",
      "Average Episode Reward: 90.89\n",
      "Average Episode Reward: 224.96\n",
      "i: 9800, Loss: 0.6129494905471802\n",
      "Average Episode Reward: 214.0\n",
      "Average Episode Reward: 144.32\n",
      "i: 10000, Loss: 0.6127974390983582\n",
      "Average Episode Reward: 142.03\n",
      "Average Episode Reward: 204.95\n",
      "i: 10200, Loss: 0.6126329898834229\n",
      "Average Episode Reward: 198.66\n",
      "Average Episode Reward: 198.26\n",
      "i: 10400, Loss: 0.6124700307846069\n",
      "Average Episode Reward: 146.17\n",
      "Average Episode Reward: 107.26\n",
      "i: 10600, Loss: 0.6123086810112\n",
      "Average Episode Reward: 186.3\n",
      "Average Episode Reward: 162.45\n",
      "i: 10800, Loss: 0.6121517419815063\n",
      "Average Episode Reward: 184.24\n",
      "Average Episode Reward: 188.15\n",
      "i: 11000, Loss: 0.611961305141449\n",
      "Average Episode Reward: 203.73\n",
      "Average Episode Reward: 212.87\n",
      "i: 11200, Loss: 0.6118003129959106\n",
      "Average Episode Reward: 116.56\n",
      "Average Episode Reward: 102.78\n",
      "i: 11400, Loss: 0.611642599105835\n",
      "Average Episode Reward: 214.32\n",
      "Average Episode Reward: 206.57\n",
      "i: 11600, Loss: 0.6114838719367981\n",
      "Average Episode Reward: 132.0\n",
      "Average Episode Reward: 192.27\n",
      "i: 11800, Loss: 0.6113326549530029\n",
      "Average Episode Reward: 77.59\n",
      "Average Episode Reward: 241.45\n",
      "i: 12000, Loss: 0.6112172603607178\n",
      "Average Episode Reward: 140.61\n",
      "Average Episode Reward: 165.5\n",
      "i: 12200, Loss: 0.611090898513794\n",
      "Average Episode Reward: 107.54\n",
      "Average Episode Reward: 203.62\n",
      "i: 12400, Loss: 0.6109820604324341\n",
      "Average Episode Reward: 171.82\n",
      "Average Episode Reward: 101.28\n",
      "i: 12600, Loss: 0.6108363270759583\n",
      "Average Episode Reward: 169.41\n",
      "Average Episode Reward: 51.56\n",
      "i: 12800, Loss: 0.610745370388031\n",
      "Average Episode Reward: 138.46\n",
      "Average Episode Reward: 209.93\n",
      "i: 13000, Loss: 0.6106477379798889\n",
      "Average Episode Reward: 169.69\n",
      "Average Episode Reward: 232.01\n",
      "i: 13200, Loss: 0.6105251908302307\n",
      "Average Episode Reward: 186.26\n",
      "Average Episode Reward: 229.97\n",
      "i: 13400, Loss: 0.6104333996772766\n",
      "Average Episode Reward: 176.1\n",
      "Average Episode Reward: 248.88\n",
      "i: 13600, Loss: 0.6103386282920837\n",
      "Average Episode Reward: 177.15\n",
      "Average Episode Reward: 208.94\n",
      "i: 13800, Loss: 0.6102477312088013\n",
      "Average Episode Reward: 179.25\n",
      "Average Episode Reward: 177.84\n",
      "i: 14000, Loss: 0.6101738810539246\n",
      "Average Episode Reward: 106.41\n",
      "Average Episode Reward: 176.69\n",
      "i: 14200, Loss: 0.610106348991394\n",
      "Average Episode Reward: 162.73\n",
      "Average Episode Reward: 203.34\n",
      "i: 14400, Loss: 0.6100413799285889\n",
      "Average Episode Reward: 223.64\n",
      "Average Episode Reward: 240.61\n",
      "i: 14600, Loss: 0.609953761100769\n",
      "Average Episode Reward: 206.83\n",
      "Average Episode Reward: 228.11\n",
      "i: 14800, Loss: 0.6098839044570923\n",
      "Average Episode Reward: 121.22\n",
      "Average Episode Reward: 218.54\n",
      "i: 15000, Loss: 0.6098076105117798\n",
      "Average Episode Reward: 210.38\n",
      "Average Episode Reward: 266.51\n",
      "i: 15200, Loss: 0.6097471714019775\n",
      "Average Episode Reward: 196.26\n",
      "Average Episode Reward: 271.74\n",
      "i: 15400, Loss: 0.6096877455711365\n",
      "Average Episode Reward: 299.69\n",
      "Average Episode Reward: 236.08\n",
      "i: 15600, Loss: 0.6096132397651672\n",
      "Average Episode Reward: 158.39\n",
      "Average Episode Reward: 243.62\n",
      "i: 15800, Loss: 0.6095600128173828\n",
      "Average Episode Reward: 270.54\n",
      "Average Episode Reward: 268.87\n",
      "i: 16000, Loss: 0.609493613243103\n",
      "Average Episode Reward: 123.69\n",
      "Average Episode Reward: 237.92\n",
      "i: 16200, Loss: 0.6094234585762024\n",
      "Average Episode Reward: 253.83\n",
      "Average Episode Reward: 84.23\n",
      "i: 16400, Loss: 0.6093981266021729\n",
      "Average Episode Reward: 175.77\n",
      "Average Episode Reward: 117.15\n",
      "i: 16600, Loss: 0.6093523502349854\n",
      "Average Episode Reward: 174.87\n",
      "Average Episode Reward: 144.07\n",
      "i: 16800, Loss: 0.6093125939369202\n",
      "Average Episode Reward: 116.53\n",
      "Average Episode Reward: 192.49\n",
      "i: 17000, Loss: 0.6092551946640015\n",
      "Average Episode Reward: 117.43\n",
      "Average Episode Reward: 340.83\n",
      "i: 17200, Loss: 0.6092150211334229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 111.55\n",
      "Average Episode Reward: 304.21\n",
      "i: 17400, Loss: 0.6091704964637756\n",
      "Average Episode Reward: 278.23\n",
      "Average Episode Reward: 255.31\n",
      "i: 17600, Loss: 0.6091257333755493\n",
      "Average Episode Reward: 142.03\n",
      "Average Episode Reward: 243.95\n",
      "i: 17800, Loss: 0.6090912818908691\n",
      "Average Episode Reward: 280.91\n",
      "Average Episode Reward: 222.87\n",
      "i: 18000, Loss: 0.6090420484542847\n",
      "Average Episode Reward: 289.27\n",
      "Average Episode Reward: 220.54\n",
      "i: 18200, Loss: 0.6090273261070251\n",
      "Average Episode Reward: 119.34\n",
      "Average Episode Reward: 232.89\n",
      "i: 18400, Loss: 0.6090124249458313\n",
      "Average Episode Reward: 213.58\n",
      "Average Episode Reward: 319.23\n",
      "i: 18600, Loss: 0.6089856624603271\n",
      "Average Episode Reward: 170.62\n",
      "Average Episode Reward: 124.15\n",
      "i: 18800, Loss: 0.6089608073234558\n",
      "Average Episode Reward: 166.42\n",
      "Average Episode Reward: 268.75\n",
      "i: 19000, Loss: 0.6089322566986084\n",
      "Average Episode Reward: 123.06\n",
      "Average Episode Reward: 202.59\n",
      "i: 19200, Loss: 0.6089104413986206\n",
      "Average Episode Reward: 233.29\n",
      "Average Episode Reward: 300.31\n",
      "i: 19400, Loss: 0.6088809967041016\n",
      "Average Episode Reward: 265.44\n",
      "Average Episode Reward: 293.79\n",
      "i: 19600, Loss: 0.6088601350784302\n",
      "Average Episode Reward: 323.96\n",
      "Average Episode Reward: 286.25\n",
      "i: 19800, Loss: 0.6088429689407349\n",
      "Average Episode Reward: 342.47\n",
      "Average Episode Reward: 276.62\n",
      "i: 20000, Loss: 0.6088303923606873\n",
      "Average Episode Reward: 268.0\n",
      "Average Episode Reward: 276.48\n",
      "i: 20200, Loss: 0.6087857484817505\n",
      "Average Episode Reward: 178.12\n",
      "Average Episode Reward: 298.7\n",
      "i: 20400, Loss: 0.6087610721588135\n",
      "Average Episode Reward: 325.63\n",
      "Average Episode Reward: 264.28\n",
      "i: 20600, Loss: 0.6087307929992676\n",
      "Average Episode Reward: 144.65\n",
      "Average Episode Reward: 291.04\n",
      "i: 20800, Loss: 0.6087185740470886\n",
      "Average Episode Reward: 293.78\n",
      "Average Episode Reward: 252.92\n",
      "i: 21000, Loss: 0.6086928844451904\n",
      "Average Episode Reward: 282.29\n",
      "Average Episode Reward: 238.85\n",
      "i: 21200, Loss: 0.6086679697036743\n",
      "Average Episode Reward: 150.5\n",
      "Average Episode Reward: 256.18\n",
      "i: 21400, Loss: 0.6086446046829224\n",
      "Average Episode Reward: 273.39\n",
      "Average Episode Reward: 331.32\n",
      "i: 21600, Loss: 0.6086335778236389\n",
      "Average Episode Reward: 116.36\n",
      "Average Episode Reward: 171.61\n",
      "i: 21800, Loss: 0.6086232662200928\n",
      "Average Episode Reward: 273.85\n",
      "Average Episode Reward: 178.77\n",
      "i: 22000, Loss: 0.6086246967315674\n",
      "Average Episode Reward: 284.06\n",
      "Average Episode Reward: 242.5\n",
      "i: 22200, Loss: 0.6085858941078186\n",
      "Average Episode Reward: 280.95\n",
      "Average Episode Reward: 268.64\n",
      "i: 22400, Loss: 0.6085636615753174\n",
      "Average Episode Reward: 353.01\n",
      "Average Episode Reward: 200.33\n",
      "i: 22600, Loss: 0.6085317134857178\n",
      "Average Episode Reward: 285.75\n",
      "Average Episode Reward: 298.36\n",
      "i: 22800, Loss: 0.6085037589073181\n",
      "Average Episode Reward: 302.77\n",
      "Average Episode Reward: 313.7\n",
      "i: 23000, Loss: 0.6084956526756287\n",
      "Average Episode Reward: 120.3\n",
      "Average Episode Reward: 271.03\n",
      "i: 23200, Loss: 0.6084794998168945\n",
      "Average Episode Reward: 244.82\n",
      "Average Episode Reward: 156.57\n",
      "i: 23400, Loss: 0.6084542870521545\n",
      "Average Episode Reward: 238.32\n",
      "Average Episode Reward: 225.15\n",
      "i: 23600, Loss: 0.6084355711936951\n",
      "Average Episode Reward: 164.34\n",
      "Average Episode Reward: 264.65\n",
      "i: 23800, Loss: 0.6084089279174805\n",
      "Average Episode Reward: 117.65\n",
      "Average Episode Reward: 304.52\n",
      "i: 24000, Loss: 0.6083961129188538\n",
      "Average Episode Reward: 141.68\n",
      "Average Episode Reward: 227.98\n",
      "i: 24200, Loss: 0.6083811521530151\n",
      "Average Episode Reward: 289.19\n",
      "Average Episode Reward: 314.8\n",
      "i: 24400, Loss: 0.6083696484565735\n",
      "Average Episode Reward: 268.49\n",
      "Average Episode Reward: 265.85\n",
      "i: 24600, Loss: 0.6083552837371826\n",
      "Average Episode Reward: 370.9\n",
      "Average Episode Reward: 361.55\n",
      "i: 24800, Loss: 0.6083422303199768\n",
      "Average Episode Reward: 315.48\n",
      "Average Episode Reward: 163.93\n",
      "i: 25000, Loss: 0.6083219647407532\n",
      "Average Episode Reward: 355.81\n",
      "Average Episode Reward: 377.89\n",
      "i: 25200, Loss: 0.6083072423934937\n",
      "Average Episode Reward: 313.9\n",
      "Average Episode Reward: 264.88\n",
      "i: 25400, Loss: 0.6082921028137207\n",
      "Average Episode Reward: 381.02\n",
      "Average Episode Reward: 238.12\n",
      "i: 25600, Loss: 0.6082716584205627\n",
      "Average Episode Reward: 296.42\n",
      "Average Episode Reward: 227.7\n",
      "i: 25800, Loss: 0.6082577705383301\n",
      "Average Episode Reward: 202.33\n",
      "Average Episode Reward: 248.24\n",
      "i: 26000, Loss: 0.6082475185394287\n",
      "Average Episode Reward: 234.02\n",
      "Average Episode Reward: 405.51\n",
      "i: 26200, Loss: 0.6082318425178528\n",
      "Average Episode Reward: 343.81\n",
      "Average Episode Reward: 230.7\n",
      "i: 26400, Loss: 0.608213484287262\n",
      "Average Episode Reward: 271.39\n",
      "Average Episode Reward: 349.13\n",
      "i: 26600, Loss: 0.6082108616828918\n",
      "Average Episode Reward: 304.9\n",
      "Average Episode Reward: 207.59\n",
      "i: 26800, Loss: 0.608187735080719\n",
      "Average Episode Reward: 110.51\n",
      "Average Episode Reward: 281.01\n",
      "i: 27000, Loss: 0.6081647276878357\n",
      "Average Episode Reward: 207.5\n",
      "Average Episode Reward: 237.93\n",
      "i: 27200, Loss: 0.608144223690033\n",
      "Average Episode Reward: 258.76\n",
      "Average Episode Reward: 175.97\n",
      "i: 27400, Loss: 0.6081365346908569\n",
      "Average Episode Reward: 208.92\n",
      "Average Episode Reward: 336.35\n",
      "i: 27600, Loss: 0.6081281304359436\n",
      "Average Episode Reward: 349.27\n",
      "Average Episode Reward: 301.27\n",
      "i: 27800, Loss: 0.6081181764602661\n",
      "Average Episode Reward: 309.24\n",
      "Average Episode Reward: 304.2\n",
      "i: 28000, Loss: 0.6081061363220215\n",
      "Average Episode Reward: 345.21\n",
      "Average Episode Reward: 84.05\n",
      "i: 28200, Loss: 0.608089029788971\n",
      "Average Episode Reward: 319.96\n",
      "Average Episode Reward: 246.6\n",
      "i: 28400, Loss: 0.6080687046051025\n",
      "Average Episode Reward: 266.11\n",
      "Average Episode Reward: 309.85\n",
      "i: 28600, Loss: 0.6080539226531982\n",
      "Average Episode Reward: 262.64\n",
      "Average Episode Reward: 187.74\n",
      "i: 28800, Loss: 0.6080499291419983\n",
      "Average Episode Reward: 273.04\n",
      "Average Episode Reward: 205.5\n",
      "i: 29000, Loss: 0.6080396175384521\n",
      "Average Episode Reward: 322.23\n",
      "Average Episode Reward: 301.77\n",
      "i: 29200, Loss: 0.6080245971679688\n",
      "Average Episode Reward: 250.65\n",
      "Average Episode Reward: 306.79\n",
      "i: 29400, Loss: 0.6080109477043152\n",
      "Average Episode Reward: 132.9\n",
      "Average Episode Reward: 277.44\n",
      "i: 29600, Loss: 0.6080016493797302\n",
      "Average Episode Reward: 150.29\n",
      "Average Episode Reward: 286.93\n",
      "i: 29800, Loss: 0.6079906225204468\n",
      "Average Episode Reward: 181.8\n",
      "Average Episode Reward: 314.02\n",
      "i: 30000, Loss: 0.6079772114753723\n",
      "Average Episode Reward: 338.48\n",
      "Average Episode Reward: 295.51\n",
      "i: 30200, Loss: 0.6079601645469666\n",
      "Average Episode Reward: 120.23\n",
      "Average Episode Reward: 221.66\n",
      "i: 30400, Loss: 0.6079546809196472\n",
      "Average Episode Reward: 296.99\n",
      "Average Episode Reward: 212.42\n",
      "i: 30600, Loss: 0.6079464554786682\n",
      "Average Episode Reward: 86.69\n",
      "Average Episode Reward: 178.12\n",
      "i: 30800, Loss: 0.6079370379447937\n",
      "Average Episode Reward: 108.96\n",
      "Average Episode Reward: 268.27\n",
      "i: 31000, Loss: 0.607928991317749\n",
      "Average Episode Reward: 255.54\n",
      "Average Episode Reward: 324.89\n",
      "i: 31200, Loss: 0.6079208254814148\n",
      "Average Episode Reward: 248.12\n",
      "Average Episode Reward: 289.65\n",
      "i: 31400, Loss: 0.6079075932502747\n",
      "Average Episode Reward: 291.93\n",
      "Average Episode Reward: 215.81\n",
      "i: 31600, Loss: 0.607893705368042\n",
      "Average Episode Reward: 300.85\n",
      "Average Episode Reward: 290.19\n",
      "i: 31800, Loss: 0.6078818440437317\n",
      "Average Episode Reward: 321.8\n",
      "Average Episode Reward: 427.7\n",
      "i: 32000, Loss: 0.6078612804412842\n",
      "Average Episode Reward: 157.93\n",
      "Average Episode Reward: 300.32\n",
      "i: 32200, Loss: 0.6078404188156128\n",
      "Average Episode Reward: 152.16\n",
      "Average Episode Reward: 208.94\n",
      "i: 32400, Loss: 0.6078295707702637\n",
      "Average Episode Reward: 327.33\n",
      "Average Episode Reward: 190.02\n",
      "i: 32600, Loss: 0.60782390832901\n",
      "Average Episode Reward: 95.94\n",
      "Average Episode Reward: 279.41\n",
      "i: 32800, Loss: 0.6078099608421326\n",
      "Average Episode Reward: 95.93\n",
      "Average Episode Reward: 286.39\n",
      "i: 33000, Loss: 0.6078002452850342\n",
      "Average Episode Reward: 301.07\n",
      "Average Episode Reward: 229.41\n",
      "i: 33200, Loss: 0.6077818274497986\n",
      "Average Episode Reward: 205.53\n",
      "Average Episode Reward: 345.77\n",
      "i: 33400, Loss: 0.6077675223350525\n",
      "Average Episode Reward: 263.47\n",
      "Average Episode Reward: 159.0\n",
      "i: 33600, Loss: 0.6077631711959839\n",
      "Average Episode Reward: 151.05\n",
      "Average Episode Reward: 169.43\n",
      "i: 33800, Loss: 0.6077560186386108\n",
      "Average Episode Reward: 366.24\n",
      "Average Episode Reward: 244.34\n",
      "i: 34000, Loss: 0.6077390909194946\n",
      "Average Episode Reward: 270.69\n",
      "Average Episode Reward: 228.58\n",
      "i: 34200, Loss: 0.6077336072921753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 175.74\n",
      "Average Episode Reward: 234.69\n",
      "i: 34400, Loss: 0.6077231764793396\n",
      "Average Episode Reward: 82.99\n",
      "Average Episode Reward: 210.21\n",
      "i: 34600, Loss: 0.6077201962471008\n",
      "Average Episode Reward: 326.8\n",
      "Average Episode Reward: 222.86\n",
      "i: 34800, Loss: 0.6077219247817993\n",
      "Average Episode Reward: 302.57\n",
      "Average Episode Reward: 248.81\n",
      "i: 35000, Loss: 0.6077096462249756\n",
      "Average Episode Reward: 306.71\n",
      "Average Episode Reward: 194.31\n",
      "i: 35200, Loss: 0.6076945662498474\n",
      "Average Episode Reward: 245.97\n",
      "Average Episode Reward: 262.98\n",
      "i: 35400, Loss: 0.6076852679252625\n",
      "Average Episode Reward: 324.76\n",
      "Average Episode Reward: 278.44\n",
      "i: 35600, Loss: 0.6076691150665283\n",
      "Average Episode Reward: 295.2\n",
      "Average Episode Reward: 237.51\n",
      "i: 35800, Loss: 0.607656717300415\n",
      "Average Episode Reward: 314.27\n",
      "Average Episode Reward: 312.83\n",
      "i: 36000, Loss: 0.6076412796974182\n",
      "Average Episode Reward: 322.2\n",
      "Average Episode Reward: 270.88\n",
      "i: 36200, Loss: 0.6076250076293945\n",
      "Average Episode Reward: 257.23\n",
      "Average Episode Reward: 325.18\n",
      "i: 36400, Loss: 0.6076164841651917\n",
      "Average Episode Reward: 193.34\n",
      "Average Episode Reward: 307.41\n",
      "i: 36600, Loss: 0.6075994372367859\n",
      "Average Episode Reward: 271.43\n",
      "Average Episode Reward: 243.12\n",
      "i: 36800, Loss: 0.6075853109359741\n",
      "Average Episode Reward: 290.53\n",
      "Average Episode Reward: 307.94\n",
      "i: 37000, Loss: 0.6075760722160339\n",
      "Average Episode Reward: 211.54\n",
      "Average Episode Reward: 294.98\n",
      "i: 37200, Loss: 0.6075698137283325\n",
      "Average Episode Reward: 162.86\n",
      "Average Episode Reward: 262.69\n",
      "i: 37400, Loss: 0.6075581908226013\n",
      "Average Episode Reward: 221.15\n",
      "Average Episode Reward: 341.66\n",
      "i: 37600, Loss: 0.60755455493927\n",
      "Average Episode Reward: 282.98\n",
      "Average Episode Reward: 226.99\n",
      "i: 37800, Loss: 0.6075425148010254\n",
      "Average Episode Reward: 173.19\n",
      "Average Episode Reward: 300.09\n",
      "i: 38000, Loss: 0.6075358986854553\n",
      "Average Episode Reward: 354.09\n",
      "Average Episode Reward: 146.75\n",
      "i: 38200, Loss: 0.6075211763381958\n",
      "Average Episode Reward: 305.69\n",
      "Average Episode Reward: 216.01\n",
      "i: 38400, Loss: 0.6075105667114258\n",
      "Average Episode Reward: 276.64\n",
      "Average Episode Reward: 267.63\n",
      "i: 38600, Loss: 0.607498824596405\n",
      "Average Episode Reward: 378.94\n",
      "Average Episode Reward: 303.93\n",
      "i: 38800, Loss: 0.6074846982955933\n",
      "Average Episode Reward: 283.34\n",
      "Average Episode Reward: 254.91\n",
      "i: 39000, Loss: 0.6074745059013367\n",
      "Average Episode Reward: 352.61\n",
      "Average Episode Reward: 186.84\n",
      "i: 39200, Loss: 0.6074638962745667\n",
      "Average Episode Reward: 314.34\n",
      "Average Episode Reward: 278.31\n",
      "i: 39400, Loss: 0.6074496507644653\n",
      "Average Episode Reward: 248.52\n",
      "Average Episode Reward: 273.91\n",
      "i: 39600, Loss: 0.6074325442314148\n",
      "Average Episode Reward: 291.45\n",
      "Average Episode Reward: 189.74\n",
      "i: 39800, Loss: 0.6074219346046448\n",
      "Average Episode Reward: 329.67\n",
      "Average Episode Reward: 286.66\n",
      "i: 40000, Loss: 0.6074082255363464\n",
      "Average Episode Reward: 219.21\n",
      "Average Episode Reward: 252.73\n",
      "i: 40200, Loss: 0.6073968410491943\n",
      "Average Episode Reward: 312.86\n",
      "Average Episode Reward: 288.34\n",
      "i: 40400, Loss: 0.6073797941207886\n",
      "Average Episode Reward: 249.14\n",
      "Average Episode Reward: 104.75\n",
      "i: 40600, Loss: 0.6073710322380066\n",
      "Average Episode Reward: 210.08\n",
      "Average Episode Reward: 297.84\n",
      "i: 40800, Loss: 0.6073560118675232\n",
      "Average Episode Reward: 240.94\n",
      "Average Episode Reward: 202.74\n",
      "i: 41000, Loss: 0.607347846031189\n",
      "Average Episode Reward: 181.68\n",
      "Average Episode Reward: 347.77\n",
      "i: 41200, Loss: 0.6073318123817444\n",
      "Average Episode Reward: 305.23\n",
      "Average Episode Reward: 277.61\n",
      "i: 41400, Loss: 0.6073198318481445\n",
      "Average Episode Reward: 278.79\n",
      "Average Episode Reward: 338.51\n",
      "i: 41600, Loss: 0.6073185205459595\n",
      "Average Episode Reward: 199.12\n",
      "Average Episode Reward: 203.77\n",
      "i: 41800, Loss: 0.607306957244873\n",
      "Average Episode Reward: 333.62\n",
      "Average Episode Reward: 283.11\n",
      "i: 42000, Loss: 0.607292890548706\n",
      "Average Episode Reward: 277.59\n",
      "Average Episode Reward: 339.97\n",
      "i: 42200, Loss: 0.6072794198989868\n",
      "Average Episode Reward: 249.55\n",
      "Average Episode Reward: 160.55\n",
      "i: 42400, Loss: 0.607265830039978\n",
      "Average Episode Reward: 261.93\n",
      "Average Episode Reward: 245.74\n",
      "i: 42600, Loss: 0.6072621941566467\n",
      "Average Episode Reward: 349.64\n",
      "Average Episode Reward: 239.39\n",
      "i: 42800, Loss: 0.6072489023208618\n",
      "Average Episode Reward: 335.78\n",
      "Average Episode Reward: 249.88\n",
      "i: 43000, Loss: 0.6072379946708679\n",
      "Average Episode Reward: 222.15\n",
      "Average Episode Reward: 367.25\n",
      "i: 43200, Loss: 0.6072239875793457\n",
      "Average Episode Reward: 209.74\n",
      "Average Episode Reward: 301.17\n",
      "i: 43400, Loss: 0.6072073578834534\n",
      "Average Episode Reward: 346.33\n",
      "Average Episode Reward: 264.65\n",
      "i: 43600, Loss: 0.6071969866752625\n",
      "Average Episode Reward: 290.66\n",
      "Average Episode Reward: 146.66\n",
      "i: 43800, Loss: 0.6071869730949402\n",
      "Average Episode Reward: 315.75\n",
      "Average Episode Reward: 295.05\n",
      "i: 44000, Loss: 0.607180655002594\n",
      "Average Episode Reward: 290.35\n",
      "Average Episode Reward: 207.57\n",
      "i: 44200, Loss: 0.6071661114692688\n",
      "Average Episode Reward: 239.19\n",
      "Average Episode Reward: 251.18\n",
      "i: 44400, Loss: 0.6071552634239197\n",
      "Average Episode Reward: 261.84\n",
      "Average Episode Reward: 278.84\n",
      "i: 44600, Loss: 0.6071518063545227\n",
      "Average Episode Reward: 249.61\n",
      "Average Episode Reward: 259.67\n",
      "i: 44800, Loss: 0.6071451902389526\n",
      "Average Episode Reward: 367.45\n",
      "Average Episode Reward: 272.57\n",
      "i: 45000, Loss: 0.6071364879608154\n",
      "Average Episode Reward: 318.22\n",
      "Average Episode Reward: 295.85\n",
      "i: 45200, Loss: 0.6071206331253052\n",
      "Average Episode Reward: 342.47\n",
      "Average Episode Reward: 375.35\n",
      "i: 45400, Loss: 0.6071104407310486\n",
      "Average Episode Reward: 311.57\n",
      "Average Episode Reward: 293.07\n",
      "i: 45600, Loss: 0.6070946455001831\n",
      "Average Episode Reward: 349.98\n",
      "Average Episode Reward: 227.28\n",
      "i: 45800, Loss: 0.6070793271064758\n",
      "Average Episode Reward: 254.55\n",
      "Average Episode Reward: 353.68\n",
      "i: 46000, Loss: 0.6070730090141296\n",
      "Average Episode Reward: 276.75\n",
      "Average Episode Reward: 311.57\n",
      "i: 46200, Loss: 0.6070588827133179\n",
      "Average Episode Reward: 358.4\n",
      "Average Episode Reward: 237.13\n",
      "i: 46400, Loss: 0.6070431470870972\n",
      "Average Episode Reward: 209.94\n",
      "Average Episode Reward: 97.59\n",
      "i: 46600, Loss: 0.6070315837860107\n",
      "Average Episode Reward: 322.3\n",
      "Average Episode Reward: 284.15\n",
      "i: 46800, Loss: 0.6070238351821899\n",
      "Average Episode Reward: 286.68\n",
      "Average Episode Reward: 124.46\n",
      "i: 47000, Loss: 0.6070169806480408\n",
      "Average Episode Reward: 286.77\n",
      "Average Episode Reward: 242.13\n",
      "i: 47200, Loss: 0.6070066094398499\n",
      "Average Episode Reward: 244.87\n",
      "Average Episode Reward: 345.1\n",
      "i: 47400, Loss: 0.6070062518119812\n",
      "Average Episode Reward: 335.31\n",
      "Average Episode Reward: 154.48\n",
      "i: 47600, Loss: 0.6069945096969604\n",
      "Average Episode Reward: 194.53\n",
      "Average Episode Reward: 381.63\n",
      "i: 47800, Loss: 0.6069871187210083\n",
      "Average Episode Reward: 339.21\n",
      "Average Episode Reward: 310.4\n",
      "i: 48000, Loss: 0.6069793105125427\n",
      "Average Episode Reward: 231.0\n",
      "Average Episode Reward: 190.65\n",
      "i: 48200, Loss: 0.6069722175598145\n",
      "Average Episode Reward: 299.99\n",
      "Average Episode Reward: 232.65\n",
      "i: 48400, Loss: 0.6069651246070862\n",
      "Average Episode Reward: 325.9\n",
      "Average Episode Reward: 192.19\n",
      "i: 48600, Loss: 0.6069630980491638\n",
      "Average Episode Reward: 315.21\n",
      "Average Episode Reward: 179.79\n",
      "i: 48800, Loss: 0.6069538593292236\n",
      "Average Episode Reward: 323.52\n",
      "Average Episode Reward: 303.21\n",
      "i: 49000, Loss: 0.6069499850273132\n",
      "Average Episode Reward: 181.47\n",
      "Average Episode Reward: 233.9\n",
      "i: 49200, Loss: 0.6069454550743103\n",
      "Average Episode Reward: 282.57\n",
      "Average Episode Reward: 334.58\n",
      "i: 49400, Loss: 0.6069368124008179\n",
      "Average Episode Reward: 158.29\n",
      "Average Episode Reward: 388.82\n",
      "i: 49600, Loss: 0.6069353222846985\n",
      "Average Episode Reward: 309.35\n",
      "Average Episode Reward: 321.31\n",
      "i: 49800, Loss: 0.6069322824478149\n",
      "Average Episode Reward: 387.27\n",
      "Average Episode Reward: 132.1\n",
      "i: 50000, Loss: 0.6069217920303345\n",
      "Average Episode Reward: 322.93\n",
      "Average Episode Reward: 340.36\n",
      "i: 50200, Loss: 0.6069178581237793\n",
      "Average Episode Reward: 317.41\n",
      "Average Episode Reward: 351.36\n",
      "i: 50400, Loss: 0.6069071888923645\n",
      "Average Episode Reward: 296.84\n",
      "Average Episode Reward: 343.61\n",
      "i: 50600, Loss: 0.606900155544281\n",
      "Average Episode Reward: 179.38\n",
      "Average Episode Reward: 221.18\n",
      "i: 50800, Loss: 0.6068909764289856\n",
      "Average Episode Reward: 372.11\n",
      "Average Episode Reward: 202.74\n",
      "i: 51000, Loss: 0.6068887114524841\n",
      "Average Episode Reward: 307.7\n",
      "Average Episode Reward: 305.78\n",
      "i: 51200, Loss: 0.6068816781044006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 251.57\n",
      "Average Episode Reward: 368.25\n",
      "i: 51400, Loss: 0.6068745255470276\n",
      "Average Episode Reward: 252.52\n",
      "Average Episode Reward: 275.9\n",
      "i: 51600, Loss: 0.6068617701530457\n",
      "Average Episode Reward: 203.01\n",
      "Average Episode Reward: 279.7\n",
      "i: 51800, Loss: 0.6068536043167114\n",
      "Average Episode Reward: 424.9\n",
      "Average Episode Reward: 293.9\n",
      "i: 52000, Loss: 0.6068450808525085\n",
      "Average Episode Reward: 329.67\n",
      "Average Episode Reward: 241.4\n",
      "i: 52200, Loss: 0.6068375110626221\n",
      "Average Episode Reward: 208.53\n",
      "Average Episode Reward: 360.07\n",
      "i: 52400, Loss: 0.6068310141563416\n",
      "Average Episode Reward: 266.32\n",
      "Average Episode Reward: 287.48\n",
      "i: 52600, Loss: 0.6068215370178223\n",
      "Average Episode Reward: 343.44\n",
      "Average Episode Reward: 252.05\n",
      "i: 52800, Loss: 0.6068099141120911\n",
      "Average Episode Reward: 293.24\n",
      "Average Episode Reward: 280.11\n",
      "i: 53000, Loss: 0.6067960262298584\n",
      "Average Episode Reward: 296.07\n",
      "Average Episode Reward: 262.5\n",
      "i: 53200, Loss: 0.6067923307418823\n",
      "Average Episode Reward: 380.88\n",
      "Average Episode Reward: 345.75\n",
      "i: 53400, Loss: 0.6067822575569153\n",
      "Average Episode Reward: 259.75\n",
      "Average Episode Reward: 276.98\n",
      "i: 53600, Loss: 0.6067790985107422\n",
      "Average Episode Reward: 374.57\n",
      "Average Episode Reward: 302.06\n",
      "i: 53800, Loss: 0.6067659854888916\n",
      "Average Episode Reward: 341.07\n",
      "Average Episode Reward: 373.94\n",
      "i: 54000, Loss: 0.6067547798156738\n",
      "Average Episode Reward: 336.79\n",
      "Average Episode Reward: 249.5\n",
      "i: 54200, Loss: 0.6067531704902649\n",
      "Average Episode Reward: 291.32\n",
      "Average Episode Reward: 313.0\n",
      "i: 54400, Loss: 0.6067472100257874\n",
      "Average Episode Reward: 352.93\n",
      "Average Episode Reward: 232.05\n",
      "i: 54600, Loss: 0.606745183467865\n",
      "Average Episode Reward: 289.91\n",
      "Average Episode Reward: 188.6\n",
      "i: 54800, Loss: 0.6067327857017517\n",
      "Average Episode Reward: 266.61\n",
      "Average Episode Reward: 360.19\n",
      "i: 55000, Loss: 0.6067289710044861\n",
      "Average Episode Reward: 311.11\n",
      "Average Episode Reward: 313.49\n",
      "i: 55200, Loss: 0.6067134141921997\n",
      "Average Episode Reward: 285.91\n",
      "Average Episode Reward: 286.19\n",
      "i: 55400, Loss: 0.6067069172859192\n",
      "Average Episode Reward: 346.53\n",
      "Average Episode Reward: 218.12\n",
      "i: 55600, Loss: 0.6066960692405701\n",
      "Average Episode Reward: 255.86\n",
      "Average Episode Reward: 291.26\n",
      "i: 55800, Loss: 0.6066877841949463\n",
      "Average Episode Reward: 237.67\n",
      "Average Episode Reward: 165.51\n",
      "i: 56000, Loss: 0.6066851019859314\n",
      "Average Episode Reward: 251.37\n",
      "Average Episode Reward: 254.34\n",
      "i: 56200, Loss: 0.6066823601722717\n",
      "Average Episode Reward: 316.9\n",
      "Average Episode Reward: 327.01\n",
      "i: 56400, Loss: 0.6066793203353882\n",
      "Average Episode Reward: 236.66\n",
      "Average Episode Reward: 348.37\n",
      "i: 56600, Loss: 0.6066768169403076\n",
      "Average Episode Reward: 244.61\n",
      "Average Episode Reward: 380.2\n",
      "i: 56800, Loss: 0.606666624546051\n",
      "Average Episode Reward: 287.77\n",
      "Average Episode Reward: 201.58\n",
      "i: 57000, Loss: 0.6066562533378601\n",
      "Average Episode Reward: 339.92\n",
      "Average Episode Reward: 314.32\n",
      "i: 57200, Loss: 0.6066474914550781\n",
      "Average Episode Reward: 252.22\n",
      "Average Episode Reward: 343.05\n",
      "i: 57400, Loss: 0.6066356301307678\n",
      "Average Episode Reward: 282.33\n",
      "Average Episode Reward: 312.16\n",
      "i: 57600, Loss: 0.6066293716430664\n",
      "Average Episode Reward: 229.57\n",
      "Average Episode Reward: 328.21\n",
      "i: 57800, Loss: 0.6066218614578247\n",
      "Average Episode Reward: 313.24\n",
      "Average Episode Reward: 170.41\n",
      "i: 58000, Loss: 0.606614351272583\n",
      "Average Episode Reward: 258.71\n",
      "Average Episode Reward: 308.66\n",
      "i: 58200, Loss: 0.6066098213195801\n",
      "Average Episode Reward: 200.2\n",
      "Average Episode Reward: 182.68\n",
      "i: 58400, Loss: 0.606606125831604\n",
      "Average Episode Reward: 329.62\n",
      "Average Episode Reward: 355.33\n",
      "i: 58600, Loss: 0.606593668460846\n",
      "Average Episode Reward: 225.93\n",
      "Average Episode Reward: 191.67\n",
      "i: 58800, Loss: 0.6065804362297058\n",
      "Average Episode Reward: 307.45\n",
      "Average Episode Reward: 271.02\n",
      "i: 59000, Loss: 0.6065755486488342\n",
      "Average Episode Reward: 277.26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8fcb26693e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Episode Reward: {np.mean(rewards)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-789d7355ebd2>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#categorical.Categorical(probs=action_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt13/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \"\"\"\n\u001b[1;32m   1351\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000000\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    segments = rb.sample(batch_size)\n",
    "    segments = np.array(segments)\n",
    "    x, y = segments_to_training(segments)\n",
    "    loss = train_step(x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    #if i % 1000 == 0:\n",
    "    #lr_scheduler.step()\n",
    "    #print(lr_scheduler.get_lr())\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        rewards = [] \n",
    "        for e in range(100):\n",
    "            cmd = rb.sample_command()\n",
    "            rewards.append(generate_episode(cmd))\n",
    "        \n",
    "        print(f\"Average Episode Reward: {np.mean(rewards)}\")\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(f'i: {i}, Loss: {loss_sum/loss_count}') #'\\t Accuracy: {accuracy_m.result()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 500.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A1:6 Generate episodes using Alg 2 and add to replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (500, 500) #rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 32.0\n",
      "Episode reward: 500.0\n",
      "Episode reward: 16.0\n",
      "Episode reward: 378.0\n",
      "Episode reward: 255.0\n",
      "Episode reward: 56.0\n",
      "Episode reward: 500.0\n",
      "Episode reward: 91.0\n",
      "Episode reward: 90.0\n",
      "Episode reward: 47.0\n",
      "Episode reward: 500.0\n",
      "Episode reward: 153.0\n",
      "Episode reward: 226.0\n",
      "Episode reward: 37.0\n",
      "Episode reward: 91.0\n",
      "Episode reward: 500.0\n",
      "Episode reward: 41.0\n",
      "Episode reward: 500.0\n",
      "Episode reward: 296.0\n",
      "Episode reward: 500.0\n",
      "Average Episode Reward: 240.45\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "avg_rewards = []\n",
    "import time \n",
    "\n",
    "def test(cmd):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        (dh, dr) = cmd\n",
    "        inputs = torch.tensor([to_training(s, dr, dh)]).float().to(device)\n",
    "        \n",
    "        action_probs = model(inputs)\n",
    "        action_probs = torch.sigmoid(action_probs) #, dim=-1)\n",
    "        \n",
    "        m = torch.distributions.bernoulli.Bernoulli(probs=action_probs) #torch.distributions.categorical.Categorical(probs=action_probs)\n",
    "        #action = int(torch.round(action_probs).detach().squeeze().cpu().numpy())\n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "        \n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "        s_old = s\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        \n",
    "        ep_reward += reward\n",
    "        dh = dh - 1\n",
    "        dr = dr - reward\n",
    "        cmd = (dh, dr)\n",
    "    \n",
    "    print(f'Episode reward: {ep_reward}')\n",
    "    return ep_reward\n",
    "\n",
    "rewards = [] \n",
    "for e in range(20):\n",
    "    rewards.append(test(cmd))\n",
    "\n",
    "env.close()\n",
    "print(f\"Average Episode Reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
