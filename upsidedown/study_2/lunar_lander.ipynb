{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from itertools import count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 32\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# class Behavior(torch.nn.Module):\n",
    "#     def __init__(self, input_shape, num_actions):\n",
    "#         super(Behavior, self).__init__()\n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(input_shape, HIDDEN), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, HIDDEN),\n",
    "#             nn.ReLU(),            \n",
    "#             nn.Linear(HIDDEN, HIDDEN), \n",
    "#             nn.ReLU(),            \n",
    "#             nn.Linear(HIDDEN, num_actions)\n",
    "#         )        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.classifier(x)\n",
    "\n",
    "\n",
    "class Behavior(nn.Module):\n",
    "    def __init__(self, state_shape, cmd_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc_state = nn.Linear(state_shape,HIDDEN)\n",
    "        self.fc_cmd = nn.Linear(cmd_shape,HIDDEN)\n",
    "        \n",
    "        self.fc1 = nn.Linear(HIDDEN,HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN,num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_spate = self.fc_state(x[0])\n",
    "        output_cmd = torch.sigmoid(self.fc_cmd(x[1]))\n",
    "        \n",
    "        output = output_spate * output_cmd\n",
    "        \n",
    "        output = torch.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(input_shape, HIDDEN), \n",
    "# #             nn.Dropout(0.1),\n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, HIDDEN),\n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "#             nn.ReLU(),            \n",
    "# #             nn.Linear(HIDDEN, HIDDEN), \n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "# #             nn.ReLU(),            \n",
    "# #             nn.Linear(HIDDEN, HIDDEN), \n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "# #             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, num_actions)\n",
    "#         )        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(state_shape=env.observation_space.shape[0], cmd_shape=2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=50, last_few=50)\n",
    "\n",
    "n_warmup_episodes = 30\n",
    "# Random rollout\n",
    "trajectories, mean_reward, length = rollout(episodes=n_warmup_episodes, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "\n",
    "# Plot initial values\n",
    "writer.add_scalar('Steps/reward', mean_reward, steps)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model([inputs[:, :-2], inputs[:, -2:]])\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action, epsilon):\n",
    "    action_logits = model([inputs[:, :-2], inputs[:, -2:]])\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if random.random() < epsilon: # Random action\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 7950, steps 183797 with loss: 1.1867789030075073\n",
      "183797\n",
      "Average Episode Reward: -126.80167492436576\n",
      "Average Episode Reward: -138.8016162009057\n",
      "i: 8000, s: 186864, Loss: 1.089489459991455\n",
      "Average Episode Reward: -113.61700519753927\n",
      "Average Episode Reward: -97.97472374535371\n",
      "Average Episode Reward: -59.4700936034032\n",
      "Average Episode Reward: -152.55199852132515\n",
      "i: 8200, s: 195005, Loss: 1.0847513675689697\n",
      "Average Episode Reward: -94.85693066370214\n",
      "Average Episode Reward: -148.20512978051391\n",
      "Average Episode Reward: -142.56741812042114\n",
      "Average Episode Reward: -123.03359636150851\n",
      "i: 8400, s: 202200, Loss: 1.0807380676269531\n",
      "Average Episode Reward: -86.51822589946589\n",
      "Average Episode Reward: -140.3833830914096\n",
      "Average Episode Reward: -114.6457021306021\n",
      "Average Episode Reward: -105.3505705885005\n",
      "i: 8600, s: 210493, Loss: 1.0775346755981445\n",
      "Average Episode Reward: -101.77017538437215\n",
      "Average Episode Reward: -141.83507439152976\n",
      "Average Episode Reward: -85.0649608082546\n",
      "Average Episode Reward: -77.21998300472887\n",
      "i: 8800, s: 216943, Loss: 1.0747991800308228\n",
      "Average Episode Reward: -121.55420546099474\n",
      "Average Episode Reward: -121.76515513913539\n",
      "Average Episode Reward: -97.4087264128799\n",
      "Average Episode Reward: -122.3105438932095\n",
      "i: 9000, s: 224560, Loss: 1.0734245777130127\n",
      "Average Episode Reward: -59.513265228538685\n",
      "Average Episode Reward: -135.86566371225666\n",
      "Average Episode Reward: -184.4487588689587\n",
      "Average Episode Reward: -100.28016176583007\n",
      "i: 9200, s: 231381, Loss: 1.0716125965118408\n",
      "Average Episode Reward: -146.1367968923545\n",
      "Average Episode Reward: -88.23208857296746\n",
      "Average Episode Reward: -102.34573420865206\n",
      "Average Episode Reward: -102.66023513379453\n",
      "i: 9400, s: 239204, Loss: 1.0700700283050537\n",
      "Average Episode Reward: -85.0766073468122\n",
      "Average Episode Reward: -48.936853689976985\n",
      "Average Episode Reward: -80.6647148952826\n",
      "Average Episode Reward: -121.53233069549358\n",
      "i: 9600, s: 246367, Loss: 1.0672717094421387\n",
      "Average Episode Reward: -153.37131947068383\n",
      "Average Episode Reward: -135.0678119033975\n",
      "Average Episode Reward: -139.44860423818136\n",
      "Average Episode Reward: -124.61516900661407\n",
      "i: 9800, s: 254305, Loss: 1.064867615699768\n",
      "Average Episode Reward: -100.46081955371857\n",
      "Average Episode Reward: -115.40081441053876\n",
      "Average Episode Reward: -183.65247847628177\n",
      "Average Episode Reward: -146.03972008970703\n",
      "i: 10000, s: 261382, Loss: 1.0627955198287964\n",
      "Average Episode Reward: -78.28149504540126\n",
      "Average Episode Reward: -161.28892616478396\n",
      "Average Episode Reward: -134.4794440083159\n",
      "Average Episode Reward: -96.47194089394279\n",
      "i: 10200, s: 270208, Loss: 1.0609674453735352\n",
      "Average Episode Reward: -106.57851390989524\n",
      "Average Episode Reward: -93.93948331315127\n",
      "Average Episode Reward: -91.31916335810746\n",
      "Average Episode Reward: -119.74477260033436\n",
      "i: 10400, s: 277932, Loss: 1.0589492321014404\n",
      "Average Episode Reward: -103.55765484392175\n",
      "Average Episode Reward: -154.80421544567395\n",
      "Average Episode Reward: -109.07261881223076\n",
      "Average Episode Reward: -187.68861415449615\n",
      "i: 10600, s: 285022, Loss: 1.0569055080413818\n",
      "Average Episode Reward: -107.46660682039082\n",
      "Average Episode Reward: -89.22545883844631\n",
      "Average Episode Reward: -148.7357084301322\n",
      "Average Episode Reward: -112.38241533902033\n",
      "i: 10800, s: 295301, Loss: 1.0548129081726074\n",
      "Average Episode Reward: -95.77933257683965\n",
      "Average Episode Reward: -65.54795312687318\n",
      "Average Episode Reward: -75.90680237617057\n",
      "Average Episode Reward: -95.72741502377325\n",
      "i: 11000, s: 301628, Loss: 1.0531994104385376\n",
      "Average Episode Reward: -118.53042660164199\n",
      "Average Episode Reward: -105.13282087006216\n",
      "Average Episode Reward: -183.55728495861013\n",
      "Average Episode Reward: -69.59599513699241\n",
      "i: 11200, s: 309898, Loss: 1.0512498617172241\n",
      "Average Episode Reward: -119.567915095617\n",
      "Average Episode Reward: -186.3808457587308\n",
      "Average Episode Reward: -68.61855590708818\n",
      "Average Episode Reward: -86.94220757231874\n",
      "i: 11400, s: 316655, Loss: 1.0489307641983032\n",
      "Average Episode Reward: -100.99687589705785\n",
      "Average Episode Reward: -107.42036347355513\n",
      "Average Episode Reward: -52.257613971338984\n",
      "Average Episode Reward: -99.24127861061245\n",
      "i: 11600, s: 323421, Loss: 1.04660964012146\n",
      "Average Episode Reward: -62.753833241633075\n",
      "Average Episode Reward: -73.59024626035102\n",
      "Average Episode Reward: -127.95588826221699\n",
      "Average Episode Reward: -36.28122334493997\n",
      "i: 11800, s: 330336, Loss: 1.0440796613693237\n",
      "Average Episode Reward: -45.218285490724\n",
      "Average Episode Reward: -60.14282650101124\n",
      "Average Episode Reward: -49.7410653484975\n",
      "Average Episode Reward: -73.1983639999589\n",
      "i: 12000, s: 337114, Loss: 1.04154634475708\n",
      "Average Episode Reward: -63.47789702399458\n",
      "Average Episode Reward: -124.59315646768235\n",
      "Average Episode Reward: -50.717318790900734\n",
      "Average Episode Reward: -46.786684414460886\n",
      "i: 12200, s: 344759, Loss: 1.0389689207077026\n",
      "Average Episode Reward: -81.31162036803548\n",
      "Average Episode Reward: -103.25808566004704\n",
      "Average Episode Reward: -34.81524162485499\n",
      "Average Episode Reward: -62.13614430399018\n",
      "i: 12400, s: 351713, Loss: 1.0362000465393066\n",
      "Average Episode Reward: -132.9569798705968\n",
      "Average Episode Reward: -68.1439483434295\n",
      "Average Episode Reward: -13.875983292778859\n",
      "Average Episode Reward: -98.46045620193985\n",
      "i: 12600, s: 359050, Loss: 1.033396601676941\n",
      "Average Episode Reward: -168.99252897329498\n",
      "Average Episode Reward: -111.0953302524822\n",
      "Average Episode Reward: -42.702693206702406\n",
      "Average Episode Reward: -66.37697772892855\n",
      "i: 12800, s: 367541, Loss: 1.0307296514511108\n",
      "Average Episode Reward: -19.070835471813826\n",
      "Average Episode Reward: -89.51088669733409\n",
      "Average Episode Reward: -48.65095634835697\n",
      "Average Episode Reward: -89.34358789194536\n",
      "i: 13000, s: 377465, Loss: 1.0280516147613525\n",
      "Average Episode Reward: -54.19990098884214\n",
      "Average Episode Reward: -107.94240756837527\n",
      "Average Episode Reward: -113.56766074382172\n",
      "Average Episode Reward: -77.54137035949084\n",
      "i: 13200, s: 386993, Loss: 1.0254884958267212\n",
      "Average Episode Reward: -62.531016739053676\n",
      "Average Episode Reward: -157.08404511982795\n",
      "Average Episode Reward: -82.47933802348757\n",
      "Average Episode Reward: -102.39887211182582\n",
      "i: 13400, s: 394256, Loss: 1.0227911472320557\n",
      "Average Episode Reward: -65.34372942186793\n",
      "Average Episode Reward: -67.29060628451145\n",
      "Average Episode Reward: -64.51093778717916\n",
      "Average Episode Reward: -92.63377755144984\n",
      "i: 13600, s: 404507, Loss: 1.0202118158340454\n",
      "Average Episode Reward: -71.9372513429145\n",
      "Average Episode Reward: -58.45153275172381\n",
      "Average Episode Reward: -16.055868041905708\n",
      "Average Episode Reward: 17.167446662362313\n",
      "i: 13800, s: 413366, Loss: 1.0176132917404175\n",
      "Average Episode Reward: -1.0927857425809235\n",
      "Average Episode Reward: -63.990764840400246\n",
      "Average Episode Reward: 36.15003941166189\n",
      "Average Episode Reward: -112.78080765160412\n",
      "i: 14000, s: 424082, Loss: 1.0151281356811523\n",
      "Average Episode Reward: -35.77879909838724\n",
      "Average Episode Reward: -10.404805270829646\n",
      "Average Episode Reward: -78.74895567223783\n",
      "Average Episode Reward: -57.22331603832581\n",
      "i: 14200, s: 434300, Loss: 1.012985110282898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c214cc28b790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mn_updates_per_iter\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n\u001b[1;32m---> 30\u001b[1;33m                               device=device, action_fn=action_fn, epsilon=0.01)\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout\u001b[1;34m(episodes, env, model, sample_action, cmd, render, replay_buffer, device, action_fn, evaluation, epsilon)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         t, reward = rollout_episode(env=env, model=model, sample_action=sample_action, cmd=cmd,\n\u001b[1;32m---> 70\u001b[1;33m                             render=render, device=device, action_fn=action_fn, epsilon=epsilon)            \n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout_episode\u001b[1;34m(env, model, sample_action, cmd, render, device, action_fn, epsilon)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2d2bd7f07656>\u001b[0m in \u001b[0;36maction_fn\u001b[1;34m(model, inputs, sample_action, epsilon)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0maction_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0maction_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a2fbdb5dda70>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0moutput_cmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_cmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_spate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0moutput_cmd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "SOLVED_MEAN_REWARD = 200\n",
    "MAX_STEPS = 10**7\n",
    "rewards = []\n",
    "\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "EVAL_EVERY = 1000\n",
    "\n",
    "for i in count(start=epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/loss', loss, i)\n",
    "    \n",
    "    (dh, dr) = rb.sample_command()\n",
    "    writer.add_scalar('Epoch/dh', dh, i)\n",
    "    writer.add_scalar('Epoch/dr', dr, i)\n",
    "\n",
    "    n_episodes_per_iter = 10\n",
    "    n_updates_per_iter = 50\n",
    "    if i % n_updates_per_iter == 0:\n",
    "        trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn, epsilon=0.01)\n",
    "        rb.add(trajectories)\n",
    "        rewards.append(mean_reward)\n",
    "        rewards = rewards[-50:] # Keep only last  rewards\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Steps/reward', mean_reward, steps)\n",
    "        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Steps/length', mean_length, steps)\n",
    "        \n",
    "        if np.mean(rewards) >= SOLVED_MEAN_REWARD:\n",
    "            print(\"Task considered solved! Stopping.\")\n",
    "            break\n",
    "        \n",
    "        if steps >= MAX_STEPS:\n",
    "            print(f\"Steps {steps} exceeds max env steps {MAX_STEPS}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "    if i % EVAL_EVERY == 0:\n",
    "        eval_episodes = 10\n",
    "        _, mean_reward, length = rollout(eval_episodes, env=env, model=model_sample, \n",
    "                            sample_action=True, replay_buffer=rb, \n",
    "                            device=device, action_fn=action_fn, evaluation=True)\n",
    "        \n",
    "        writer.add_scalar('Eval/reward', mean_reward, i)        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Eval/length', mean_length, i)\n",
    "        \n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94.6, -154.08065847387493)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 5700, steps 107461 with loss: 1.1478462219238281\n",
      "Average Episode Reward: -73.02952937640885\n"
     ]
    }
   ],
   "source": [
    "       # dh ,dr\n",
    "cmd = rb.sample_command()\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l,_ = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward, _ = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn, epsilon=0.0)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
