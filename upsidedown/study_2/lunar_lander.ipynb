{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from itertools import count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 32\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# class Behavior(torch.nn.Module):\n",
    "#     def __init__(self, input_shape, num_actions):\n",
    "#         super(Behavior, self).__init__()\n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(input_shape, HIDDEN), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, HIDDEN),\n",
    "#             nn.ReLU(),            \n",
    "#             nn.Linear(HIDDEN, HIDDEN), \n",
    "#             nn.ReLU(),            \n",
    "#             nn.Linear(HIDDEN, num_actions)\n",
    "#         )        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.classifier(x)\n",
    "\n",
    "\n",
    "class Behavior(nn.Module):\n",
    "    def __init__(self, state_shape, cmd_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc_state = nn.Linear(state_shape,HIDDEN)\n",
    "        self.fc_cmd = nn.Linear(cmd_shape,HIDDEN)\n",
    "        \n",
    "        self.fc1 = nn.Linear(HIDDEN,HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN,num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_spate = self.fc_state(x[0])\n",
    "        output_cmd = torch.sigmoid(self.fc_cmd(x[1]))\n",
    "        \n",
    "        output = output_spate * output_cmd\n",
    "        \n",
    "        output = torch.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(input_shape, HIDDEN), \n",
    "# #             nn.Dropout(0.1),\n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, HIDDEN),\n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "#             nn.ReLU(),            \n",
    "# #             nn.Linear(HIDDEN, HIDDEN), \n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "# #             nn.ReLU(),            \n",
    "# #             nn.Linear(HIDDEN, HIDDEN), \n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "# #             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, num_actions)\n",
    "#         )        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(state_shape=env.observation_space.shape[0], cmd_shape=2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=50, last_few=50)\n",
    "\n",
    "n_warmup_episodes = 30\n",
    "# Random rollout\n",
    "trajectories, mean_reward, length = rollout(episodes=n_warmup_episodes, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "\n",
    "# Plot initial values\n",
    "writer.add_scalar('Steps/reward', mean_reward, steps)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model([inputs[:, :-2], inputs[:, -2:]])\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model([inputs[:, :-2], inputs[:, -2:]])\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 0, steps 2708 with loss: 0.0\n",
      "2708\n",
      "Average Episode Reward: -135.70336424006987\n",
      "i: 0, s: 3590, Loss: 1.3809409141540527\n",
      "Average Episode Reward: -206.48656064840628\n",
      "Average Episode Reward: -128.45597849974465\n",
      "Average Episode Reward: -176.93884917142188\n",
      "Average Episode Reward: -106.53178139909687\n",
      "i: 200, s: 7209, Loss: 1.3806339502334595\n",
      "Average Episode Reward: -122.77196296109064\n",
      "Average Episode Reward: -108.96610735962595\n",
      "Average Episode Reward: -138.83799883588534\n",
      "Average Episode Reward: -112.38006119765642\n",
      "i: 400, s: 11675, Loss: 1.3749736547470093\n",
      "Average Episode Reward: -180.02195700344117\n",
      "Average Episode Reward: -136.91283766876512\n",
      "Average Episode Reward: -117.23518846640154\n",
      "Average Episode Reward: -102.60508300387278\n",
      "i: 600, s: 15490, Loss: 1.3693886995315552\n",
      "Average Episode Reward: -99.33764365497233\n",
      "Average Episode Reward: -110.87165980865134\n",
      "Average Episode Reward: -93.37697881818329\n",
      "Average Episode Reward: -89.24884876904794\n",
      "i: 800, s: 18909, Loss: 1.3639711141586304\n",
      "Average Episode Reward: -124.82926266189904\n",
      "Average Episode Reward: -96.19224349817264\n",
      "Average Episode Reward: -89.33497739887062\n",
      "Average Episode Reward: -100.70238370413871\n",
      "i: 1000, s: 22615, Loss: 1.3589245080947876\n",
      "Average Episode Reward: -82.42771223630872\n",
      "Average Episode Reward: -88.58467177273631\n",
      "Average Episode Reward: -97.33104669209821\n",
      "Average Episode Reward: -117.79364715594997\n",
      "i: 1200, s: 26459, Loss: 1.3539360761642456\n",
      "Average Episode Reward: -128.06704127144354\n",
      "Average Episode Reward: -80.57065479389794\n",
      "Average Episode Reward: -89.73575648224451\n",
      "Average Episode Reward: -103.4177156657446\n",
      "i: 1400, s: 30316, Loss: 1.3478517532348633\n",
      "Average Episode Reward: -96.42355257216656\n",
      "Average Episode Reward: -75.03644813532945\n",
      "Average Episode Reward: -97.41578161148075\n",
      "Average Episode Reward: -72.94887749722903\n",
      "i: 1600, s: 34364, Loss: 1.34101140499115\n",
      "Average Episode Reward: -147.72789704941735\n",
      "Average Episode Reward: -88.93557928894778\n",
      "Average Episode Reward: -129.97905141665683\n",
      "Average Episode Reward: -92.51142389964352\n",
      "i: 1800, s: 38510, Loss: 1.3333500623703003\n",
      "Average Episode Reward: -81.07314846480648\n",
      "Average Episode Reward: -81.45287999250097\n",
      "Average Episode Reward: -94.73643903128031\n",
      "Average Episode Reward: -131.0918367629011\n",
      "i: 2000, s: 42610, Loss: 1.3259257078170776\n",
      "Average Episode Reward: -57.21486287158236\n",
      "Average Episode Reward: -112.81525384403889\n",
      "Average Episode Reward: -117.96838613652744\n",
      "Average Episode Reward: -102.85694037554381\n",
      "i: 2200, s: 47095, Loss: 1.3177781105041504\n",
      "Average Episode Reward: -106.63963778699511\n",
      "Average Episode Reward: -83.8799083364838\n",
      "Average Episode Reward: -69.66419282596816\n",
      "Average Episode Reward: -97.44169188456394\n",
      "i: 2400, s: 51717, Loss: 1.3087666034698486\n",
      "Average Episode Reward: -92.98784460512556\n",
      "Average Episode Reward: -84.74330996692092\n",
      "Average Episode Reward: -117.54092359741483\n",
      "Average Episode Reward: -82.98130921244287\n",
      "i: 2600, s: 56494, Loss: 1.2990537881851196\n",
      "Average Episode Reward: -111.9951971242737\n",
      "Average Episode Reward: -31.697816885967484\n",
      "Average Episode Reward: -117.28410355718754\n",
      "Average Episode Reward: -82.05844311487343\n",
      "i: 2800, s: 61176, Loss: 1.2877538204193115\n",
      "Average Episode Reward: -94.22637275341819\n",
      "Average Episode Reward: -54.47765448669453\n",
      "Average Episode Reward: -96.0664494682813\n",
      "Average Episode Reward: -104.44097897787651\n",
      "i: 3000, s: 65609, Loss: 1.2763595581054688\n",
      "Average Episode Reward: -26.325892335249954\n",
      "Average Episode Reward: -34.65899215488287\n",
      "Average Episode Reward: -87.25671606107366\n",
      "Average Episode Reward: -96.78306146357082\n",
      "i: 3200, s: 70124, Loss: 1.266021728515625\n",
      "Average Episode Reward: -78.19829847558415\n",
      "Average Episode Reward: -76.73795727395768\n",
      "Average Episode Reward: -51.47709638562384\n",
      "Average Episode Reward: -118.02554987797467\n",
      "i: 3400, s: 74649, Loss: 1.254792332649231\n",
      "Average Episode Reward: -78.43523894164271\n",
      "Average Episode Reward: -135.7205664065957\n",
      "Average Episode Reward: -59.33043682826146\n",
      "Average Episode Reward: -68.7493775837476\n",
      "i: 3600, s: 79030, Loss: 1.2446008920669556\n",
      "Average Episode Reward: -35.230080777518765\n",
      "Average Episode Reward: -98.30476070108948\n",
      "Average Episode Reward: -100.64596218651721\n",
      "Average Episode Reward: -64.38533004932181\n",
      "i: 3800, s: 85276, Loss: 1.2350404262542725\n",
      "Average Episode Reward: -79.123232802626\n",
      "Average Episode Reward: -58.49391812178074\n",
      "Average Episode Reward: -90.31459161778386\n",
      "Average Episode Reward: -97.19216555632417\n",
      "i: 4000, s: 91263, Loss: 1.2260559797286987\n",
      "Average Episode Reward: -65.76787854796075\n",
      "Average Episode Reward: -81.12744298428612\n",
      "Average Episode Reward: -124.62953892565456\n",
      "Average Episode Reward: -102.05292842774844\n",
      "i: 4200, s: 98870, Loss: 1.2177207469940186\n",
      "Average Episode Reward: -20.69850925137767\n",
      "Average Episode Reward: -166.10531193233084\n",
      "Average Episode Reward: -179.80719631869997\n",
      "Average Episode Reward: -220.45221912138626\n",
      "i: 4400, s: 109788, Loss: 1.209271788597107\n",
      "Average Episode Reward: -165.51840770676523\n",
      "Average Episode Reward: -124.34749257819688\n",
      "Average Episode Reward: -232.59220993334566\n",
      "Average Episode Reward: -179.98027167892488\n",
      "i: 4600, s: 117651, Loss: 1.200292944908142\n",
      "Average Episode Reward: -90.17110667487998\n",
      "Average Episode Reward: -157.2021586874639\n",
      "Average Episode Reward: -204.82066914328476\n",
      "Average Episode Reward: -171.673302743477\n",
      "i: 4800, s: 128138, Loss: 1.1914883852005005\n",
      "Average Episode Reward: -188.634230749629\n",
      "Average Episode Reward: -168.6851131819622\n",
      "Average Episode Reward: -209.10127039812514\n",
      "Average Episode Reward: -154.09600745086323\n",
      "i: 5000, s: 137588, Loss: 1.1822361946105957\n",
      "Average Episode Reward: -253.0714108667471\n",
      "Average Episode Reward: -287.38046060411534\n",
      "Average Episode Reward: -193.72976762049683\n",
      "Average Episode Reward: -288.09812409799764\n",
      "i: 5200, s: 144247, Loss: 1.1729445457458496\n",
      "Average Episode Reward: -251.52527011266088\n",
      "Average Episode Reward: -156.9141989474745\n",
      "Average Episode Reward: -237.30523630273692\n",
      "Average Episode Reward: -241.21046049884475\n",
      "i: 5400, s: 151642, Loss: 1.1643556356430054\n",
      "Average Episode Reward: -279.33123724204546\n",
      "Average Episode Reward: -256.0250703985619\n",
      "Average Episode Reward: -249.96430167393464\n",
      "Average Episode Reward: -187.40130018672363\n",
      "i: 5600, s: 158376, Loss: 1.156293511390686\n",
      "Average Episode Reward: -184.96641510905118\n",
      "Average Episode Reward: -254.2762212303948\n",
      "Average Episode Reward: -209.87341536041686\n",
      "Average Episode Reward: -240.48932876250848\n",
      "i: 5800, s: 165846, Loss: 1.1481759548187256\n",
      "Average Episode Reward: -166.921084711716\n",
      "Average Episode Reward: -272.64419479468154\n",
      "Average Episode Reward: -198.7404665096979\n",
      "Average Episode Reward: -166.04703405438596\n",
      "i: 6000, s: 174261, Loss: 1.1405006647109985\n",
      "Average Episode Reward: -227.89245242268075\n",
      "Average Episode Reward: -234.02470188576723\n",
      "Average Episode Reward: -156.33235579202358\n",
      "Average Episode Reward: -181.55965660285295\n",
      "i: 6200, s: 182754, Loss: 1.133219838142395\n",
      "Average Episode Reward: -194.12067837467333\n",
      "Average Episode Reward: -179.03501473735142\n",
      "Average Episode Reward: -190.20981320045175\n",
      "Average Episode Reward: -215.1627064027892\n",
      "i: 6400, s: 189834, Loss: 1.126158595085144\n",
      "Average Episode Reward: -193.9096038802443\n",
      "Average Episode Reward: -162.5844823996199\n",
      "Average Episode Reward: -234.46618901545406\n",
      "Average Episode Reward: -195.72469063879464\n",
      "i: 6600, s: 197588, Loss: 1.119452953338623\n",
      "Average Episode Reward: -142.91854151378013\n",
      "Average Episode Reward: -192.5600659751472\n",
      "Average Episode Reward: -170.05113095169085\n",
      "Average Episode Reward: -128.0816392998525\n",
      "i: 6800, s: 208126, Loss: 1.1129183769226074\n",
      "Average Episode Reward: -188.36725092608086\n",
      "Average Episode Reward: -153.1930393756823\n",
      "Average Episode Reward: -214.82645723005993\n",
      "Average Episode Reward: -202.54154885119937\n",
      "i: 7000, s: 216376, Loss: 1.1064436435699463\n",
      "Average Episode Reward: -262.4578359485522\n",
      "Average Episode Reward: -108.85537320840736\n",
      "Average Episode Reward: -85.81466313709946\n",
      "Average Episode Reward: -165.13952418615952\n",
      "i: 7200, s: 225772, Loss: 1.1000717878341675\n",
      "Average Episode Reward: -246.89048503775416\n",
      "Average Episode Reward: -170.77744771215862\n",
      "Average Episode Reward: -170.16642895534068\n",
      "Average Episode Reward: -181.94115534406427\n",
      "i: 7400, s: 233841, Loss: 1.0938819646835327\n",
      "Average Episode Reward: -230.45617028687553\n",
      "Average Episode Reward: -105.6385145808525\n",
      "Average Episode Reward: -253.2673866547415\n",
      "Average Episode Reward: -224.947326364748\n",
      "i: 7600, s: 243986, Loss: 1.0877567529678345\n",
      "Average Episode Reward: -90.80623390483474\n",
      "Average Episode Reward: -129.4835242555949\n",
      "Average Episode Reward: -109.86032925352694\n",
      "Average Episode Reward: -199.3583400489301\n",
      "i: 7800, s: 254580, Loss: 1.0815364122390747\n",
      "Average Episode Reward: -228.30245001261363\n",
      "Average Episode Reward: -274.14439919125255\n",
      "Average Episode Reward: -176.7662668086852\n",
      "Average Episode Reward: -101.19112878239733\n",
      "i: 8000, s: 261936, Loss: 1.0753262042999268\n",
      "Average Episode Reward: -191.74284603395117\n",
      "Average Episode Reward: -256.3779347904516\n",
      "Average Episode Reward: -78.31422169638904\n",
      "Average Episode Reward: -97.4306900402947\n",
      "i: 8200, s: 270804, Loss: 1.0691465139389038\n",
      "Average Episode Reward: -148.27365282247973\n",
      "Average Episode Reward: -124.84183127872407\n",
      "Average Episode Reward: -93.65424828209322\n",
      "Average Episode Reward: -179.27467968771518\n",
      "i: 8400, s: 281566, Loss: 1.063187837600708\n",
      "Average Episode Reward: -110.97022821012976\n",
      "Average Episode Reward: -192.3665123871492\n",
      "Average Episode Reward: -184.67650329245504\n",
      "Average Episode Reward: -58.964730203515366\n",
      "i: 8600, s: 290318, Loss: 1.0573837757110596\n",
      "Average Episode Reward: -221.72931157245253\n",
      "Average Episode Reward: -136.8508191142459\n",
      "Average Episode Reward: -147.49437260922758\n",
      "Average Episode Reward: -304.13467601880325\n",
      "i: 8800, s: 299541, Loss: 1.0515762567520142\n",
      "Average Episode Reward: -219.5312303082009\n",
      "Average Episode Reward: -113.13401769233482\n",
      "Average Episode Reward: -221.87554818706025\n",
      "Average Episode Reward: -116.24953238588712\n",
      "i: 9000, s: 310333, Loss: 1.045746922492981\n",
      "Average Episode Reward: -153.78268328933427\n",
      "Average Episode Reward: -174.50912474445562\n",
      "Average Episode Reward: -89.31057406635334\n",
      "Average Episode Reward: -74.61088478270071\n",
      "i: 9200, s: 319952, Loss: 1.0400519371032715\n",
      "Average Episode Reward: -145.5937561296143\n",
      "Average Episode Reward: -95.11656712507765\n",
      "Average Episode Reward: -247.2118825090102\n",
      "Average Episode Reward: -66.15218231328001\n",
      "i: 9400, s: 332040, Loss: 1.0345580577850342\n",
      "Average Episode Reward: -90.11035277002057\n",
      "Average Episode Reward: -200.7789512378846\n",
      "Average Episode Reward: -29.828141398035125\n",
      "Average Episode Reward: -101.15898678390133\n",
      "i: 9600, s: 343480, Loss: 1.0285745859146118\n",
      "Average Episode Reward: -49.735868834494795\n",
      "Average Episode Reward: -99.25781492621266\n",
      "Average Episode Reward: -227.42768826884458\n",
      "Average Episode Reward: -93.2700809519057\n",
      "i: 9800, s: 352713, Loss: 1.0223453044891357\n",
      "Average Episode Reward: -55.869511856746215\n",
      "Average Episode Reward: -153.87473081153038\n",
      "Average Episode Reward: -109.29417540190977\n",
      "Average Episode Reward: -163.5514605740972\n",
      "i: 10000, s: 365014, Loss: 1.0160648822784424\n",
      "Average Episode Reward: -69.90727322029663\n",
      "Average Episode Reward: -103.92304300095903\n",
      "Average Episode Reward: -80.20086462808645\n",
      "Average Episode Reward: -20.3098696165287\n",
      "i: 10200, s: 375470, Loss: 1.009537935256958\n",
      "Average Episode Reward: -47.59421528687052\n",
      "Average Episode Reward: -124.54236174494324\n",
      "Average Episode Reward: -62.45158177443805\n",
      "Average Episode Reward: -91.8861397130531\n",
      "i: 10400, s: 386777, Loss: 1.0030678510665894\n",
      "Average Episode Reward: -131.05040078757762\n",
      "Average Episode Reward: -63.45095851312577\n",
      "Average Episode Reward: -43.99613522376577\n",
      "Average Episode Reward: -57.97481752390807\n",
      "i: 10600, s: 396736, Loss: 0.9968435168266296\n",
      "Average Episode Reward: -102.11244702459358\n",
      "Average Episode Reward: -83.69069194597776\n",
      "Average Episode Reward: -88.58919678090214\n",
      "Average Episode Reward: -67.63203286028693\n",
      "i: 10800, s: 407231, Loss: 0.9907281398773193\n",
      "Average Episode Reward: -89.15845690391349\n",
      "Average Episode Reward: -104.46822921182925\n",
      "Average Episode Reward: -108.38334791409993\n",
      "Average Episode Reward: -86.33503596468067\n",
      "i: 11000, s: 417908, Loss: 0.9848450422286987\n",
      "Average Episode Reward: -47.57782722720751\n",
      "Average Episode Reward: -93.33148325494034\n",
      "Average Episode Reward: -83.27463827601925\n",
      "Average Episode Reward: -27.555695513944375\n",
      "i: 11200, s: 427953, Loss: 0.9791483879089355\n",
      "Average Episode Reward: -29.471977590175687\n",
      "Average Episode Reward: -42.04793074275953\n",
      "Average Episode Reward: -77.21812248260869\n",
      "Average Episode Reward: -96.83461814256272\n",
      "i: 11400, s: 439780, Loss: 0.9736488461494446\n",
      "Average Episode Reward: -74.60041553997851\n",
      "Average Episode Reward: -79.98224386825322\n",
      "Average Episode Reward: -51.552593149063185\n",
      "Average Episode Reward: -60.974678927033835\n",
      "i: 11600, s: 449373, Loss: 0.9682233929634094\n",
      "Average Episode Reward: -52.28840661013326\n",
      "Average Episode Reward: -29.46318227929986\n",
      "Average Episode Reward: -57.470795360831495\n",
      "Average Episode Reward: -99.18568232070076\n",
      "i: 11800, s: 460593, Loss: 0.962899386882782\n",
      "Average Episode Reward: -104.3024584259163\n",
      "Average Episode Reward: -132.68798056610265\n",
      "Average Episode Reward: -73.68770110974576\n",
      "Average Episode Reward: -78.5774138810975\n",
      "i: 12000, s: 467422, Loss: 0.957751989364624\n",
      "Average Episode Reward: -107.99747728709106\n",
      "Average Episode Reward: -165.67393385080362\n",
      "Average Episode Reward: -62.95905249298962\n",
      "Average Episode Reward: -66.02286334047726\n",
      "i: 12200, s: 477966, Loss: 0.9527515172958374\n",
      "Average Episode Reward: -72.76308623134086\n",
      "Average Episode Reward: -104.00759242532847\n",
      "Average Episode Reward: -64.30585719875859\n",
      "Average Episode Reward: -12.425776233317416\n",
      "i: 12400, s: 488187, Loss: 0.9478070735931396\n",
      "Average Episode Reward: -121.11575309946218\n",
      "Average Episode Reward: -82.3391560732427\n",
      "Average Episode Reward: -109.29857369698777\n",
      "Average Episode Reward: -82.02678374007208\n",
      "i: 12600, s: 500171, Loss: 0.9430280923843384\n",
      "Average Episode Reward: -83.69535273980614\n",
      "Average Episode Reward: -71.95884476043068\n",
      "Average Episode Reward: -123.52856887187654\n",
      "Average Episode Reward: -78.28649302507405\n",
      "i: 12800, s: 508166, Loss: 0.9383609294891357\n",
      "Average Episode Reward: -65.42702044428802\n",
      "Average Episode Reward: -98.268668377044\n",
      "Average Episode Reward: -86.80559336255627\n",
      "Average Episode Reward: -106.8165772354321\n",
      "i: 13000, s: 521256, Loss: 0.9338477253913879\n",
      "Average Episode Reward: -149.41030491373533\n",
      "Average Episode Reward: -100.75570075440746\n",
      "Average Episode Reward: -87.87485784523565\n",
      "Average Episode Reward: -46.74715248879914\n",
      "i: 13200, s: 530448, Loss: 0.9294036030769348\n",
      "Average Episode Reward: -120.10510078616282\n",
      "Average Episode Reward: -45.354995409865694\n",
      "Average Episode Reward: -81.60289582224794\n",
      "Average Episode Reward: -63.74552008280716\n",
      "i: 13400, s: 539427, Loss: 0.9251222014427185\n",
      "Average Episode Reward: -95.90120648138097\n",
      "Average Episode Reward: -67.38104764026369\n",
      "Average Episode Reward: -38.24562153917564\n",
      "Average Episode Reward: -62.59164086817314\n",
      "i: 13600, s: 549457, Loss: 0.9208890795707703\n",
      "Average Episode Reward: -145.76503795353227\n",
      "Average Episode Reward: -95.43386041471382\n",
      "Average Episode Reward: -126.69600419261897\n",
      "Average Episode Reward: -75.32473611390049\n",
      "i: 13800, s: 562843, Loss: 0.916803777217865\n",
      "Average Episode Reward: -114.67267256636485\n",
      "Average Episode Reward: -95.84798685833711\n",
      "Average Episode Reward: -81.03006983399578\n",
      "Average Episode Reward: -20.26982719395976\n",
      "i: 14000, s: 572312, Loss: 0.9127467274665833\n",
      "Average Episode Reward: -36.20125145307684\n",
      "Average Episode Reward: -69.15819012654995\n",
      "Average Episode Reward: -94.33671356677311\n",
      "Average Episode Reward: -124.58379752046253\n",
      "i: 14200, s: 580902, Loss: 0.9087551236152649\n",
      "Average Episode Reward: -57.95384960340876\n",
      "Average Episode Reward: -30.154971641262524\n",
      "Average Episode Reward: -107.604666749988\n",
      "Average Episode Reward: -36.2400121267011\n",
      "i: 14400, s: 590698, Loss: 0.9048637747764587\n",
      "Average Episode Reward: -40.752895318871396\n",
      "Average Episode Reward: -85.3534252974564\n",
      "Average Episode Reward: -39.82862550725406\n",
      "Average Episode Reward: -54.48292139738263\n",
      "i: 14600, s: 599730, Loss: 0.9010823369026184\n",
      "Average Episode Reward: -29.326971691190685\n",
      "Average Episode Reward: -70.7146946805732\n",
      "Average Episode Reward: -129.41398230054435\n",
      "Average Episode Reward: -72.58097328253622\n",
      "i: 14800, s: 613156, Loss: 0.8975043296813965\n",
      "Average Episode Reward: 13.71753294698461\n",
      "Average Episode Reward: -131.84318579230836\n",
      "Average Episode Reward: -86.60778768625921\n",
      "Average Episode Reward: 60.839168718614644\n",
      "i: 15000, s: 626772, Loss: 0.8939785957336426\n",
      "Average Episode Reward: -159.36833851487307\n",
      "Average Episode Reward: 43.57109694889724\n",
      "Average Episode Reward: 22.858709535709618\n",
      "Average Episode Reward: 15.768282914268545\n",
      "i: 15200, s: 638013, Loss: 0.8905859589576721\n",
      "Average Episode Reward: -55.685858141033144\n",
      "Average Episode Reward: -124.79800045039967\n",
      "Average Episode Reward: 34.34468006914286\n",
      "Average Episode Reward: 15.876124622953572\n",
      "i: 15400, s: 647822, Loss: 0.8872657418251038\n",
      "Average Episode Reward: -36.31643482665055\n",
      "Average Episode Reward: -54.02562139973044\n",
      "Average Episode Reward: -48.42880429130817\n",
      "Average Episode Reward: 35.797861456275434\n",
      "i: 15600, s: 658063, Loss: 0.8841317892074585\n",
      "Average Episode Reward: -8.880311600876103\n",
      "Average Episode Reward: -4.793492891004719\n",
      "Average Episode Reward: -69.11379192891674\n",
      "Average Episode Reward: 29.241102559201767\n",
      "i: 15800, s: 670808, Loss: 0.8810752034187317\n",
      "Average Episode Reward: -4.273986426754654\n",
      "Average Episode Reward: -11.487137361788593\n",
      "Average Episode Reward: -65.79095932485352\n",
      "Average Episode Reward: -22.927438768883942\n",
      "i: 16000, s: 681267, Loss: 0.8781070709228516\n",
      "Average Episode Reward: -2.0234233380894153\n",
      "Average Episode Reward: 33.33278935967966\n",
      "Average Episode Reward: -24.632986138767187\n",
      "Average Episode Reward: 0.5147049475792045\n",
      "i: 16200, s: 692261, Loss: 0.8751967549324036\n",
      "Average Episode Reward: -117.68170294832835\n",
      "Average Episode Reward: 11.999215330778167\n",
      "Average Episode Reward: -19.502423140558204\n",
      "Average Episode Reward: -70.54723868179579\n",
      "i: 16400, s: 702828, Loss: 0.8724313378334045\n",
      "Average Episode Reward: -5.367775005005657\n",
      "Average Episode Reward: 13.028684678622408\n",
      "Average Episode Reward: -18.109903703985825\n",
      "Average Episode Reward: 15.319361301724266\n",
      "i: 16600, s: 713725, Loss: 0.8697108030319214\n",
      "Average Episode Reward: -55.62943369166082\n",
      "Average Episode Reward: -11.67365479814868\n",
      "Average Episode Reward: -13.277174113080003\n",
      "Average Episode Reward: -26.144449758086502\n",
      "i: 16800, s: 724492, Loss: 0.8671461939811707\n",
      "Average Episode Reward: -72.23911584539425\n",
      "Average Episode Reward: 14.851060959281972\n",
      "Average Episode Reward: -107.75965928580656\n",
      "Average Episode Reward: -152.22237180194557\n",
      "i: 17000, s: 733459, Loss: 0.864707350730896\n",
      "Average Episode Reward: -3.1095440325965766\n",
      "Average Episode Reward: -39.196065452079814\n",
      "Average Episode Reward: -11.757403200431426\n",
      "Average Episode Reward: -120.99127372197435\n",
      "i: 17200, s: 743192, Loss: 0.8624199032783508\n",
      "Average Episode Reward: 19.452696746215132\n",
      "Average Episode Reward: 18.958397796742297\n",
      "Average Episode Reward: -42.73541859719519\n",
      "Average Episode Reward: -86.39510029835601\n",
      "i: 17400, s: 753900, Loss: 0.8602073192596436\n",
      "Average Episode Reward: -8.692433368877351\n",
      "Average Episode Reward: -57.30790455138377\n",
      "Average Episode Reward: 26.417127070030084\n",
      "Average Episode Reward: 16.682453261321584\n",
      "i: 17600, s: 764321, Loss: 0.8580691814422607\n",
      "Average Episode Reward: -35.536015455354445\n",
      "Average Episode Reward: 3.9269220029505534\n",
      "Average Episode Reward: -7.61435091055201\n",
      "Average Episode Reward: 45.10406583207898\n",
      "i: 17800, s: 774863, Loss: 0.8560343384742737\n",
      "Average Episode Reward: -30.434201668424862\n",
      "Average Episode Reward: -24.045283967731923\n",
      "Average Episode Reward: -37.991505101999245\n",
      "Average Episode Reward: -4.562626795015072\n",
      "i: 18000, s: 785745, Loss: 0.854042112827301\n",
      "Average Episode Reward: 58.19686076727602\n",
      "Average Episode Reward: 22.305439280006684\n",
      "Average Episode Reward: -17.43572920161503\n",
      "Average Episode Reward: -25.329706509549673\n",
      "i: 18200, s: 798212, Loss: 0.8521100878715515\n",
      "Average Episode Reward: 22.499413921747433\n",
      "Average Episode Reward: -10.434909056895314\n",
      "Average Episode Reward: 46.89212565105354\n",
      "Average Episode Reward: 30.599966655933418\n",
      "i: 18400, s: 809970, Loss: 0.8502245545387268\n",
      "Average Episode Reward: 5.214004827206731\n",
      "Average Episode Reward: 53.315984046019786\n",
      "Average Episode Reward: 10.393599165000607\n",
      "Average Episode Reward: 70.47739025507623\n",
      "i: 18600, s: 822076, Loss: 0.8483589291572571\n",
      "Average Episode Reward: -45.2464935463751\n",
      "Average Episode Reward: 24.943431056933843\n",
      "Average Episode Reward: -44.99372492701252\n",
      "Average Episode Reward: 37.434958119342376\n",
      "i: 18800, s: 832915, Loss: 0.8465381264686584\n",
      "Average Episode Reward: 14.956374634743687\n",
      "Average Episode Reward: 52.895248206792246\n",
      "Average Episode Reward: 67.35376245839494\n",
      "Average Episode Reward: 4.608902692083978\n",
      "i: 19000, s: 845220, Loss: 0.8447363376617432\n",
      "Average Episode Reward: -61.832264198955\n",
      "Average Episode Reward: 80.78705485716893\n",
      "Average Episode Reward: -33.06722829700297\n",
      "Average Episode Reward: 16.12484556771692\n",
      "i: 19200, s: 857423, Loss: 0.8430249691009521\n",
      "Average Episode Reward: 76.7240182106481\n",
      "Average Episode Reward: -19.580558392002665\n",
      "Average Episode Reward: 11.366994487057553\n",
      "Average Episode Reward: 1.8323300076512823\n",
      "i: 19400, s: 867909, Loss: 0.8413152694702148\n",
      "Average Episode Reward: 31.74334098402229\n",
      "Average Episode Reward: 19.959734683889565\n",
      "Average Episode Reward: 6.533742279129069\n",
      "Average Episode Reward: 65.36315944058444\n",
      "i: 19600, s: 880025, Loss: 0.8395886421203613\n",
      "Average Episode Reward: 20.06096359646143\n",
      "Average Episode Reward: 52.02411420227152\n",
      "Average Episode Reward: -25.756323329340482\n",
      "Average Episode Reward: 2.164553904847746\n",
      "i: 19800, s: 891128, Loss: 0.837958574295044\n",
      "Average Episode Reward: -42.85935198613144\n",
      "Average Episode Reward: -52.89913199053333\n",
      "Average Episode Reward: 8.659694103208158\n",
      "Average Episode Reward: 54.79848366319614\n",
      "i: 20000, s: 903380, Loss: 0.8364224433898926\n",
      "Average Episode Reward: -34.73963515746704\n",
      "Average Episode Reward: 34.32693379206877\n",
      "Average Episode Reward: 126.73145211623016\n",
      "Average Episode Reward: -95.69943620417182\n",
      "i: 20200, s: 914625, Loss: 0.8349111676216125\n",
      "Average Episode Reward: 107.05759385879792\n",
      "Average Episode Reward: 13.49301002653923\n",
      "Average Episode Reward: -46.466926752219806\n",
      "Average Episode Reward: -13.860340387214368\n",
      "i: 20400, s: 927094, Loss: 0.8333917856216431\n",
      "Average Episode Reward: -16.847397494811617\n",
      "Average Episode Reward: 96.18209814363985\n",
      "Average Episode Reward: 34.42786388506196\n",
      "Average Episode Reward: -12.278803342015005\n",
      "i: 20600, s: 937423, Loss: 0.8318994641304016\n",
      "Average Episode Reward: 83.83185563475506\n",
      "Average Episode Reward: 60.02609178884355\n",
      "Average Episode Reward: 11.36637257827241\n",
      "Average Episode Reward: 8.795314914370092\n",
      "i: 20800, s: 949011, Loss: 0.8304429054260254\n",
      "Average Episode Reward: -36.66275121624478\n",
      "Average Episode Reward: 63.46593572868572\n",
      "Average Episode Reward: 77.92448633702843\n",
      "Average Episode Reward: 14.728996153718475\n",
      "i: 21000, s: 960903, Loss: 0.828947126865387\n",
      "Average Episode Reward: 46.54425875967512\n",
      "Average Episode Reward: -23.152341488267076\n",
      "Average Episode Reward: 15.624019910248506\n",
      "Average Episode Reward: -78.37941798702062\n",
      "i: 21200, s: 971341, Loss: 0.8274905681610107\n",
      "Average Episode Reward: 52.846062235912406\n",
      "Average Episode Reward: 47.41907188866628\n",
      "Average Episode Reward: 51.87448077816612\n",
      "Average Episode Reward: -33.92722964949579\n",
      "i: 21400, s: 981757, Loss: 0.8260244727134705\n",
      "Average Episode Reward: -40.2263960561527\n",
      "Average Episode Reward: 15.098665046075633\n",
      "Average Episode Reward: 26.578282330317062\n",
      "Average Episode Reward: 19.693419643404564\n",
      "i: 21600, s: 991512, Loss: 0.8246510624885559\n",
      "Average Episode Reward: 21.71773906896947\n",
      "Average Episode Reward: 9.516284235848854\n",
      "Average Episode Reward: 117.12974473802747\n",
      "Average Episode Reward: 59.860108249665664\n",
      "i: 21800, s: 1003555, Loss: 0.8232831954956055\n",
      "Average Episode Reward: 90.98612382576599\n",
      "Average Episode Reward: 6.914723563038888\n",
      "Average Episode Reward: -62.795121350949344\n",
      "Average Episode Reward: 40.00686408576882\n",
      "i: 22000, s: 1013892, Loss: 0.8219318389892578\n",
      "Average Episode Reward: -15.061458747994175\n",
      "Average Episode Reward: 8.733629652643128\n",
      "Average Episode Reward: 70.84651147413261\n",
      "Average Episode Reward: 83.62858873515083\n",
      "i: 22200, s: 1026000, Loss: 0.8205633759498596\n",
      "Average Episode Reward: -41.1913157015493\n",
      "Average Episode Reward: 87.09554507547556\n",
      "Average Episode Reward: -33.38954702823475\n",
      "Average Episode Reward: 47.83845646021805\n",
      "i: 22400, s: 1036755, Loss: 0.81920325756073\n",
      "Average Episode Reward: 117.82836977274042\n",
      "Average Episode Reward: 73.1014114725956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fcacf6dd1aeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch_size, device)\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "SOLVED_MEAN_REWARD = 200\n",
    "MAX_STEPS = 10**7\n",
    "rewards = []\n",
    "\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "EVAL_EVERY = 1000\n",
    "\n",
    "for i in count(start=epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/loss', loss, i)\n",
    "    \n",
    "    (dh, dr) = rb.sample_command()\n",
    "    writer.add_scalar('Epoch/dh', dh, i)\n",
    "    writer.add_scalar('Epoch/dr', dr, i)\n",
    "\n",
    "    n_episodes_per_iter = 10\n",
    "    n_updates_per_iter = 50\n",
    "    if i % n_updates_per_iter == 0:\n",
    "        trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn)\n",
    "        rb.add(trajectories)\n",
    "        rewards.append(mean_reward)\n",
    "        rewards = rewards[-50:] # Keep only last  rewards\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Steps/reward', mean_reward, steps)\n",
    "        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Steps/length', mean_length, steps)\n",
    "        \n",
    "        if np.mean(rewards) >= SOLVED_MEAN_REWARD:\n",
    "            print(\"Task considered solved! Stopping.\")\n",
    "            break\n",
    "        \n",
    "        if steps >= MAX_STEPS:\n",
    "            print(f\"Steps {steps} exceeds max env steps {MAX_STEPS}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "    if i % EVAL_EVERY == 0:\n",
    "        eval_episodes = 10\n",
    "        _, mean_reward, length = rollout(eval_episodes, env=env, model=model_sample, \n",
    "                            sample_action=True, replay_buffer=rb, \n",
    "                            device=device, action_fn=action_fn, evaluation=True)\n",
    "        \n",
    "        writer.add_scalar('Eval/reward', mean_reward, i)        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Eval/length', mean_length, i)\n",
    "        \n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       # dh ,dr\n",
    "cmd = rb.sample_command()\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l,_ = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward, _ = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
