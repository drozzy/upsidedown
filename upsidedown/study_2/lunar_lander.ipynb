{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 64\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_shape, HIDDEN), \n",
    "#             nn.Dropout(0.1),\n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN),\n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN), \n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN), \n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, num_actions)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=150, last_few=100)\n",
    "\n",
    "n_warmup_episodes = 10\n",
    "# Random rollout\n",
    "trajectories, mean_reward, length = rollout(episodes=n_warmup_episodes, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "\n",
    "# Plot initial values\n",
    "writer.add_scalar('Steps/reward', mean_reward, steps)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 248800, steps 297727 with loss: 0.8303920030593872\n",
      "297727\n",
      "i: 248800, s: 297727, Loss: 0.8284735679626465\n",
      "Average Episode Reward: -125.2201213586819\n",
      "i: 249000, s: 298905, Loss: 0.8345566391944885\n",
      "i: 249200, s: 298905, Loss: 0.8319385647773743\n",
      "i: 249400, s: 298905, Loss: 0.8321632146835327\n",
      "i: 249600, s: 298905, Loss: 0.8308256268501282\n",
      "i: 249800, s: 298905, Loss: 0.8305871486663818\n",
      "Average Episode Reward: -277.17310294431957\n",
      "i: 250000, s: 300187, Loss: 0.8295911550521851\n",
      "i: 250200, s: 300187, Loss: 0.8290166258811951\n",
      "i: 250400, s: 300187, Loss: 0.829026460647583\n",
      "i: 250600, s: 300187, Loss: 0.8282209038734436\n",
      "i: 250800, s: 300187, Loss: 0.828019917011261\n",
      "Average Episode Reward: -239.04224280106988\n",
      "i: 251000, s: 302441, Loss: 0.8276103138923645\n",
      "i: 251200, s: 302441, Loss: 0.8285086750984192\n",
      "i: 251400, s: 302441, Loss: 0.8285440802574158\n",
      "i: 251600, s: 302441, Loss: 0.82841557264328\n",
      "i: 251800, s: 302441, Loss: 0.8283458948135376\n",
      "Average Episode Reward: -184.6108392830667\n",
      "i: 252000, s: 303792, Loss: 0.8284348845481873\n",
      "i: 252200, s: 303792, Loss: 0.8282487392425537\n",
      "i: 252400, s: 303792, Loss: 0.8280174136161804\n",
      "i: 252600, s: 303792, Loss: 0.8278348445892334\n",
      "i: 252800, s: 303792, Loss: 0.8276800513267517\n",
      "Average Episode Reward: -40.52705761239001\n",
      "i: 253000, s: 306254, Loss: 0.8278073668479919\n",
      "i: 253200, s: 306254, Loss: 0.8282327055931091\n",
      "i: 253400, s: 306254, Loss: 0.8283818364143372\n",
      "i: 253600, s: 306254, Loss: 0.8284890651702881\n",
      "i: 253800, s: 306254, Loss: 0.8284783363342285\n",
      "Average Episode Reward: -213.12659388786383\n",
      "i: 254000, s: 307616, Loss: 0.8284901976585388\n",
      "i: 254200, s: 307616, Loss: 0.8289217352867126\n",
      "i: 254400, s: 307616, Loss: 0.8291177153587341\n",
      "i: 254600, s: 307616, Loss: 0.8291217088699341\n",
      "i: 254800, s: 307616, Loss: 0.8291710615158081\n",
      "Average Episode Reward: -126.83747059814087\n",
      "i: 255000, s: 309088, Loss: 0.8292288780212402\n",
      "i: 255200, s: 309088, Loss: 0.8293963670730591\n",
      "i: 255400, s: 309088, Loss: 0.829546332359314\n",
      "i: 255600, s: 309088, Loss: 0.8297094106674194\n",
      "i: 255800, s: 309088, Loss: 0.8296735286712646\n",
      "Average Episode Reward: -235.3680601831285\n",
      "i: 256000, s: 310421, Loss: 0.8297601342201233\n",
      "i: 256200, s: 310421, Loss: 0.8297080993652344\n",
      "i: 256400, s: 310421, Loss: 0.8297796845436096\n",
      "i: 256600, s: 310421, Loss: 0.8297680020332336\n",
      "i: 256800, s: 310421, Loss: 0.8296593427658081\n",
      "Average Episode Reward: -214.69020768238087\n",
      "i: 257000, s: 311735, Loss: 0.8296512961387634\n",
      "i: 257200, s: 311735, Loss: 0.8296219706535339\n",
      "i: 257400, s: 311735, Loss: 0.8295666575431824\n",
      "i: 257600, s: 311735, Loss: 0.8295196890830994\n",
      "i: 257800, s: 311735, Loss: 0.8294349908828735\n",
      "Average Episode Reward: -114.66431832274478\n",
      "i: 258000, s: 313076, Loss: 0.8292288780212402\n",
      "i: 258200, s: 313076, Loss: 0.829400897026062\n",
      "i: 258400, s: 313076, Loss: 0.8293716907501221\n",
      "i: 258600, s: 313076, Loss: 0.8292884826660156\n",
      "i: 258800, s: 313076, Loss: 0.8293821811676025\n",
      "Average Episode Reward: -63.39401040950783\n",
      "i: 259000, s: 315043, Loss: 0.8293235898017883\n",
      "i: 259200, s: 315043, Loss: 0.8293684720993042\n",
      "i: 259400, s: 315043, Loss: 0.829401969909668\n",
      "i: 259600, s: 315043, Loss: 0.8293887376785278\n",
      "i: 259800, s: 315043, Loss: 0.8294219970703125\n",
      "Average Episode Reward: -194.15731429037743\n",
      "i: 260000, s: 316214, Loss: 0.8293895125389099\n",
      "i: 260200, s: 316214, Loss: 0.8293106555938721\n",
      "i: 260400, s: 316214, Loss: 0.8292553424835205\n",
      "i: 260600, s: 316214, Loss: 0.8292580246925354\n",
      "i: 260800, s: 316214, Loss: 0.8291906714439392\n",
      "Average Episode Reward: -101.84806441578212\n",
      "i: 261000, s: 317935, Loss: 0.8291492462158203\n",
      "i: 261200, s: 317935, Loss: 0.8293694853782654\n",
      "i: 261400, s: 317935, Loss: 0.8294580578804016\n",
      "i: 261600, s: 317935, Loss: 0.8294228315353394\n",
      "i: 261800, s: 317935, Loss: 0.8294274210929871\n",
      "Average Episode Reward: -255.13535477180935\n",
      "i: 262000, s: 319197, Loss: 0.8293718099594116\n",
      "i: 262200, s: 319197, Loss: 0.8293150067329407\n",
      "i: 262400, s: 319197, Loss: 0.8292344808578491\n",
      "i: 262600, s: 319197, Loss: 0.8292517066001892\n",
      "i: 262800, s: 319197, Loss: 0.8292047381401062\n",
      "Average Episode Reward: -110.18126594925225\n",
      "i: 263000, s: 320332, Loss: 0.8291686773300171\n",
      "i: 263200, s: 320332, Loss: 0.8291670083999634\n",
      "i: 263400, s: 320332, Loss: 0.8291720747947693\n",
      "i: 263600, s: 320332, Loss: 0.8291386365890503\n",
      "i: 263800, s: 320332, Loss: 0.8290797472000122\n",
      "Average Episode Reward: -118.88218565373577\n",
      "i: 264000, s: 321523, Loss: 0.8289997577667236\n",
      "i: 264200, s: 321523, Loss: 0.8293761610984802\n",
      "i: 264400, s: 321523, Loss: 0.8293634653091431\n",
      "i: 264600, s: 321523, Loss: 0.8293307423591614\n",
      "i: 264800, s: 321523, Loss: 0.8292721509933472\n",
      "Average Episode Reward: -134.88490401420432\n",
      "i: 265000, s: 323744, Loss: 0.8292405605316162\n",
      "i: 265200, s: 323744, Loss: 0.8292906284332275\n",
      "i: 265400, s: 323744, Loss: 0.829300045967102\n",
      "i: 265600, s: 323744, Loss: 0.8292878270149231\n",
      "i: 265800, s: 323744, Loss: 0.8293024897575378\n",
      "Average Episode Reward: -142.15199700269162\n",
      "i: 266000, s: 325280, Loss: 0.8292978405952454\n",
      "i: 266200, s: 325280, Loss: 0.8292755484580994\n",
      "i: 266400, s: 325280, Loss: 0.8292531371116638\n",
      "i: 266600, s: 325280, Loss: 0.829230785369873\n",
      "i: 266800, s: 325280, Loss: 0.829195499420166\n",
      "Average Episode Reward: -146.57864706887307\n",
      "i: 267000, s: 326816, Loss: 0.8291675448417664\n",
      "i: 267200, s: 326816, Loss: 0.8291501998901367\n",
      "i: 267400, s: 326816, Loss: 0.8291258215904236\n",
      "i: 267600, s: 326816, Loss: 0.8290796279907227\n",
      "i: 267800, s: 326816, Loss: 0.8290755748748779\n",
      "Average Episode Reward: -107.28289046528259\n",
      "i: 268000, s: 328101, Loss: 0.8290651440620422\n",
      "i: 268200, s: 328101, Loss: 0.8290554881095886\n",
      "i: 268400, s: 328101, Loss: 0.8289549946784973\n",
      "i: 268600, s: 328101, Loss: 0.8289103507995605\n",
      "i: 268800, s: 328101, Loss: 0.8288735151290894\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-69b33805b5bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mloss_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-31a24c78cfa2>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, inputs, targets)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "# eval_every = 2000\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/loss', loss, i)\n",
    "    \n",
    "    (dh, dr) = rb.sample_command()\n",
    "    writer.add_scalar('Epoch/dh', dh, i)\n",
    "    writer.add_scalar('Epoch/dr', dr, i)\n",
    "\n",
    "    n_episodes_per_iter = 10\n",
    "    n_updates_per_iter = 1000\n",
    "    if i % n_updates_per_iter == 0:\n",
    "        trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn)\n",
    "        rb.add(trajectories)\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Steps/reward', mean_reward, steps)\n",
    "        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Steps/length', mean_length, steps)\n",
    "        \n",
    "        \n",
    "#     if i % eval_every == 0:\n",
    "#         eval_episodes = 10\n",
    "#         _, mean_reward, length = rollout(eval_episodes, env=env, model=model_sample, \n",
    "#                             sample_action=True, replay_buffer=rb, \n",
    "#                             device=device, action_fn=action_fn)\n",
    "        \n",
    "#         writer.add_scalar('Epoch/reward', mean_reward, i)        \n",
    "#         mean_length = length*1.0/n_episodes_per_iter\n",
    "#         writer.add_scalar('Epoch/length', mean_length, i)\n",
    "        \n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384.11, 217.64070418127295)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 239800, steps 283882 with loss: 0.9338444471359253\n",
      "Average Episode Reward: -305.3344898485258\n"
     ]
    }
   ],
   "source": [
    "cmd = (384.11, 217.64070418127295)\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l,_ = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward, _ = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
