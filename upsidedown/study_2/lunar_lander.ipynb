{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 64\n",
    "curr_step = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_shape, HIDDEN), \n",
    "            nn.Dropout(0.1),\n",
    "            torch.nn.LayerNorm(HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN),\n",
    "            torch.nn.LayerNorm(HIDDEN),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN), \n",
    "            torch.nn.LayerNorm(HIDDEN),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN), \n",
    "            torch.nn.LayerNorm(HIDDEN),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, num_actions)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n",
      "Average Episode Reward: -142.06266750572985\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=50, last_few=25)\n",
    "\n",
    "n_warmup_episodes = 10\n",
    "# Random rollout\n",
    "trajectories, avg_reward, length = rollout(episodes=n_warmup_episodes, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "writer.add_scalar('Mean_Reward', avg_reward, steps)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 0, steps 1023 with loss: 0.0\n",
      "1023\n",
      "Average Episode Reward: -143.45522507728782\n",
      "i: 0, s: 2177, Loss: 1.4234389066696167\n",
      "Average Episode Reward: -105.68092268977098\n",
      "Average Episode Reward: -267.7498974470312\n",
      "i: 200, s: 4279, Loss: 1.2525383234024048\n",
      "Average Episode Reward: -228.05643536829726\n",
      "Average Episode Reward: -220.82039724990574\n",
      "i: 400, s: 6278, Loss: 1.2552679777145386\n",
      "Average Episode Reward: -180.4540491135382\n",
      "Average Episode Reward: -159.65957405974171\n",
      "i: 600, s: 8070, Loss: 1.2644959688186646\n",
      "Average Episode Reward: -297.5513678458101\n",
      "Average Episode Reward: -159.26715559667403\n",
      "i: 800, s: 10049, Loss: 1.2679307460784912\n",
      "Average Episode Reward: -161.82036088744374\n",
      "Average Episode Reward: -220.44808951121377\n",
      "i: 1000, s: 12127, Loss: 1.26726496219635\n",
      "Average Episode Reward: -173.71109208253773\n",
      "Average Episode Reward: -211.79766476934384\n",
      "i: 1200, s: 14022, Loss: 1.2649240493774414\n",
      "Average Episode Reward: -117.16883715569092\n",
      "Average Episode Reward: -153.14855392407807\n",
      "i: 1400, s: 16034, Loss: 1.2623790502548218\n",
      "Average Episode Reward: -182.75730170936376\n",
      "Average Episode Reward: -191.14145809455823\n",
      "i: 1600, s: 18332, Loss: 1.2592463493347168\n",
      "Average Episode Reward: -172.5153081545149\n",
      "Average Episode Reward: -143.74925383845152\n",
      "i: 1800, s: 20367, Loss: 1.2557191848754883\n",
      "Average Episode Reward: -220.048754975808\n",
      "Average Episode Reward: -160.30233364544227\n",
      "i: 2000, s: 22537, Loss: 1.2518718242645264\n",
      "Average Episode Reward: -145.1510818429854\n",
      "Average Episode Reward: -183.54837713287074\n",
      "i: 2200, s: 24602, Loss: 1.2478970289230347\n",
      "Average Episode Reward: -178.6919806781131\n",
      "Average Episode Reward: -115.1250618529215\n",
      "i: 2400, s: 26981, Loss: 1.244131088256836\n",
      "Average Episode Reward: -204.45631844137557\n",
      "Average Episode Reward: -132.41840550337062\n",
      "i: 2600, s: 29204, Loss: 1.2403180599212646\n",
      "Average Episode Reward: -122.96967019549456\n",
      "Average Episode Reward: -148.74761356732134\n",
      "i: 2800, s: 31392, Loss: 1.2369539737701416\n",
      "Average Episode Reward: -230.23913413749506\n",
      "Average Episode Reward: -152.62424899739318\n",
      "i: 3000, s: 33569, Loss: 1.2331291437149048\n",
      "Average Episode Reward: -125.64262458810374\n",
      "Average Episode Reward: -121.35708668667414\n",
      "i: 3200, s: 35981, Loss: 1.229751706123352\n",
      "Average Episode Reward: -141.5620853140113\n",
      "Average Episode Reward: -178.93187891542223\n",
      "i: 3400, s: 38414, Loss: 1.2264724969863892\n",
      "Average Episode Reward: -144.95584403174357\n",
      "Average Episode Reward: -203.15672311922623\n",
      "i: 3600, s: 40793, Loss: 1.223304033279419\n",
      "Average Episode Reward: -173.59614217206175\n",
      "Average Episode Reward: -163.18937396323696\n",
      "i: 3800, s: 43213, Loss: 1.2206883430480957\n",
      "Average Episode Reward: -106.12244638015588\n",
      "Average Episode Reward: -116.44800636044968\n",
      "i: 4000, s: 45474, Loss: 1.218083143234253\n",
      "Average Episode Reward: -90.57995934641669\n",
      "Average Episode Reward: -193.9047357442268\n",
      "i: 4200, s: 47902, Loss: 1.2156471014022827\n",
      "Average Episode Reward: -111.04820963673521\n",
      "Average Episode Reward: -147.2015186665508\n",
      "i: 4400, s: 50430, Loss: 1.2131731510162354\n",
      "Average Episode Reward: -155.16123446847766\n",
      "Average Episode Reward: -139.99226655583044\n",
      "i: 4600, s: 52816, Loss: 1.2107352018356323\n",
      "Average Episode Reward: -236.3049543852275\n",
      "Average Episode Reward: -71.83077206498609\n",
      "i: 4800, s: 55295, Loss: 1.2083696126937866\n",
      "Average Episode Reward: -157.14342520439263\n",
      "Average Episode Reward: -179.91102988498565\n",
      "i: 5000, s: 57923, Loss: 1.2058988809585571\n",
      "Average Episode Reward: -127.46082952054284\n",
      "Average Episode Reward: -112.60327477628263\n",
      "i: 5200, s: 60747, Loss: 1.203663945198059\n",
      "Average Episode Reward: -201.54236436937458\n",
      "Average Episode Reward: -128.70830491240343\n",
      "i: 5400, s: 63863, Loss: 1.2016716003417969\n",
      "Average Episode Reward: -98.77265780151143\n",
      "Average Episode Reward: -130.29387540601138\n",
      "i: 5600, s: 66266, Loss: 1.1996690034866333\n",
      "Average Episode Reward: -174.70073443558016\n",
      "Average Episode Reward: -196.9051528478572\n",
      "i: 5800, s: 69956, Loss: 1.1976805925369263\n",
      "Average Episode Reward: -131.74502905748352\n",
      "Average Episode Reward: -187.71940456688964\n",
      "i: 6000, s: 72517, Loss: 1.1958390474319458\n",
      "Average Episode Reward: -110.43563302598177\n",
      "Average Episode Reward: -131.10680994084706\n",
      "i: 6200, s: 75214, Loss: 1.1939921379089355\n",
      "Average Episode Reward: -94.76514834813798\n",
      "Average Episode Reward: -139.34185193322634\n",
      "i: 6400, s: 77909, Loss: 1.192107915878296\n",
      "Average Episode Reward: -93.7339053617026\n",
      "Average Episode Reward: -120.03485090430271\n",
      "i: 6600, s: 80813, Loss: 1.1904014348983765\n",
      "Average Episode Reward: -137.99074652479004\n",
      "Average Episode Reward: -107.93894934550318\n",
      "i: 6800, s: 83258, Loss: 1.188768982887268\n",
      "Average Episode Reward: -150.52233537912528\n",
      "Average Episode Reward: -98.84087522844209\n",
      "i: 7000, s: 85740, Loss: 1.187123417854309\n",
      "Average Episode Reward: -126.83720228171717\n",
      "Average Episode Reward: -162.19791535915112\n",
      "i: 7200, s: 88876, Loss: 1.1855418682098389\n",
      "Average Episode Reward: -134.32053400298793\n",
      "Average Episode Reward: -123.08301439883458\n",
      "i: 7400, s: 91426, Loss: 1.1839675903320312\n",
      "Average Episode Reward: -114.68999581649484\n",
      "Average Episode Reward: -137.178364525461\n",
      "i: 7600, s: 93555, Loss: 1.18242609500885\n",
      "Average Episode Reward: -102.53664377277775\n",
      "Average Episode Reward: -107.83585819000034\n",
      "i: 7800, s: 96408, Loss: 1.1810318231582642\n",
      "Average Episode Reward: -121.64312108974931\n",
      "Average Episode Reward: -117.39235828376384\n",
      "i: 8000, s: 98900, Loss: 1.1796894073486328\n",
      "Average Episode Reward: -156.95080291243124\n",
      "Average Episode Reward: -152.90046982346956\n",
      "i: 8200, s: 101544, Loss: 1.1783220767974854\n",
      "Average Episode Reward: -95.78028357468766\n",
      "Average Episode Reward: -137.6504687344263\n",
      "i: 8400, s: 104340, Loss: 1.1770261526107788\n",
      "Average Episode Reward: -116.97488867210068\n",
      "Average Episode Reward: -122.81056612461632\n",
      "i: 8600, s: 106876, Loss: 1.1758264303207397\n",
      "Average Episode Reward: -112.50349668345136\n",
      "Average Episode Reward: -112.1177965922454\n",
      "i: 8800, s: 109532, Loss: 1.1746569871902466\n",
      "Average Episode Reward: -153.88764629405966\n",
      "Average Episode Reward: -144.57064891570107\n",
      "i: 9000, s: 112598, Loss: 1.173626184463501\n",
      "Average Episode Reward: -166.67789532937937\n",
      "Average Episode Reward: -232.2518292049821\n",
      "i: 9200, s: 115325, Loss: 1.172621726989746\n",
      "Average Episode Reward: -153.10942671676813\n",
      "Average Episode Reward: -93.96811257130058\n",
      "i: 9400, s: 118848, Loss: 1.1716605424880981\n",
      "Average Episode Reward: -99.82742998935666\n",
      "Average Episode Reward: -99.76650814408517\n",
      "i: 9600, s: 121383, Loss: 1.1707955598831177\n",
      "Average Episode Reward: -136.24566646969294\n",
      "Average Episode Reward: -165.75054809968213\n",
      "i: 9800, s: 124915, Loss: 1.1699175834655762\n",
      "Average Episode Reward: -136.79520700183986\n",
      "Average Episode Reward: -157.04019440748073\n",
      "i: 10000, s: 127729, Loss: 1.1690930128097534\n",
      "Average Episode Reward: -91.19995607768706\n",
      "Average Episode Reward: -207.57237269469897\n",
      "i: 10200, s: 130459, Loss: 1.1682032346725464\n",
      "Average Episode Reward: -152.82327474624435\n",
      "Average Episode Reward: -92.65282204323299\n",
      "i: 10400, s: 133441, Loss: 1.1672998666763306\n",
      "Average Episode Reward: -135.69755612460088\n",
      "Average Episode Reward: -143.47908291579853\n",
      "i: 10600, s: 136341, Loss: 1.166417121887207\n",
      "Average Episode Reward: -90.26416958679073\n",
      "Average Episode Reward: -125.7636586719537\n",
      "i: 10800, s: 139107, Loss: 1.1654690504074097\n",
      "Average Episode Reward: -92.6372730812071\n",
      "Average Episode Reward: -86.13488943178564\n",
      "i: 11000, s: 142539, Loss: 1.1645625829696655\n",
      "Average Episode Reward: -142.59974969338023\n",
      "Average Episode Reward: -129.84669393906904\n",
      "i: 11200, s: 145328, Loss: 1.16374671459198\n",
      "Average Episode Reward: -165.80519333836634\n",
      "Average Episode Reward: -180.3896817249976\n",
      "i: 11400, s: 149088, Loss: 1.1629327535629272\n",
      "Average Episode Reward: -106.62593561695977\n",
      "Average Episode Reward: -166.1253299404957\n",
      "i: 11600, s: 151881, Loss: 1.162151575088501\n",
      "Average Episode Reward: -112.67767150239884\n",
      "Average Episode Reward: -136.275608835926\n",
      "i: 11800, s: 155241, Loss: 1.1614516973495483\n",
      "Average Episode Reward: -135.54422327071865\n",
      "Average Episode Reward: -97.15981153766916\n",
      "i: 12000, s: 157757, Loss: 1.1607706546783447\n",
      "Average Episode Reward: -66.0686573717444\n",
      "Average Episode Reward: -139.96363159326933\n",
      "i: 12200, s: 160550, Loss: 1.1600288152694702\n",
      "Average Episode Reward: -176.52930428385145\n",
      "Average Episode Reward: -121.67615079709451\n",
      "i: 12400, s: 163418, Loss: 1.1591839790344238\n",
      "Average Episode Reward: -153.30585721145997\n",
      "Average Episode Reward: -121.79059511671153\n",
      "i: 12600, s: 166149, Loss: 1.1581941843032837\n",
      "Average Episode Reward: -135.86472934525833\n",
      "Average Episode Reward: -103.2306776332122\n",
      "i: 12800, s: 168951, Loss: 1.157213568687439\n",
      "Average Episode Reward: -181.1253018734206\n",
      "Average Episode Reward: -126.30064698395715\n",
      "i: 13000, s: 171781, Loss: 1.1561857461929321\n",
      "Average Episode Reward: -118.15713462158064\n",
      "Average Episode Reward: -120.96902956818926\n",
      "i: 13200, s: 174527, Loss: 1.155070185661316\n",
      "Average Episode Reward: -165.66962439998406\n",
      "Average Episode Reward: -92.33535174317907\n",
      "i: 13400, s: 177723, Loss: 1.153920292854309\n",
      "Average Episode Reward: -141.62176076520427\n",
      "Average Episode Reward: -63.47414086920114\n",
      "i: 13600, s: 180546, Loss: 1.152834415435791\n",
      "Average Episode Reward: -97.57874845151214\n",
      "Average Episode Reward: -119.36464797380606\n",
      "i: 13800, s: 183939, Loss: 1.1517353057861328\n",
      "Average Episode Reward: -146.26729096588494\n",
      "Average Episode Reward: -167.7539753763752\n",
      "i: 14000, s: 187038, Loss: 1.1506270170211792\n",
      "Average Episode Reward: -270.96682127493267\n",
      "Average Episode Reward: -90.11461872481618\n",
      "i: 14200, s: 190776, Loss: 1.1494556665420532\n",
      "Average Episode Reward: -179.46326360837014\n",
      "Average Episode Reward: -99.49242546208968\n",
      "i: 14400, s: 193991, Loss: 1.148289442062378\n",
      "Average Episode Reward: -257.4515080182632\n",
      "Average Episode Reward: -85.17992777001714\n",
      "i: 14600, s: 197457, Loss: 1.1471272706985474\n",
      "Average Episode Reward: -130.33243450718751\n",
      "Average Episode Reward: -130.7066021618944\n",
      "i: 14800, s: 200322, Loss: 1.1460105180740356\n",
      "Average Episode Reward: -80.66935754580221\n",
      "Average Episode Reward: -87.75803039434228\n",
      "i: 15000, s: 203327, Loss: 1.1448441743850708\n",
      "Average Episode Reward: -216.87160384406326\n",
      "Average Episode Reward: -188.19247424917947\n",
      "i: 15200, s: 206994, Loss: 1.1436995267868042\n",
      "Average Episode Reward: -151.50005774035844\n",
      "Average Episode Reward: -161.96570949207913\n",
      "i: 15400, s: 210101, Loss: 1.1425979137420654\n",
      "Average Episode Reward: -147.69712227827114\n",
      "Average Episode Reward: -158.40408170403018\n",
      "i: 15600, s: 213074, Loss: 1.141489863395691\n",
      "Average Episode Reward: -214.47235195428775\n",
      "Average Episode Reward: -117.38502755985469\n",
      "i: 15800, s: 215990, Loss: 1.1403728723526\n",
      "Average Episode Reward: -84.42962126033778\n",
      "Average Episode Reward: -108.54767848771432\n",
      "i: 16000, s: 218567, Loss: 1.1392592191696167\n",
      "Average Episode Reward: -294.8423281685548\n",
      "Average Episode Reward: -126.88580730605872\n",
      "i: 16200, s: 221722, Loss: 1.1381418704986572\n",
      "Average Episode Reward: -183.6088655689799\n",
      "Average Episode Reward: -107.79643860906454\n",
      "i: 16400, s: 224666, Loss: 1.1369878053665161\n",
      "Average Episode Reward: -69.67609317197073\n",
      "Average Episode Reward: -59.411311893742166\n",
      "i: 16600, s: 227596, Loss: 1.1358681917190552\n",
      "Average Episode Reward: -127.94485586155966\n",
      "Average Episode Reward: -124.45243623674745\n",
      "i: 16800, s: 230579, Loss: 1.1347486972808838\n",
      "Average Episode Reward: -83.08465920756862\n",
      "Average Episode Reward: -139.78414327384252\n",
      "i: 17000, s: 233466, Loss: 1.1336684226989746\n",
      "Average Episode Reward: -95.47892570696966\n",
      "Average Episode Reward: -111.07539571665272\n",
      "i: 17200, s: 235882, Loss: 1.1326451301574707\n",
      "Average Episode Reward: -66.76756279322203\n",
      "Average Episode Reward: -107.90624911281854\n",
      "i: 17400, s: 238746, Loss: 1.1316561698913574\n",
      "Average Episode Reward: -156.26556419707018\n",
      "Average Episode Reward: -106.27468925573662\n",
      "i: 17600, s: 241552, Loss: 1.1306428909301758\n",
      "Average Episode Reward: -55.46124768844279\n",
      "Average Episode Reward: -97.12938384256665\n",
      "i: 17800, s: 244286, Loss: 1.1296494007110596\n",
      "Average Episode Reward: -121.9968305021127\n",
      "Average Episode Reward: -50.787456017967585\n",
      "i: 18000, s: 247164, Loss: 1.1286731958389282\n",
      "Average Episode Reward: -136.19691211992352\n",
      "Average Episode Reward: -166.81281636139065\n",
      "i: 18200, s: 250329, Loss: 1.1276748180389404\n",
      "Average Episode Reward: -42.58475398951313\n",
      "Average Episode Reward: -116.99519749855358\n",
      "i: 18400, s: 253624, Loss: 1.1267125606536865\n",
      "Average Episode Reward: -89.27130820558575\n",
      "Average Episode Reward: -150.10236253507713\n",
      "i: 18600, s: 256568, Loss: 1.125705361366272\n",
      "Average Episode Reward: -164.98450842847575\n",
      "Average Episode Reward: -101.91624805843169\n",
      "i: 18800, s: 259498, Loss: 1.1246885061264038\n",
      "Average Episode Reward: -128.6799015451387\n",
      "Average Episode Reward: -70.26045937769152\n",
      "i: 19000, s: 262458, Loss: 1.1237382888793945\n",
      "Average Episode Reward: -64.39231620280275\n",
      "Average Episode Reward: -73.39575405108337\n",
      "i: 19200, s: 265566, Loss: 1.1228128671646118\n",
      "Average Episode Reward: -103.31763995101406\n",
      "Average Episode Reward: -109.90275376768713\n",
      "i: 19400, s: 268388, Loss: 1.1218667030334473\n",
      "Average Episode Reward: -85.3451859950016\n",
      "Average Episode Reward: -111.06408475339671\n",
      "i: 19600, s: 271362, Loss: 1.120967149734497\n",
      "Average Episode Reward: -75.05697137338504\n",
      "Average Episode Reward: -104.36932640673986\n",
      "i: 19800, s: 274115, Loss: 1.12000572681427\n",
      "Average Episode Reward: -80.9129737572487\n",
      "Average Episode Reward: -111.21895956910646\n",
      "i: 20000, s: 277179, Loss: 1.119018316268921\n",
      "Average Episode Reward: -99.95996012682355\n",
      "Average Episode Reward: -74.79049107722469\n",
      "i: 20200, s: 280068, Loss: 1.1180579662322998\n",
      "Average Episode Reward: -85.73618782702987\n",
      "Average Episode Reward: -102.16866072693071\n",
      "i: 20400, s: 282832, Loss: 1.1171313524246216\n",
      "Average Episode Reward: -176.915370166522\n",
      "Average Episode Reward: -152.94086883200575\n",
      "i: 20600, s: 286160, Loss: 1.1161860227584839\n",
      "Average Episode Reward: -119.11543287824944\n",
      "Average Episode Reward: -85.67537180614852\n",
      "i: 20800, s: 289788, Loss: 1.1152472496032715\n",
      "Average Episode Reward: -86.24849906371867\n",
      "Average Episode Reward: -80.63213367115938\n",
      "i: 21000, s: 292407, Loss: 1.1142791509628296\n",
      "Average Episode Reward: -73.12571397057005\n",
      "Average Episode Reward: -122.94648096202756\n",
      "i: 21200, s: 296051, Loss: 1.1133233308792114\n",
      "Average Episode Reward: -172.12651160515344\n",
      "Average Episode Reward: -87.79350258631469\n",
      "i: 21400, s: 299436, Loss: 1.1123981475830078\n",
      "Average Episode Reward: -152.59851279232151\n",
      "Average Episode Reward: -170.02663181605448\n",
      "i: 21600, s: 302763, Loss: 1.1114288568496704\n",
      "Average Episode Reward: -205.88126531278203\n",
      "Average Episode Reward: -166.7635322948285\n",
      "i: 21800, s: 305705, Loss: 1.1104854345321655\n",
      "Average Episode Reward: -168.49362828309037\n",
      "Average Episode Reward: -145.47684137045312\n",
      "i: 22000, s: 308363, Loss: 1.1095356941223145\n",
      "Average Episode Reward: -112.06936318530659\n",
      "Average Episode Reward: -95.03782575699132\n",
      "i: 22200, s: 311229, Loss: 1.1086102724075317\n",
      "Average Episode Reward: -125.56389353081018\n",
      "Average Episode Reward: -95.73442408664367\n",
      "i: 22400, s: 313870, Loss: 1.1076935529708862\n",
      "Average Episode Reward: -141.22179063036089\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, i)\n",
    "    \n",
    "    n_episodes_per_iter = 10\n",
    "    n_updates_per_iter = 100\n",
    "    if i % n_updates_per_iter == 0:        \n",
    "        trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn)\n",
    "        rb.add(trajectories)\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Mean_Reward', mean_reward, steps)\n",
    "        \n",
    "\n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (280.085, 318.91295076177306)\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
