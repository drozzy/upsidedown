{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 64\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_shape, HIDDEN), \n",
    "#             nn.Dropout(0.1),\n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN),\n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN), \n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN), \n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, num_actions)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=150, last_few=100)\n",
    "\n",
    "n_warmup_episodes = 10\n",
    "# Random rollout\n",
    "trajectories, mean_reward, length = rollout(episodes=n_warmup_episodes, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "\n",
    "# Plot initial values\n",
    "writer.add_scalar('Steps/reward', mean_reward, steps)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 0, steps 864 with loss: 0.0\n",
      "864\n",
      "Average Episode Reward: -192.79306413717734\n",
      "i: 0, s: 1740, Loss: 1.5553230047225952\n",
      "Average Episode Reward: -170.74703283161512\n",
      "Average Episode Reward: -166.28558643487798\n",
      "i: 200, s: 3528, Loss: 1.3806486129760742\n",
      "Average Episode Reward: -210.00028865272316\n",
      "Average Episode Reward: -197.42022935332525\n",
      "i: 400, s: 5378, Loss: 1.3783584833145142\n",
      "Average Episode Reward: -234.72498951710517\n",
      "Average Episode Reward: -109.93945381486685\n",
      "i: 600, s: 7166, Loss: 1.3772085905075073\n",
      "Average Episode Reward: -226.87068049324913\n",
      "Average Episode Reward: -201.72852621232246\n",
      "i: 800, s: 9048, Loss: 1.3757730722427368\n",
      "Average Episode Reward: -193.07927051363947\n",
      "Average Episode Reward: -127.57049004650852\n",
      "i: 1000, s: 10812, Loss: 1.3746272325515747\n",
      "Average Episode Reward: -109.93802857672922\n",
      "Average Episode Reward: -166.0632920331154\n",
      "i: 1200, s: 12635, Loss: 1.3737704753875732\n",
      "Average Episode Reward: -158.2054344121453\n",
      "Average Episode Reward: -127.60100300200591\n",
      "i: 1400, s: 14301, Loss: 1.3732308149337769\n",
      "Average Episode Reward: -144.48706979640872\n",
      "Average Episode Reward: -135.69329581794258\n",
      "i: 1600, s: 16117, Loss: 1.3732812404632568\n",
      "Average Episode Reward: -161.68936915838538\n",
      "Average Episode Reward: -162.10828363607993\n",
      "i: 1800, s: 18030, Loss: 1.3730193376541138\n",
      "Average Episode Reward: -107.302384879698\n",
      "Average Episode Reward: -130.59836482535502\n",
      "i: 2000, s: 19798, Loss: 1.3726649284362793\n",
      "Average Episode Reward: -106.40300022736554\n",
      "Average Episode Reward: -103.3491277126424\n",
      "i: 2200, s: 21320, Loss: 1.3723578453063965\n",
      "Average Episode Reward: -123.154398787531\n",
      "Average Episode Reward: -107.77412185529514\n",
      "i: 2400, s: 23011, Loss: 1.3718870878219604\n",
      "Average Episode Reward: -95.64102807599157\n",
      "Average Episode Reward: -144.2755919280867\n",
      "i: 2600, s: 24902, Loss: 1.3712886571884155\n",
      "Average Episode Reward: -101.20152895420509\n",
      "Average Episode Reward: -121.53540294845023\n",
      "i: 2800, s: 26718, Loss: 1.3706539869308472\n",
      "Average Episode Reward: -118.88992966952097\n",
      "Average Episode Reward: -112.15322770823073\n",
      "i: 3000, s: 28524, Loss: 1.369996190071106\n",
      "Average Episode Reward: -194.4474980213631\n",
      "Average Episode Reward: -156.20473821026064\n",
      "i: 3200, s: 30416, Loss: 1.3692132234573364\n",
      "Average Episode Reward: -121.17040224754842\n",
      "Average Episode Reward: -122.27030007550593\n",
      "i: 3400, s: 32182, Loss: 1.3684614896774292\n",
      "Average Episode Reward: -120.81379653208982\n",
      "Average Episode Reward: -99.68347965534049\n",
      "i: 3600, s: 33772, Loss: 1.3675752878189087\n",
      "Average Episode Reward: -110.35542996209624\n",
      "Average Episode Reward: -122.75411825743879\n",
      "i: 3800, s: 35484, Loss: 1.3665275573730469\n",
      "Average Episode Reward: -127.5181729439189\n",
      "Average Episode Reward: -139.11552703187866\n",
      "i: 4000, s: 37348, Loss: 1.3655064105987549\n",
      "Average Episode Reward: -148.9144693016381\n",
      "Average Episode Reward: -140.455185761751\n",
      "i: 4200, s: 39224, Loss: 1.3645092248916626\n",
      "Average Episode Reward: -104.32094930238068\n",
      "Average Episode Reward: -106.12303955407461\n",
      "i: 4400, s: 41073, Loss: 1.3634618520736694\n",
      "Average Episode Reward: -116.11867705597228\n",
      "Average Episode Reward: -158.1977097395905\n",
      "i: 4600, s: 42770, Loss: 1.3623920679092407\n",
      "Average Episode Reward: -103.02688644988912\n",
      "Average Episode Reward: -151.5011486365132\n",
      "i: 4800, s: 44667, Loss: 1.3613039255142212\n",
      "Average Episode Reward: -144.47544343384442\n",
      "Average Episode Reward: -121.79588790454184\n",
      "i: 5000, s: 46575, Loss: 1.3601667881011963\n",
      "Average Episode Reward: -112.0258344587721\n",
      "Average Episode Reward: -107.92106399831346\n",
      "i: 5200, s: 48497, Loss: 1.3589717149734497\n",
      "Average Episode Reward: -161.49276209880833\n",
      "Average Episode Reward: -95.27892176342505\n",
      "i: 5400, s: 50561, Loss: 1.3577464818954468\n",
      "Average Episode Reward: -108.20593542453521\n",
      "Average Episode Reward: -105.23978622786407\n",
      "i: 5600, s: 52443, Loss: 1.356459140777588\n",
      "Average Episode Reward: -113.84769455091251\n",
      "Average Episode Reward: -79.49897508762183\n",
      "i: 5800, s: 54424, Loss: 1.3551751375198364\n",
      "Average Episode Reward: -126.40076446485037\n",
      "Average Episode Reward: -125.19541060279832\n",
      "i: 6000, s: 56381, Loss: 1.35383141040802\n",
      "Average Episode Reward: -152.79823596305485\n",
      "Average Episode Reward: -96.6609391114247\n",
      "i: 6200, s: 58247, Loss: 1.3524129390716553\n",
      "Average Episode Reward: -140.6639663070881\n",
      "Average Episode Reward: -150.4690730205585\n",
      "i: 6400, s: 60173, Loss: 1.3509268760681152\n",
      "Average Episode Reward: -113.1470895101688\n",
      "Average Episode Reward: -87.45942272248854\n",
      "i: 6600, s: 62105, Loss: 1.3494477272033691\n",
      "Average Episode Reward: -113.97411674442715\n",
      "Average Episode Reward: -109.68959513757102\n",
      "i: 6800, s: 64007, Loss: 1.3479171991348267\n",
      "Average Episode Reward: -104.01588457277042\n",
      "Average Episode Reward: -136.43305651063298\n",
      "i: 7000, s: 65913, Loss: 1.3464123010635376\n",
      "Average Episode Reward: -125.94355786368237\n",
      "Average Episode Reward: -171.84068940377236\n",
      "i: 7200, s: 67956, Loss: 1.344824194908142\n",
      "Average Episode Reward: -91.19571131854336\n",
      "Average Episode Reward: -97.4071658581839\n",
      "i: 7400, s: 69853, Loss: 1.3432449102401733\n",
      "Average Episode Reward: -109.5261041387073\n",
      "Average Episode Reward: -113.04614727228997\n",
      "i: 7600, s: 71765, Loss: 1.341626524925232\n",
      "Average Episode Reward: -79.80469546723961\n",
      "Average Episode Reward: -90.88334847609734\n",
      "i: 7800, s: 73687, Loss: 1.3399708271026611\n",
      "Average Episode Reward: -117.05093446293836\n",
      "Average Episode Reward: -109.89807393558628\n",
      "i: 8000, s: 75695, Loss: 1.3383458852767944\n",
      "Average Episode Reward: -81.56754521330183\n",
      "Average Episode Reward: -71.93547568139884\n",
      "i: 8200, s: 77649, Loss: 1.3367817401885986\n",
      "Average Episode Reward: -113.25366407130619\n",
      "Average Episode Reward: -88.58260169480614\n",
      "i: 8400, s: 79628, Loss: 1.3352209329605103\n",
      "Average Episode Reward: -93.53569455803286\n",
      "Average Episode Reward: -115.0401209256444\n",
      "i: 8600, s: 81500, Loss: 1.333708643913269\n",
      "Average Episode Reward: -60.762182027562766\n",
      "Average Episode Reward: -80.4852030995474\n",
      "i: 8800, s: 83408, Loss: 1.3321704864501953\n",
      "Average Episode Reward: -88.52485379699058\n",
      "Average Episode Reward: -108.4326604972154\n",
      "i: 9000, s: 85382, Loss: 1.3305563926696777\n",
      "Average Episode Reward: -146.30815092083202\n",
      "Average Episode Reward: -95.75600798126696\n",
      "i: 9200, s: 87316, Loss: 1.328885793685913\n",
      "Average Episode Reward: -75.28643345180235\n",
      "Average Episode Reward: -55.1361072373144\n",
      "i: 9400, s: 89273, Loss: 1.3271180391311646\n",
      "Average Episode Reward: -184.50771746658612\n",
      "Average Episode Reward: -104.66452886391241\n",
      "i: 9600, s: 91295, Loss: 1.3253728151321411\n",
      "Average Episode Reward: -73.84944642055812\n",
      "Average Episode Reward: -69.99216967387149\n",
      "i: 9800, s: 93169, Loss: 1.3235660791397095\n",
      "Average Episode Reward: -78.40591254998881\n",
      "Average Episode Reward: -102.36803206800002\n",
      "i: 10000, s: 95131, Loss: 1.3218708038330078\n",
      "Average Episode Reward: -80.13073055919622\n",
      "Average Episode Reward: -107.34446153392801\n",
      "i: 10200, s: 97086, Loss: 1.3202029466629028\n",
      "Average Episode Reward: -115.274687481162\n",
      "Average Episode Reward: -171.39342114198416\n",
      "i: 10400, s: 98970, Loss: 1.3183125257492065\n",
      "Average Episode Reward: -69.00140040641936\n",
      "Average Episode Reward: -112.1600436269136\n",
      "i: 10600, s: 100914, Loss: 1.3164323568344116\n",
      "Average Episode Reward: -136.3808838958581\n",
      "Average Episode Reward: -106.15164334456111\n",
      "i: 10800, s: 102769, Loss: 1.3144404888153076\n",
      "Average Episode Reward: -96.62984079282802\n",
      "Average Episode Reward: -69.45359205560337\n",
      "i: 11000, s: 104675, Loss: 1.31241774559021\n",
      "Average Episode Reward: -95.02398033895425\n",
      "Average Episode Reward: -55.964770627765596\n",
      "i: 11200, s: 106453, Loss: 1.31027090549469\n",
      "Average Episode Reward: -83.16755403351664\n",
      "Average Episode Reward: -107.34651215183234\n",
      "i: 11400, s: 108360, Loss: 1.3080805540084839\n",
      "Average Episode Reward: -75.97375311056959\n",
      "Average Episode Reward: -96.1264069678459\n",
      "i: 11600, s: 110159, Loss: 1.3059247732162476\n",
      "Average Episode Reward: -129.0729658252942\n",
      "Average Episode Reward: -99.31142987667891\n",
      "i: 11800, s: 111996, Loss: 1.3037841320037842\n",
      "Average Episode Reward: -115.47536707305407\n",
      "Average Episode Reward: -88.77684263662755\n",
      "i: 12000, s: 113887, Loss: 1.3014923334121704\n",
      "Average Episode Reward: -79.75058943206966\n",
      "Average Episode Reward: -85.05402089679441\n",
      "i: 12200, s: 115787, Loss: 1.2992373704910278\n",
      "Average Episode Reward: -132.3261326938509\n",
      "Average Episode Reward: -74.84051900844763\n",
      "i: 12400, s: 117666, Loss: 1.2969141006469727\n",
      "Average Episode Reward: -129.20892658144794\n",
      "Average Episode Reward: -89.8837376263961\n",
      "i: 12600, s: 119462, Loss: 1.2945626974105835\n",
      "Average Episode Reward: -95.94658330208736\n",
      "Average Episode Reward: -94.77932779175053\n",
      "i: 12800, s: 121246, Loss: 1.292212724685669\n",
      "Average Episode Reward: -132.0568290778674\n",
      "Average Episode Reward: -106.8712213407601\n",
      "i: 13000, s: 123075, Loss: 1.289857268333435\n",
      "Average Episode Reward: -96.8123018330953\n",
      "Average Episode Reward: -125.94224891713831\n",
      "i: 13200, s: 124865, Loss: 1.287564754486084\n",
      "Average Episode Reward: -65.13167623459607\n",
      "Average Episode Reward: -74.70309603061096\n",
      "i: 13400, s: 126769, Loss: 1.2851836681365967\n",
      "Average Episode Reward: -136.9545573526628\n",
      "Average Episode Reward: -119.98087642514265\n",
      "i: 13600, s: 128641, Loss: 1.282794713973999\n",
      "Average Episode Reward: -161.37490378743513\n",
      "Average Episode Reward: -97.33385195111116\n",
      "i: 13800, s: 130433, Loss: 1.2803131341934204\n",
      "Average Episode Reward: -67.72932000467517\n",
      "Average Episode Reward: -115.29663426626318\n",
      "i: 14000, s: 132334, Loss: 1.277822494506836\n",
      "Average Episode Reward: -72.65888411125522\n",
      "Average Episode Reward: -123.5278128754048\n",
      "i: 14200, s: 134188, Loss: 1.2753369808197021\n",
      "Average Episode Reward: -98.88983651762332\n",
      "Average Episode Reward: -99.53191657985562\n",
      "i: 14400, s: 135927, Loss: 1.2729618549346924\n",
      "Average Episode Reward: -66.96505445446766\n",
      "Average Episode Reward: -96.65969842496436\n",
      "i: 14600, s: 137772, Loss: 1.2705973386764526\n",
      "Average Episode Reward: -70.27429772276096\n",
      "Average Episode Reward: -120.6079843461354\n",
      "i: 14800, s: 139631, Loss: 1.2683169841766357\n",
      "Average Episode Reward: -138.63919138811065\n",
      "Average Episode Reward: -156.61175885223088\n",
      "i: 15000, s: 141490, Loss: 1.2660784721374512\n",
      "Average Episode Reward: -127.36866651225391\n",
      "Average Episode Reward: -127.43973512754083\n",
      "i: 15200, s: 143248, Loss: 1.2637933492660522\n",
      "Average Episode Reward: -106.64355643341113\n",
      "Average Episode Reward: -110.48262401382479\n",
      "i: 15400, s: 144974, Loss: 1.2615376710891724\n",
      "Average Episode Reward: -99.85693754423502\n",
      "Average Episode Reward: -124.38997371449287\n",
      "i: 15600, s: 146881, Loss: 1.2592557668685913\n",
      "Average Episode Reward: -118.00679557247841\n",
      "Average Episode Reward: -59.96628697710842\n",
      "i: 15800, s: 148705, Loss: 1.2569994926452637\n",
      "Average Episode Reward: -108.12948562794512\n",
      "Average Episode Reward: -107.57222381007887\n",
      "i: 16000, s: 150545, Loss: 1.2548274993896484\n",
      "Average Episode Reward: -110.3333248350444\n",
      "Average Episode Reward: -173.73574579014834\n",
      "i: 16200, s: 152366, Loss: 1.252616286277771\n",
      "Average Episode Reward: -160.64414485004758\n",
      "Average Episode Reward: -82.56779041810863\n",
      "i: 16400, s: 154187, Loss: 1.250382661819458\n",
      "Average Episode Reward: -50.225970105881196\n",
      "Average Episode Reward: -94.90848045617757\n",
      "i: 16600, s: 155971, Loss: 1.2482092380523682\n",
      "Average Episode Reward: -144.78383785953753\n",
      "Average Episode Reward: -132.75677455255828\n",
      "i: 16800, s: 157789, Loss: 1.2460503578186035\n",
      "Average Episode Reward: -96.27794927444671\n",
      "Average Episode Reward: -170.98309648650925\n",
      "i: 17000, s: 159629, Loss: 1.2438644170761108\n",
      "Average Episode Reward: -94.6912573297387\n",
      "Average Episode Reward: -134.67420434231977\n",
      "i: 17200, s: 161442, Loss: 1.2417110204696655\n",
      "Average Episode Reward: -181.41302427460315\n",
      "Average Episode Reward: -178.7965033741014\n",
      "i: 17400, s: 163253, Loss: 1.2395116090774536\n",
      "Average Episode Reward: -87.60035396995718\n",
      "Average Episode Reward: -135.58513031951423\n",
      "i: 17600, s: 165097, Loss: 1.2373052835464478\n",
      "Average Episode Reward: -170.51972090525783\n",
      "Average Episode Reward: -87.23562374671349\n",
      "i: 17800, s: 166912, Loss: 1.235161304473877\n",
      "Average Episode Reward: -134.96864072445447\n",
      "Average Episode Reward: -128.79935692835613\n",
      "i: 18000, s: 168735, Loss: 1.232978105545044\n",
      "Average Episode Reward: -116.03194372180533\n",
      "Average Episode Reward: -126.9872540082471\n",
      "i: 18200, s: 170588, Loss: 1.230785846710205\n",
      "Average Episode Reward: -126.33311292280459\n",
      "Average Episode Reward: -139.15767869433606\n",
      "i: 18400, s: 172545, Loss: 1.2286096811294556\n",
      "Average Episode Reward: -148.4985447156352\n",
      "Average Episode Reward: -139.6827954389335\n",
      "i: 18600, s: 174425, Loss: 1.2264281511306763\n",
      "Average Episode Reward: -119.47373357242324\n",
      "Average Episode Reward: -116.26399892144396\n",
      "i: 18800, s: 176302, Loss: 1.224307656288147\n",
      "Average Episode Reward: -133.96388277662868\n",
      "Average Episode Reward: -119.2841202492413\n",
      "i: 19000, s: 178088, Loss: 1.2222121953964233\n",
      "Average Episode Reward: -86.8948997058852\n",
      "Average Episode Reward: -70.57808884047232\n",
      "i: 19200, s: 179962, Loss: 1.2201228141784668\n",
      "Average Episode Reward: -99.77923677145596\n",
      "Average Episode Reward: -141.4317861826052\n",
      "i: 19400, s: 181740, Loss: 1.2179924249649048\n",
      "Average Episode Reward: -152.8462482939729\n",
      "Average Episode Reward: -25.63053803779827\n",
      "i: 19600, s: 183638, Loss: 1.2158489227294922\n",
      "Average Episode Reward: -141.70721781098013\n",
      "Average Episode Reward: -120.9392973571693\n",
      "i: 19800, s: 185568, Loss: 1.213708519935608\n",
      "Average Episode Reward: -145.8725332002947\n",
      "Average Episode Reward: -93.67957260413615\n",
      "i: 20000, s: 187450, Loss: 1.2115895748138428\n",
      "Average Episode Reward: -119.01644686412124\n",
      "Average Episode Reward: -130.83921025185515\n",
      "i: 20200, s: 189428, Loss: 1.209455966949463\n",
      "Average Episode Reward: -124.14522580104635\n",
      "Average Episode Reward: -125.41423133385938\n",
      "i: 20400, s: 191242, Loss: 1.2073194980621338\n",
      "Average Episode Reward: -92.07380183550586\n",
      "Average Episode Reward: -147.96813440295372\n",
      "i: 20600, s: 193071, Loss: 1.2052725553512573\n",
      "Average Episode Reward: -102.7983679365133\n",
      "Average Episode Reward: -150.05523951812796\n",
      "i: 20800, s: 194928, Loss: 1.2031612396240234\n",
      "Average Episode Reward: -81.32429421622203\n",
      "Average Episode Reward: -158.80563106355936\n",
      "i: 21000, s: 196711, Loss: 1.2010308504104614\n",
      "Average Episode Reward: -69.35683089567574\n",
      "Average Episode Reward: -189.84701081696033\n",
      "i: 21200, s: 198571, Loss: 1.198978066444397\n",
      "Average Episode Reward: -113.29641605022917\n",
      "Average Episode Reward: -46.37573906644478\n",
      "i: 21400, s: 200430, Loss: 1.1969120502471924\n",
      "Average Episode Reward: -192.74960030240626\n",
      "Average Episode Reward: -97.24097420247827\n",
      "i: 21600, s: 202343, Loss: 1.1949071884155273\n",
      "Average Episode Reward: -55.32668938372039\n",
      "Average Episode Reward: -129.7282104256169\n",
      "i: 21800, s: 204172, Loss: 1.1928682327270508\n",
      "Average Episode Reward: -58.6936617802874\n",
      "Average Episode Reward: -95.1566957220158\n",
      "i: 22000, s: 206106, Loss: 1.1908845901489258\n",
      "Average Episode Reward: -111.8921241989176\n",
      "Average Episode Reward: -64.11032826814503\n",
      "i: 22200, s: 207910, Loss: 1.1889439821243286\n",
      "Average Episode Reward: -98.55541735538547\n",
      "Average Episode Reward: -89.41710820428631\n",
      "i: 22400, s: 209893, Loss: 1.186987280845642\n",
      "Average Episode Reward: -113.07658425574445\n",
      "Average Episode Reward: -111.68323932959174\n",
      "i: 22600, s: 211714, Loss: 1.185028076171875\n",
      "Average Episode Reward: -87.63600890084692\n",
      "Average Episode Reward: -122.508838916063\n",
      "i: 22800, s: 213619, Loss: 1.1830544471740723\n",
      "Average Episode Reward: -97.52803295881736\n",
      "Average Episode Reward: -63.55757180211456\n",
      "i: 23000, s: 215423, Loss: 1.1810561418533325\n",
      "Average Episode Reward: -87.32029142236442\n",
      "Average Episode Reward: -141.1995649114985\n",
      "i: 23200, s: 217231, Loss: 1.1790724992752075\n",
      "Average Episode Reward: -122.89074278045871\n",
      "Average Episode Reward: -83.96185245125396\n",
      "i: 23400, s: 219061, Loss: 1.1771693229675293\n",
      "Average Episode Reward: -140.71797864777335\n",
      "Average Episode Reward: -109.24937494905933\n",
      "i: 23600, s: 220926, Loss: 1.1752393245697021\n",
      "Average Episode Reward: -65.43890325794932\n",
      "Average Episode Reward: -51.395284966777226\n",
      "i: 23800, s: 222843, Loss: 1.1733778715133667\n",
      "Average Episode Reward: -117.16533422355795\n",
      "Average Episode Reward: -81.2472355413786\n",
      "i: 24000, s: 224663, Loss: 1.1715400218963623\n",
      "Average Episode Reward: -103.51909214094114\n",
      "Average Episode Reward: -76.52951476231992\n",
      "i: 24200, s: 226495, Loss: 1.169684648513794\n",
      "Average Episode Reward: -72.62191263965869\n",
      "Average Episode Reward: -63.73798084594223\n",
      "i: 24400, s: 228345, Loss: 1.1678112745285034\n",
      "Average Episode Reward: -83.16283027876584\n",
      "Average Episode Reward: -90.27653453884821\n",
      "i: 24600, s: 230152, Loss: 1.16599440574646\n",
      "Average Episode Reward: -143.60865275142731\n",
      "Average Episode Reward: -79.11352155372342\n",
      "i: 24800, s: 231977, Loss: 1.1641254425048828\n",
      "Average Episode Reward: -23.28026751701342\n",
      "Average Episode Reward: -155.59229154955148\n",
      "i: 25000, s: 233893, Loss: 1.1622843742370605\n",
      "Average Episode Reward: -73.0842200480114\n",
      "Average Episode Reward: -147.17271609491138\n",
      "i: 25200, s: 235702, Loss: 1.1604580879211426\n",
      "Average Episode Reward: -85.45142067714292\n",
      "Average Episode Reward: -117.31090692721932\n",
      "i: 25400, s: 237515, Loss: 1.1585966348648071\n",
      "Average Episode Reward: -80.45757889806544\n",
      "Average Episode Reward: -155.37978686387822\n",
      "i: 25600, s: 239342, Loss: 1.1567692756652832\n",
      "Average Episode Reward: -84.8885552041042\n",
      "Average Episode Reward: -119.72099996638978\n",
      "i: 25800, s: 241199, Loss: 1.1549707651138306\n",
      "Average Episode Reward: -95.20096436979256\n",
      "Average Episode Reward: -77.82915597647043\n",
      "i: 26000, s: 243122, Loss: 1.1531661748886108\n",
      "Average Episode Reward: -54.03500573637319\n",
      "Average Episode Reward: -76.41907070428198\n",
      "i: 26200, s: 245059, Loss: 1.151380181312561\n",
      "Average Episode Reward: -104.93149838775116\n",
      "Average Episode Reward: -88.37797505707331\n",
      "i: 26400, s: 246974, Loss: 1.1496294736862183\n",
      "Average Episode Reward: -68.82043996640223\n",
      "Average Episode Reward: -117.99009550008589\n",
      "i: 26600, s: 248884, Loss: 1.1478595733642578\n",
      "Average Episode Reward: -156.01886571762893\n",
      "Average Episode Reward: -146.4509978334834\n",
      "i: 26800, s: 250767, Loss: 1.146106481552124\n",
      "Average Episode Reward: -174.68812642371398\n",
      "Average Episode Reward: -138.5419432586229\n",
      "i: 27000, s: 252709, Loss: 1.1443129777908325\n",
      "Average Episode Reward: -109.9332878344635\n",
      "Average Episode Reward: -94.07472634691621\n",
      "i: 27200, s: 254585, Loss: 1.142568588256836\n",
      "Average Episode Reward: -95.36443446745182\n",
      "Average Episode Reward: -57.696705495323954\n",
      "i: 27400, s: 256476, Loss: 1.140825867652893\n",
      "Average Episode Reward: -112.00061140988642\n",
      "Average Episode Reward: -125.48187585730648\n",
      "i: 27600, s: 258456, Loss: 1.139091968536377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d4eeea6672bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mn_updates_per_iter\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n\u001b[1;32m---> 28\u001b[1;33m                               device=device, action_fn=action_fn)\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout\u001b[1;34m(episodes, env, model, sample_action, cmd, render, replay_buffer, device, action_fn)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         t, reward = rollout_episode(env=env, model=model, sample_action=sample_action, cmd=cmd,\n\u001b[1;32m---> 66\u001b[1;33m                             render=render, device=device, action_fn=action_fn)            \n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout_episode\u001b[1;34m(env, model, sample_action, cmd, render, device, action_fn)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-31a24c78cfa2>\u001b[0m in \u001b[0;36maction_fn\u001b[1;34m(model, inputs, sample_action)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0maction_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0maction_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "# eval_every = 2000\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/loss', loss, i)\n",
    "    \n",
    "    (dh, dr) = rb.sample_command()\n",
    "    writer.add_scalar('Epoch/dh', dh, i)\n",
    "    writer.add_scalar('Epoch/dr', dr, i)\n",
    "\n",
    "    n_episodes_per_iter = 10\n",
    "    n_updates_per_iter = 100\n",
    "    if i % n_updates_per_iter == 0:\n",
    "        trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn)\n",
    "        rb.add(trajectories)\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Steps/reward', mean_reward, steps)\n",
    "        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Steps/length', mean_length, steps)\n",
    "        \n",
    "        \n",
    "#     if i % eval_every == 0:\n",
    "#         eval_episodes = 10\n",
    "#         _, mean_reward, length = rollout(eval_episodes, env=env, model=model_sample, \n",
    "#                             sample_action=True, replay_buffer=rb, \n",
    "#                             device=device, action_fn=action_fn)\n",
    "        \n",
    "#         writer.add_scalar('Epoch/reward', mean_reward, i)        \n",
    "#         mean_length = length*1.0/n_episodes_per_iter\n",
    "#         writer.add_scalar('Epoch/length', mean_length, i)\n",
    "        \n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384.11, 217.64070418127295)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 239800, steps 283882 with loss: 0.9338444471359253\n",
      "Average Episode Reward: -305.3344898485258\n"
     ]
    }
   ],
   "source": [
    "cmd = (384.11, 217.64070418127295)\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l,_ = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward, _ = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
