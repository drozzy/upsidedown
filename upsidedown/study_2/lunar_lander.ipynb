{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 64\n",
    "curr_step = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_shape, HIDDEN), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN),      \n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN),      \n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN),       \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, num_actions)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n",
      "Average Episode Reward: -196.17139496307186\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=500, last_few=25)\n",
    "\n",
    "n_warmup_episodes = 10\n",
    "# Random rollout\n",
    "trajectories, avg_reward, length = rollout(episodes=n_warmup_episodes, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "writer.add_scalar('Mean_Reward', avg_reward, steps)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 0, steps 873 with loss: 0.0\n",
      "873\n",
      "Average Episode Reward: -270.8382872397604\n",
      "i: 0, s: 1762, Loss: 1.4075428247451782\n",
      "Average Episode Reward: -479.4280424597431\n",
      "i: 200, s: 2814, Loss: 1.3656699657440186\n",
      "Average Episode Reward: -259.0408612220241\n",
      "i: 400, s: 3693, Loss: 1.3508198261260986\n",
      "Average Episode Reward: -326.2165240048614\n",
      "i: 600, s: 4692, Loss: 1.3410078287124634\n",
      "Average Episode Reward: -281.82230039545027\n",
      "i: 800, s: 5614, Loss: 1.332353115081787\n",
      "Average Episode Reward: -305.13289893327845\n",
      "i: 1000, s: 6544, Loss: 1.3239821195602417\n",
      "Average Episode Reward: -291.1179316894763\n",
      "i: 1200, s: 7445, Loss: 1.316978096961975\n",
      "Average Episode Reward: -296.30426978792366\n",
      "i: 1400, s: 8311, Loss: 1.310514211654663\n",
      "Average Episode Reward: -277.79088870576777\n",
      "i: 1600, s: 9384, Loss: 1.3048697710037231\n",
      "Average Episode Reward: -309.97205954199114\n",
      "i: 1800, s: 10349, Loss: 1.2992591857910156\n",
      "Average Episode Reward: -229.6182587336529\n",
      "i: 2000, s: 11219, Loss: 1.2942049503326416\n",
      "Average Episode Reward: -236.32144933174862\n",
      "i: 2200, s: 12313, Loss: 1.2894682884216309\n",
      "Average Episode Reward: -209.3916744482868\n",
      "i: 2400, s: 13325, Loss: 1.2850549221038818\n",
      "Average Episode Reward: -274.48840143004287\n",
      "i: 2600, s: 14316, Loss: 1.2811030149459839\n",
      "Average Episode Reward: -295.26798954799955\n",
      "i: 2800, s: 15422, Loss: 1.277409553527832\n",
      "Average Episode Reward: -418.64495732330624\n",
      "i: 3000, s: 16917, Loss: 1.2738533020019531\n",
      "Average Episode Reward: -203.46588534858716\n",
      "i: 3200, s: 17821, Loss: 1.2706152200698853\n",
      "Average Episode Reward: -322.2592996737127\n",
      "i: 3400, s: 18970, Loss: 1.267544150352478\n",
      "Average Episode Reward: -328.65945861315925\n",
      "i: 3600, s: 20214, Loss: 1.2647755146026611\n",
      "Average Episode Reward: -366.445064289002\n",
      "i: 3800, s: 21447, Loss: 1.2620142698287964\n",
      "Average Episode Reward: -249.09964816090343\n",
      "i: 4000, s: 22431, Loss: 1.2588715553283691\n",
      "Average Episode Reward: -227.27154839751287\n",
      "i: 4200, s: 23463, Loss: 1.2558246850967407\n",
      "Average Episode Reward: -280.15151491422967\n",
      "i: 4400, s: 24447, Loss: 1.2528064250946045\n",
      "Average Episode Reward: -295.23757898422826\n",
      "i: 4600, s: 25578, Loss: 1.2497791051864624\n",
      "Average Episode Reward: -269.9356710768737\n",
      "i: 4800, s: 26606, Loss: 1.2467797994613647\n",
      "Average Episode Reward: -286.18905500161156\n",
      "i: 5000, s: 27680, Loss: 1.2438725233078003\n",
      "Average Episode Reward: -331.88438359595943\n",
      "i: 5200, s: 28742, Loss: 1.2409050464630127\n",
      "Average Episode Reward: -446.3444639862926\n",
      "i: 5400, s: 30129, Loss: 1.2380391359329224\n",
      "Average Episode Reward: -294.74826917396456\n",
      "i: 5600, s: 31236, Loss: 1.2350183725357056\n",
      "Average Episode Reward: -318.3127121661307\n",
      "i: 5800, s: 32445, Loss: 1.2321594953536987\n",
      "Average Episode Reward: -250.14435809268812\n",
      "i: 6000, s: 33490, Loss: 1.2291334867477417\n",
      "Average Episode Reward: -347.3916146414632\n",
      "i: 6200, s: 34609, Loss: 1.2261886596679688\n",
      "Average Episode Reward: -275.4114261977983\n",
      "i: 6400, s: 35703, Loss: 1.2232940196990967\n",
      "Average Episode Reward: -305.81635106596593\n",
      "i: 6600, s: 36846, Loss: 1.2205238342285156\n",
      "Average Episode Reward: -484.8098110541623\n",
      "i: 6800, s: 38443, Loss: 1.217736005783081\n",
      "Average Episode Reward: -280.40677052784355\n",
      "i: 7000, s: 39636, Loss: 1.2148854732513428\n",
      "Average Episode Reward: -330.16700714489457\n",
      "i: 7200, s: 40724, Loss: 1.2120282649993896\n",
      "Average Episode Reward: -320.29304519657046\n",
      "i: 7400, s: 41766, Loss: 1.2093005180358887\n",
      "Average Episode Reward: -230.5081604603788\n",
      "i: 7600, s: 42763, Loss: 1.2066189050674438\n",
      "Average Episode Reward: -256.64813916292394\n",
      "i: 7800, s: 43775, Loss: 1.2039941549301147\n",
      "Average Episode Reward: -320.77224340368764\n",
      "i: 8000, s: 44960, Loss: 1.2015514373779297\n",
      "Average Episode Reward: -326.6752637824632\n",
      "i: 8200, s: 46272, Loss: 1.1991076469421387\n",
      "Average Episode Reward: -274.0337825606367\n",
      "i: 8400, s: 47317, Loss: 1.1966934204101562\n",
      "Average Episode Reward: -310.5690333191533\n",
      "i: 8600, s: 48471, Loss: 1.1942542791366577\n",
      "Average Episode Reward: -340.8984978217821\n",
      "i: 8800, s: 49679, Loss: 1.1918458938598633\n",
      "Average Episode Reward: -388.7975056535016\n",
      "i: 9000, s: 50975, Loss: 1.189486026763916\n",
      "Average Episode Reward: -242.99529620539656\n",
      "i: 9200, s: 51988, Loss: 1.1870660781860352\n",
      "Average Episode Reward: -350.14797276940413\n",
      "i: 9400, s: 53253, Loss: 1.1847918033599854\n",
      "Average Episode Reward: -464.3797943724555\n",
      "i: 9600, s: 54738, Loss: 1.182580828666687\n",
      "Average Episode Reward: -372.0078209269313\n",
      "i: 9800, s: 55994, Loss: 1.1803637742996216\n",
      "Average Episode Reward: -325.1625745760101\n",
      "i: 10000, s: 57207, Loss: 1.178298830986023\n",
      "Average Episode Reward: -383.1207633154971\n",
      "i: 10200, s: 58451, Loss: 1.176347255706787\n",
      "Average Episode Reward: -308.703591747028\n",
      "i: 10400, s: 59644, Loss: 1.1745043992996216\n",
      "Average Episode Reward: -339.48620024690365\n",
      "i: 10600, s: 61024, Loss: 1.1727800369262695\n",
      "Average Episode Reward: -272.0164049304407\n",
      "i: 10800, s: 62238, Loss: 1.1711426973342896\n",
      "Average Episode Reward: -238.28111067412183\n",
      "i: 11000, s: 63323, Loss: 1.1694836616516113\n",
      "Average Episode Reward: -318.69069511968235\n",
      "i: 11200, s: 64622, Loss: 1.1679511070251465\n",
      "Average Episode Reward: -314.0432995002532\n",
      "i: 11400, s: 65922, Loss: 1.1665112972259521\n",
      "Average Episode Reward: -333.8544881009077\n",
      "i: 11600, s: 67186, Loss: 1.165089726448059\n",
      "Average Episode Reward: -290.17092559235846\n",
      "i: 11800, s: 68414, Loss: 1.1637581586837769\n",
      "Average Episode Reward: -327.08337128334614\n",
      "i: 12000, s: 69625, Loss: 1.1624706983566284\n",
      "Average Episode Reward: -284.3119918217822\n",
      "i: 12200, s: 70856, Loss: 1.1612615585327148\n",
      "Average Episode Reward: -243.70371534190494\n",
      "i: 12400, s: 72114, Loss: 1.160068154335022\n",
      "Average Episode Reward: -283.421537386031\n",
      "i: 12600, s: 73431, Loss: 1.1589329242706299\n",
      "Average Episode Reward: -367.67749065561406\n",
      "i: 12800, s: 74808, Loss: 1.1577900648117065\n",
      "Average Episode Reward: -235.38606145549656\n",
      "i: 13000, s: 76003, Loss: 1.156718134880066\n",
      "Average Episode Reward: -520.4198686806585\n",
      "i: 13200, s: 77766, Loss: 1.1557445526123047\n",
      "Average Episode Reward: -433.2478172455643\n",
      "i: 13400, s: 79334, Loss: 1.1547796726226807\n",
      "Average Episode Reward: -298.2635822472713\n",
      "i: 13600, s: 80573, Loss: 1.15383780002594\n",
      "Average Episode Reward: -226.50242023220463\n",
      "i: 13800, s: 81717, Loss: 1.1529629230499268\n",
      "Average Episode Reward: -275.47440307569184\n",
      "i: 14000, s: 82970, Loss: 1.1520891189575195\n",
      "Average Episode Reward: -344.10789004540663\n",
      "i: 14200, s: 84419, Loss: 1.1513034105300903\n",
      "Average Episode Reward: -230.83163596822715\n",
      "i: 14400, s: 85629, Loss: 1.1504788398742676\n",
      "Average Episode Reward: -225.40950090446557\n",
      "i: 14600, s: 86767, Loss: 1.149667501449585\n",
      "Average Episode Reward: -236.25736385566407\n",
      "i: 14800, s: 87829, Loss: 1.1488420963287354\n",
      "Average Episode Reward: -181.04063567851293\n",
      "i: 15000, s: 88851, Loss: 1.1479902267456055\n",
      "Average Episode Reward: -221.4112119600759\n",
      "i: 15200, s: 89896, Loss: 1.1472082138061523\n",
      "Average Episode Reward: -238.49280516300897\n",
      "i: 15400, s: 91067, Loss: 1.146436333656311\n",
      "Average Episode Reward: -207.39658675030145\n",
      "i: 15600, s: 92241, Loss: 1.1457126140594482\n",
      "Average Episode Reward: -251.27777291650182\n",
      "i: 15800, s: 93448, Loss: 1.1449776887893677\n",
      "Average Episode Reward: -266.53363428539603\n",
      "i: 16000, s: 94711, Loss: 1.1442302465438843\n",
      "Average Episode Reward: -210.02335616174847\n",
      "i: 16200, s: 95839, Loss: 1.1435294151306152\n",
      "Average Episode Reward: -324.8634152612318\n",
      "i: 16400, s: 97249, Loss: 1.1428288221359253\n",
      "Average Episode Reward: -354.84637623524\n",
      "i: 16600, s: 98685, Loss: 1.1421408653259277\n",
      "Average Episode Reward: -540.4693836303949\n",
      "i: 16800, s: 100176, Loss: 1.1415002346038818\n",
      "Average Episode Reward: -323.26487932149155\n",
      "i: 17000, s: 101612, Loss: 1.1409181356430054\n",
      "Average Episode Reward: -273.69154888597177\n",
      "i: 17200, s: 102842, Loss: 1.140359878540039\n",
      "Average Episode Reward: -206.2565864592304\n",
      "i: 17400, s: 103925, Loss: 1.139875054359436\n",
      "Average Episode Reward: -131.42826182465402\n",
      "i: 17600, s: 104940, Loss: 1.1393738985061646\n",
      "Average Episode Reward: -246.75407958936157\n",
      "i: 17800, s: 106104, Loss: 1.1389275789260864\n",
      "Average Episode Reward: -348.1050707879056\n",
      "i: 18000, s: 107503, Loss: 1.1385161876678467\n",
      "Average Episode Reward: -289.5716463965072\n",
      "i: 18200, s: 108850, Loss: 1.138118028640747\n",
      "Average Episode Reward: -132.9380881183929\n",
      "i: 18400, s: 109802, Loss: 1.137776255607605\n",
      "Average Episode Reward: -397.67674254887913\n",
      "i: 18600, s: 111450, Loss: 1.1374115943908691\n",
      "Average Episode Reward: -334.2928330046266\n",
      "i: 18800, s: 113026, Loss: 1.1370329856872559\n",
      "Average Episode Reward: -190.93379740235054\n",
      "i: 19000, s: 114180, Loss: 1.1366474628448486\n",
      "Average Episode Reward: -186.91830874998496\n",
      "i: 19200, s: 115331, Loss: 1.1362810134887695\n",
      "Average Episode Reward: -257.9215073299982\n",
      "i: 19400, s: 116534, Loss: 1.1359245777130127\n",
      "Average Episode Reward: -171.57743347551488\n",
      "i: 19600, s: 117633, Loss: 1.1355698108673096\n",
      "Average Episode Reward: -397.36645463843564\n",
      "i: 19800, s: 119295, Loss: 1.1352498531341553\n",
      "Average Episode Reward: -477.3053514554287\n",
      "i: 20000, s: 120883, Loss: 1.1349533796310425\n",
      "Average Episode Reward: -520.3163123499501\n",
      "i: 20200, s: 122696, Loss: 1.1346495151519775\n",
      "Average Episode Reward: -250.83106327771807\n",
      "i: 20400, s: 124092, Loss: 1.1343693733215332\n",
      "Average Episode Reward: -217.90962538169705\n",
      "i: 20600, s: 125254, Loss: 1.1340456008911133\n",
      "Average Episode Reward: -324.1049850451508\n",
      "i: 20800, s: 126640, Loss: 1.133731484413147\n",
      "Average Episode Reward: -584.028100820473\n",
      "i: 21000, s: 128134, Loss: 1.1334667205810547\n",
      "Average Episode Reward: -326.6620573413744\n",
      "i: 21200, s: 129467, Loss: 1.1332238912582397\n",
      "Average Episode Reward: -283.3035540293755\n",
      "i: 21400, s: 130670, Loss: 1.1330007314682007\n",
      "Average Episode Reward: -251.4487946386733\n",
      "i: 21600, s: 132042, Loss: 1.132771372795105\n",
      "Average Episode Reward: -220.5284970341958\n",
      "i: 21800, s: 133298, Loss: 1.1325242519378662\n",
      "Average Episode Reward: -224.2013501602657\n",
      "i: 22000, s: 134620, Loss: 1.132318377494812\n",
      "Average Episode Reward: -339.9290059318249\n",
      "i: 22200, s: 136126, Loss: 1.1320761442184448\n",
      "Average Episode Reward: -146.7833923879849\n",
      "i: 22400, s: 137182, Loss: 1.1318550109863281\n",
      "Average Episode Reward: -204.43621151811624\n",
      "i: 22600, s: 138477, Loss: 1.1316258907318115\n",
      "Average Episode Reward: -260.079420778843\n",
      "i: 22800, s: 139783, Loss: 1.131403923034668\n",
      "Average Episode Reward: -347.33635587160995\n",
      "i: 23000, s: 141272, Loss: 1.1311976909637451\n",
      "Average Episode Reward: -302.1080183750829\n",
      "i: 23200, s: 142735, Loss: 1.1310107707977295\n",
      "Average Episode Reward: -160.2397557041415\n",
      "i: 23400, s: 143835, Loss: 1.1308071613311768\n",
      "Average Episode Reward: -221.44692118067923\n",
      "i: 23600, s: 145156, Loss: 1.1306229829788208\n",
      "Average Episode Reward: -167.16868877793686\n",
      "i: 23800, s: 146357, Loss: 1.1304564476013184\n",
      "Average Episode Reward: -263.4163453658142\n",
      "i: 24000, s: 147722, Loss: 1.1302895545959473\n",
      "Average Episode Reward: -296.8641231798273\n",
      "i: 24200, s: 149252, Loss: 1.1301413774490356\n",
      "Average Episode Reward: -234.77710461463062\n",
      "i: 24400, s: 150701, Loss: 1.130008578300476\n",
      "Average Episode Reward: -270.97499977662574\n",
      "i: 24600, s: 152089, Loss: 1.1298617124557495\n",
      "Average Episode Reward: -151.59885021029282\n",
      "i: 24800, s: 153317, Loss: 1.1297330856323242\n",
      "Average Episode Reward: -184.85538094643348\n",
      "i: 25000, s: 154666, Loss: 1.1296052932739258\n",
      "Average Episode Reward: -322.53023060613776\n",
      "i: 25200, s: 156313, Loss: 1.129467487335205\n",
      "Average Episode Reward: -203.5763713012045\n",
      "i: 25400, s: 157702, Loss: 1.12935471534729\n",
      "Average Episode Reward: -235.93281876850574\n",
      "i: 25600, s: 159066, Loss: 1.1292543411254883\n",
      "Average Episode Reward: -232.21463694009338\n",
      "i: 25800, s: 160427, Loss: 1.1291755437850952\n",
      "Average Episode Reward: -169.67139243915545\n",
      "i: 26000, s: 161674, Loss: 1.1290704011917114\n",
      "Average Episode Reward: -227.6592413019736\n",
      "i: 26200, s: 162986, Loss: 1.1289775371551514\n",
      "Average Episode Reward: -174.1775438871342\n",
      "i: 26400, s: 164248, Loss: 1.1288719177246094\n",
      "Average Episode Reward: -151.51148004250314\n",
      "i: 26600, s: 165420, Loss: 1.1288071870803833\n",
      "Average Episode Reward: -349.77406712424715\n",
      "i: 26800, s: 167102, Loss: 1.1287102699279785\n",
      "Average Episode Reward: -240.109505379976\n",
      "i: 27000, s: 168278, Loss: 1.1286324262619019\n",
      "Average Episode Reward: -350.5442137474536\n",
      "i: 27200, s: 169880, Loss: 1.1285712718963623\n",
      "Average Episode Reward: -260.17803783447937\n",
      "i: 27400, s: 171353, Loss: 1.1285102367401123\n",
      "Average Episode Reward: -62.70794799636635\n",
      "i: 27600, s: 172484, Loss: 1.1284846067428589\n",
      "Average Episode Reward: -152.37809582812088\n",
      "i: 27800, s: 173685, Loss: 1.12847101688385\n",
      "Average Episode Reward: -112.72740353538154\n",
      "i: 28000, s: 174653, Loss: 1.128450870513916\n",
      "Average Episode Reward: -209.9705682931614\n",
      "i: 28200, s: 176001, Loss: 1.128433108329773\n",
      "Average Episode Reward: -172.76566820972297\n",
      "i: 28400, s: 177319, Loss: 1.1284364461898804\n",
      "Average Episode Reward: -237.07818573364483\n",
      "i: 28600, s: 178767, Loss: 1.1284347772598267\n",
      "Average Episode Reward: -233.18792945657634\n",
      "i: 28800, s: 180189, Loss: 1.128434181213379\n",
      "Average Episode Reward: -310.14141476717947\n",
      "i: 29000, s: 181746, Loss: 1.1284133195877075\n",
      "Average Episode Reward: -195.94238727982412\n",
      "i: 29200, s: 183063, Loss: 1.128401517868042\n",
      "Average Episode Reward: -110.57935008268073\n",
      "i: 29400, s: 184280, Loss: 1.1283899545669556\n",
      "Average Episode Reward: -256.93505865831395\n",
      "i: 29600, s: 185712, Loss: 1.128368854522705\n",
      "Average Episode Reward: -147.2942473944891\n",
      "i: 29800, s: 186911, Loss: 1.1283587217330933\n",
      "Average Episode Reward: -185.61262267997088\n",
      "i: 30000, s: 188259, Loss: 1.1283327341079712\n",
      "Average Episode Reward: -225.3022852631936\n",
      "i: 30200, s: 189698, Loss: 1.1282941102981567\n",
      "Average Episode Reward: -338.5336181885533\n",
      "i: 30400, s: 191425, Loss: 1.1282594203948975\n",
      "Average Episode Reward: -343.50370025820746\n",
      "i: 30600, s: 193090, Loss: 1.1282254457473755\n",
      "Average Episode Reward: -260.0881184115618\n",
      "i: 30800, s: 194662, Loss: 1.1281986236572266\n",
      "Average Episode Reward: -118.33221509980747\n",
      "i: 31000, s: 195805, Loss: 1.128164291381836\n",
      "Average Episode Reward: -217.71252553422238\n",
      "i: 31200, s: 197200, Loss: 1.1281099319458008\n",
      "Average Episode Reward: -256.20935696059723\n",
      "i: 31400, s: 198711, Loss: 1.1280686855316162\n",
      "Average Episode Reward: -325.04878536287595\n",
      "i: 31600, s: 200250, Loss: 1.12806236743927\n",
      "Average Episode Reward: -181.22732596984943\n",
      "i: 31800, s: 201519, Loss: 1.1280207633972168\n",
      "Average Episode Reward: -138.34853248938063\n",
      "i: 32000, s: 202850, Loss: 1.127974271774292\n",
      "Average Episode Reward: -232.7915816829647\n",
      "i: 32200, s: 204232, Loss: 1.1279181241989136\n",
      "Average Episode Reward: -363.05353364374906\n",
      "i: 32400, s: 205881, Loss: 1.127861738204956\n",
      "Average Episode Reward: -190.07029203136202\n",
      "i: 32600, s: 207279, Loss: 1.1278172731399536\n",
      "Average Episode Reward: -300.13583234477863\n",
      "i: 32800, s: 208771, Loss: 1.1277762651443481\n",
      "Average Episode Reward: -301.37200250027547\n",
      "i: 33000, s: 210268, Loss: 1.127716064453125\n",
      "Average Episode Reward: -345.5943327637692\n",
      "i: 33200, s: 211955, Loss: 1.1276541948318481\n",
      "Average Episode Reward: -166.6323302267377\n",
      "i: 33400, s: 213183, Loss: 1.1275994777679443\n",
      "Average Episode Reward: -244.60111761408984\n",
      "i: 33600, s: 214555, Loss: 1.1275439262390137\n",
      "Average Episode Reward: -378.40233216278966\n",
      "i: 33800, s: 216272, Loss: 1.1274954080581665\n",
      "Average Episode Reward: -214.37020183741907\n",
      "i: 34000, s: 217596, Loss: 1.1274473667144775\n",
      "Average Episode Reward: -194.34891146793885\n",
      "i: 34200, s: 219026, Loss: 1.1274021863937378\n",
      "Average Episode Reward: -534.7166912956768\n",
      "i: 34400, s: 220887, Loss: 1.1273505687713623\n",
      "Average Episode Reward: -258.4388192999495\n",
      "i: 34600, s: 222394, Loss: 1.127305507659912\n",
      "Average Episode Reward: -192.71937489766782\n",
      "i: 34800, s: 223622, Loss: 1.1272767782211304\n",
      "Average Episode Reward: -291.11652831598417\n",
      "i: 35000, s: 225224, Loss: 1.1272341012954712\n",
      "Average Episode Reward: -154.85548843032674\n",
      "i: 35200, s: 226376, Loss: 1.1271977424621582\n",
      "Average Episode Reward: -239.6294942237218\n",
      "i: 35400, s: 227786, Loss: 1.1271697282791138\n",
      "Average Episode Reward: -227.9343822615077\n",
      "i: 35600, s: 229188, Loss: 1.1271302700042725\n",
      "Average Episode Reward: -149.57233609709033\n",
      "i: 35800, s: 230443, Loss: 1.1270819902420044\n",
      "Average Episode Reward: -125.4975928724028\n",
      "i: 36000, s: 231636, Loss: 1.127051830291748\n",
      "Average Episode Reward: -219.91775868311802\n",
      "i: 36200, s: 233043, Loss: 1.126986026763916\n",
      "Average Episode Reward: -302.8885701793107\n",
      "i: 36400, s: 234542, Loss: 1.1269336938858032\n",
      "Average Episode Reward: -294.0452997243573\n",
      "i: 36600, s: 236164, Loss: 1.1268718242645264\n",
      "Average Episode Reward: -1272.4420538475529\n",
      "i: 36800, s: 238173, Loss: 1.1268019676208496\n",
      "Average Episode Reward: -195.59176260790028\n",
      "i: 37000, s: 239531, Loss: 1.1267319917678833\n",
      "Average Episode Reward: -260.6351026699215\n",
      "i: 37200, s: 241011, Loss: 1.126678228378296\n",
      "Average Episode Reward: -156.16561581730457\n",
      "i: 37400, s: 242368, Loss: 1.1266186237335205\n",
      "Average Episode Reward: -231.8211106529553\n",
      "i: 37600, s: 243856, Loss: 1.1265352964401245\n",
      "Average Episode Reward: -380.98978110150676\n",
      "i: 37800, s: 245689, Loss: 1.1264837980270386\n",
      "Average Episode Reward: -184.1092592859342\n",
      "i: 38000, s: 247017, Loss: 1.1264104843139648\n",
      "Average Episode Reward: -171.7263560734007\n",
      "i: 38200, s: 248224, Loss: 1.126348853111267\n",
      "Average Episode Reward: -399.95294853602195\n",
      "i: 38400, s: 250050, Loss: 1.1262820959091187\n",
      "Average Episode Reward: -116.76461732324006\n",
      "i: 38600, s: 251114, Loss: 1.1262167692184448\n",
      "Average Episode Reward: -250.44927908705972\n",
      "i: 38800, s: 252616, Loss: 1.1261569261550903\n",
      "Average Episode Reward: -306.71897945589814\n",
      "i: 39000, s: 254173, Loss: 1.1261019706726074\n",
      "Average Episode Reward: -285.29646494967915\n",
      "i: 39200, s: 255803, Loss: 1.126029372215271\n",
      "Average Episode Reward: -375.72345516191916\n",
      "i: 39400, s: 257509, Loss: 1.1259491443634033\n",
      "Average Episode Reward: -313.9726461960359\n",
      "i: 39600, s: 259165, Loss: 1.1258869171142578\n",
      "Average Episode Reward: -380.4443842168017\n",
      "i: 39800, s: 260999, Loss: 1.1258130073547363\n",
      "Average Episode Reward: -501.21761203875803\n",
      "i: 40000, s: 262985, Loss: 1.1257553100585938\n",
      "Average Episode Reward: -218.84627922927598\n",
      "i: 40200, s: 264309, Loss: 1.1256968975067139\n",
      "Average Episode Reward: -255.90691593872538\n",
      "i: 40400, s: 265762, Loss: 1.125605821609497\n",
      "Average Episode Reward: -173.4801376962921\n",
      "i: 40600, s: 267093, Loss: 1.1255449056625366\n",
      "Average Episode Reward: -428.37808035236515\n",
      "i: 40800, s: 269004, Loss: 1.1254878044128418\n",
      "Average Episode Reward: -267.24014815716816\n",
      "i: 41000, s: 270514, Loss: 1.1254162788391113\n",
      "Average Episode Reward: -470.0016499948171\n",
      "i: 41200, s: 272439, Loss: 1.1253527402877808\n",
      "Average Episode Reward: -266.87880667094004\n",
      "i: 41400, s: 273978, Loss: 1.1252713203430176\n",
      "Average Episode Reward: -233.1945738629021\n",
      "i: 41600, s: 275404, Loss: 1.1252135038375854\n",
      "Average Episode Reward: -255.15266130908958\n",
      "i: 41800, s: 276911, Loss: 1.1251424551010132\n",
      "Average Episode Reward: -232.94938157331268\n",
      "i: 42000, s: 278383, Loss: 1.1250860691070557\n",
      "Average Episode Reward: -172.42609288296583\n",
      "i: 42200, s: 279686, Loss: 1.1250213384628296\n",
      "Average Episode Reward: -130.7490041766081\n",
      "i: 42400, s: 280993, Loss: 1.1249653100967407\n",
      "Average Episode Reward: -401.7037800148402\n",
      "i: 42600, s: 282785, Loss: 1.124894380569458\n",
      "Average Episode Reward: -208.89229586918117\n",
      "i: 42800, s: 284158, Loss: 1.12483811378479\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1bff39801316>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mn_updates_per_iter\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n\u001b[1;32m---> 23\u001b[1;33m                               device=device, action_fn=action_fn)\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout\u001b[1;34m(episodes, env, model, sample_action, cmd, render, replay_buffer, device, action_fn)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         t, reward = rollout_episode(env=env, model=model, sample_action=sample_action, cmd=cmd,\n\u001b[1;32m---> 62\u001b[1;33m                             render=render, device=device, action_fn=action_fn)            \n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout_episode\u001b[1;34m(env, model, sample_action, cmd, render, device, action_fn)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-31a24c78cfa2>\u001b[0m in \u001b[0;36maction_fn\u001b[1;34m(model, inputs, sample_action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, i)\n",
    "    \n",
    "    n_episodes_per_iter = 10\n",
    "    n_updates_per_iter = 300\n",
    "    if i % n_updates_per_iter == 0:        \n",
    "        trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn)\n",
    "        rb.add(trajectories)\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Mean_Reward', mean_reward, steps)\n",
    "        \n",
    "\n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (280.085, 318.91295076177306)\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
