{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 64\n",
    "curr_step = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_shape, HIDDEN), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN),      \n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN),      \n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(HIDDEN, HIDDEN),       \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, num_actions)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 7200, steps 188261 with loss: 1.308440923690796\n",
      "Average Episode Reward: -179.51300150790735\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=500, last_few=200)\n",
    "\n",
    "# Random rollout\n",
    "trajectories, avg_reward, length = rollout(episodes=500, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "writer.add_scalar('Mean_Reward', avg_reward, steps)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 123600, steps 4690634 with loss: 0.927336573600769\n",
      "4690634\n",
      "i: 123600, s: 4690634, Loss: 0.8920902013778687\n",
      "i: 123800, s: 4690634, Loss: 0.9279770255088806\n",
      "Average Episode Reward: -8.499768230625975\n",
      "i: 124000, s: 4716383, Loss: 0.9281600713729858\n",
      "i: 124200, s: 4716383, Loss: 0.9322767853736877\n",
      "i: 124400, s: 4716383, Loss: 0.9341564774513245\n",
      "Average Episode Reward: -37.61002065796311\n",
      "i: 124600, s: 4737118, Loss: 0.9357600212097168\n",
      "i: 124800, s: 4737118, Loss: 0.9370045065879822\n",
      "Average Episode Reward: -43.43657153080461\n",
      "i: 125000, s: 4751972, Loss: 0.9377341866493225\n",
      "i: 125200, s: 4751972, Loss: 0.9389219880104065\n",
      "i: 125400, s: 4751972, Loss: 0.9393056631088257\n",
      "Average Episode Reward: 18.344449546997897\n",
      "i: 125600, s: 4770916, Loss: 0.9400268197059631\n",
      "i: 125800, s: 4770916, Loss: 0.9409062266349792\n",
      "Average Episode Reward: -24.647082901031716\n",
      "i: 126000, s: 4785169, Loss: 0.9417028427124023\n",
      "i: 126200, s: 4785169, Loss: 0.9424408078193665\n",
      "i: 126400, s: 4785169, Loss: 0.9427548050880432\n",
      "Average Episode Reward: 21.09970331945206\n",
      "i: 126600, s: 4800927, Loss: 0.9432523250579834\n",
      "i: 126800, s: 4800927, Loss: 0.9441363215446472\n",
      "Average Episode Reward: 53.009470965841494\n",
      "i: 127000, s: 4822009, Loss: 0.9447410702705383\n",
      "i: 127200, s: 4822009, Loss: 0.9460859298706055\n",
      "i: 127400, s: 4822009, Loss: 0.946970522403717\n",
      "Average Episode Reward: 26.78361443539644\n",
      "i: 127600, s: 4840793, Loss: 0.9477993249893188\n",
      "i: 127800, s: 4840793, Loss: 0.9487259387969971\n",
      "Average Episode Reward: 17.466043919405475\n",
      "i: 128000, s: 4857552, Loss: 0.9495111107826233\n",
      "i: 128200, s: 4857552, Loss: 0.9503483176231384\n",
      "i: 128400, s: 4857552, Loss: 0.9511728882789612\n",
      "Average Episode Reward: 46.32118565079064\n",
      "i: 128600, s: 4882521, Loss: 0.9520249366760254\n",
      "i: 128800, s: 4882521, Loss: 0.9530029296875\n",
      "Average Episode Reward: 36.16130639680061\n",
      "i: 129000, s: 4907638, Loss: 0.9539020657539368\n",
      "i: 129200, s: 4907638, Loss: 0.9549580216407776\n",
      "i: 129400, s: 4907638, Loss: 0.9558369517326355\n",
      "Average Episode Reward: 6.7106346363160325\n",
      "i: 129600, s: 4925659, Loss: 0.9567194581031799\n",
      "i: 129800, s: 4925659, Loss: 0.9575619101524353\n",
      "Average Episode Reward: -8.863371737330034\n",
      "i: 130000, s: 4941672, Loss: 0.9583116173744202\n",
      "i: 130200, s: 4941672, Loss: 0.9591785669326782\n",
      "i: 130400, s: 4941672, Loss: 0.9598793983459473\n",
      "Average Episode Reward: -21.366969904945016\n",
      "i: 130600, s: 4958229, Loss: 0.960629940032959\n",
      "i: 130800, s: 4958229, Loss: 0.9612613916397095\n",
      "Average Episode Reward: 33.726681073056334\n",
      "i: 131000, s: 4978717, Loss: 0.9618603587150574\n",
      "i: 131200, s: 4978717, Loss: 0.9626443386077881\n",
      "i: 131400, s: 4978717, Loss: 0.9632390737533569\n",
      "Average Episode Reward: 19.532283855211983\n",
      "i: 131600, s: 4998471, Loss: 0.9638720750808716\n",
      "i: 131800, s: 4998471, Loss: 0.9645116925239563\n",
      "Average Episode Reward: 76.27628908587572\n",
      "i: 132000, s: 5026702, Loss: 0.9650713801383972\n",
      "i: 132200, s: 5026702, Loss: 0.9658594727516174\n",
      "i: 132400, s: 5026702, Loss: 0.9665530920028687\n",
      "Average Episode Reward: 57.35611639023799\n",
      "i: 132600, s: 5053251, Loss: 0.9673047661781311\n",
      "i: 132800, s: 5053251, Loss: 0.9680151343345642\n",
      "Average Episode Reward: 41.95307952015548\n",
      "i: 133000, s: 5075467, Loss: 0.9687603116035461\n",
      "i: 133200, s: 5075467, Loss: 0.9696117043495178\n",
      "i: 133400, s: 5075467, Loss: 0.9703648686408997\n",
      "Average Episode Reward: 25.515769589265314\n",
      "i: 133600, s: 5096867, Loss: 0.971064031124115\n",
      "i: 133800, s: 5096867, Loss: 0.9717496633529663\n",
      "Average Episode Reward: 83.99762770509207\n",
      "i: 134000, s: 5122354, Loss: 0.9724496006965637\n",
      "i: 134200, s: 5122354, Loss: 0.9732130169868469\n",
      "i: 134400, s: 5122354, Loss: 0.9739589095115662\n",
      "Average Episode Reward: 29.229295528934685\n",
      "i: 134600, s: 5140230, Loss: 0.9747596383094788\n",
      "i: 134800, s: 5140230, Loss: 0.9754424095153809\n",
      "Average Episode Reward: -14.131672264415583\n",
      "i: 135000, s: 5154804, Loss: 0.9761466979980469\n",
      "i: 135200, s: 5154804, Loss: 0.9768338799476624\n",
      "i: 135400, s: 5154804, Loss: 0.9775816798210144\n",
      "Average Episode Reward: 10.568116409378467\n",
      "i: 135600, s: 5172954, Loss: 0.9782529473304749\n",
      "i: 135800, s: 5172954, Loss: 0.9789370894432068\n",
      "Average Episode Reward: 18.791391405067987\n",
      "i: 136000, s: 5194223, Loss: 0.9795650839805603\n",
      "i: 136200, s: 5194223, Loss: 0.9802431464195251\n",
      "i: 136400, s: 5194223, Loss: 0.9808522462844849\n",
      "Average Episode Reward: 14.530098568010281\n",
      "i: 136600, s: 5212946, Loss: 0.9814556241035461\n",
      "i: 136800, s: 5212946, Loss: 0.9820864200592041\n",
      "Average Episode Reward: 28.486577218335405\n",
      "i: 137000, s: 5233130, Loss: 0.9826493859291077\n",
      "i: 137200, s: 5233130, Loss: 0.9833011627197266\n",
      "i: 137400, s: 5233130, Loss: 0.9838961362838745\n",
      "Average Episode Reward: 40.1654549952983\n",
      "i: 137600, s: 5255504, Loss: 0.9845021963119507\n",
      "i: 137800, s: 5255504, Loss: 0.9851327538490295\n",
      "Average Episode Reward: -3.960347013782754\n",
      "i: 138000, s: 5271853, Loss: 0.9857875108718872\n",
      "i: 138200, s: 5271853, Loss: 0.9863894581794739\n",
      "i: 138400, s: 5271853, Loss: 0.986990749835968\n",
      "Average Episode Reward: -5.015113125730391\n",
      "i: 138600, s: 5303932, Loss: 0.9875890016555786\n",
      "i: 138800, s: 5303932, Loss: 0.9882178902626038\n",
      "Average Episode Reward: 4.020214083172244\n",
      "i: 139000, s: 5317457, Loss: 0.9888178110122681\n",
      "i: 139200, s: 5317457, Loss: 0.9893902540206909\n",
      "i: 139400, s: 5317457, Loss: 0.98993319272995\n",
      "Average Episode Reward: -27.080193674434195\n",
      "i: 139600, s: 5343576, Loss: 0.9904905557632446\n",
      "i: 139800, s: 5343576, Loss: 0.9910733699798584\n",
      "Average Episode Reward: 58.49696815782827\n",
      "i: 140000, s: 5372060, Loss: 0.9916449785232544\n",
      "i: 140200, s: 5372060, Loss: 0.9921218752861023\n",
      "i: 140400, s: 5372060, Loss: 0.9926168322563171\n",
      "Average Episode Reward: 11.228948585567398\n",
      "i: 140600, s: 5390640, Loss: 0.9931079149246216\n",
      "i: 140800, s: 5390640, Loss: 0.9936284422874451\n",
      "Average Episode Reward: 40.83064819931994\n",
      "i: 141000, s: 5412273, Loss: 0.9941332340240479\n",
      "i: 141200, s: 5412273, Loss: 0.9946500658988953\n",
      "i: 141400, s: 5412273, Loss: 0.9951249361038208\n",
      "Average Episode Reward: 21.236332534753565\n",
      "i: 141600, s: 5428119, Loss: 0.9956005215644836\n",
      "i: 141800, s: 5428119, Loss: 0.996082067489624\n",
      "Average Episode Reward: 15.499918289921432\n",
      "i: 142000, s: 5441948, Loss: 0.9965274333953857\n",
      "i: 142200, s: 5441948, Loss: 0.9969625473022461\n",
      "i: 142400, s: 5441948, Loss: 0.9974029064178467\n",
      "Average Episode Reward: 71.98331890465788\n",
      "i: 142600, s: 5463975, Loss: 0.9978119730949402\n",
      "i: 142800, s: 5463975, Loss: 0.9982516169548035\n",
      "Average Episode Reward: 41.13317029294012\n",
      "i: 143000, s: 5484017, Loss: 0.9986840486526489\n",
      "i: 143200, s: 5484017, Loss: 0.9991283416748047\n",
      "i: 143400, s: 5484017, Loss: 0.9995928406715393\n",
      "Average Episode Reward: 47.32576031047046\n",
      "i: 143600, s: 5504757, Loss: 1.0000358819961548\n",
      "i: 143800, s: 5504757, Loss: 1.000491976737976\n",
      "Average Episode Reward: 63.868627777006765\n",
      "i: 144000, s: 5526898, Loss: 1.0009175539016724\n",
      "i: 144200, s: 5526898, Loss: 1.001369833946228\n",
      "i: 144400, s: 5526898, Loss: 1.001825213432312\n",
      "Average Episode Reward: 61.222227109381066\n",
      "i: 144600, s: 5552995, Loss: 1.0022928714752197\n",
      "i: 144800, s: 5552995, Loss: 1.0027674436569214\n",
      "Average Episode Reward: 13.818111007222848\n",
      "i: 145000, s: 5569396, Loss: 1.0032254457473755\n",
      "i: 145200, s: 5569396, Loss: 1.0037113428115845\n",
      "i: 145400, s: 5569396, Loss: 1.0041736364364624\n",
      "Average Episode Reward: 10.599247513701306\n",
      "i: 145600, s: 5588168, Loss: 1.0046497583389282\n",
      "i: 145800, s: 5588168, Loss: 1.0050888061523438\n",
      "Average Episode Reward: 30.897452534570355\n",
      "i: 146000, s: 5605128, Loss: 1.005528211593628\n",
      "i: 146200, s: 5605128, Loss: 1.0059455633163452\n",
      "i: 146400, s: 5605128, Loss: 1.0063766241073608\n",
      "Average Episode Reward: 39.30398596074607\n",
      "i: 146600, s: 5626351, Loss: 1.006806492805481\n",
      "i: 146800, s: 5626351, Loss: 1.0072433948516846\n",
      "Average Episode Reward: 28.50898439444395\n",
      "i: 147000, s: 5641340, Loss: 1.0076628923416138\n",
      "i: 147200, s: 5641340, Loss: 1.0080595016479492\n",
      "i: 147400, s: 5641340, Loss: 1.0084673166275024\n",
      "Average Episode Reward: 51.15013708528107\n",
      "i: 147600, s: 5662521, Loss: 1.0088688135147095\n",
      "i: 147800, s: 5662521, Loss: 1.009264349937439\n",
      "Average Episode Reward: 87.40505214673043\n",
      "i: 148000, s: 5688469, Loss: 1.0096522569656372\n",
      "i: 148200, s: 5688469, Loss: 1.0100597143173218\n",
      "i: 148400, s: 5688469, Loss: 1.010454535484314\n",
      "Average Episode Reward: 54.44563532236752\n",
      "i: 148600, s: 5711629, Loss: 1.0108312368392944\n",
      "i: 148800, s: 5711629, Loss: 1.0112262964248657\n",
      "Average Episode Reward: 42.86059420670178\n",
      "i: 149000, s: 5730821, Loss: 1.0116015672683716\n",
      "i: 149200, s: 5730821, Loss: 1.011987328529358\n",
      "i: 149400, s: 5730821, Loss: 1.0123674869537354\n",
      "Average Episode Reward: 44.27206903610451\n",
      "i: 149600, s: 5753825, Loss: 1.0127252340316772\n",
      "i: 149800, s: 5753825, Loss: 1.0131036043167114\n",
      "Average Episode Reward: -1.0139235203824923\n",
      "i: 150000, s: 5768442, Loss: 1.0134638547897339\n",
      "i: 150200, s: 5768442, Loss: 1.0138332843780518\n",
      "i: 150400, s: 5768442, Loss: 1.014176368713379\n",
      "Average Episode Reward: 15.900261965697807\n",
      "i: 150600, s: 5786457, Loss: 1.014514446258545\n",
      "i: 150800, s: 5786457, Loss: 1.0148662328720093\n",
      "Average Episode Reward: 27.79699919060075\n",
      "i: 151000, s: 5801244, Loss: 1.015202522277832\n",
      "i: 151200, s: 5801244, Loss: 1.0155471563339233\n",
      "i: 151400, s: 5801244, Loss: 1.015879511833191\n",
      "Average Episode Reward: 8.749655933140541\n",
      "i: 151600, s: 5816465, Loss: 1.0161807537078857\n",
      "i: 151800, s: 5816465, Loss: 1.0165023803710938\n",
      "Average Episode Reward: 34.611928590725775\n",
      "i: 152000, s: 5833076, Loss: 1.016804814338684\n",
      "i: 152200, s: 5833076, Loss: 1.0171064138412476\n",
      "i: 152400, s: 5833076, Loss: 1.0174074172973633\n",
      "Average Episode Reward: -57.637925978340874\n",
      "i: 152600, s: 5865408, Loss: 1.017696499824524\n",
      "i: 152800, s: 5865408, Loss: 1.0180009603500366\n",
      "Average Episode Reward: 4.327737054528357\n",
      "i: 153000, s: 5878921, Loss: 1.0182700157165527\n",
      "i: 153200, s: 5878921, Loss: 1.0185750722885132\n",
      "i: 153400, s: 5878921, Loss: 1.0188653469085693\n",
      "Average Episode Reward: 19.95985884935969\n",
      "i: 153600, s: 5896704, Loss: 1.0191413164138794\n",
      "i: 153800, s: 5896704, Loss: 1.0194268226623535\n",
      "Average Episode Reward: -16.283907659408914\n",
      "i: 154000, s: 5920505, Loss: 1.0196985006332397\n",
      "i: 154200, s: 5920505, Loss: 1.019997000694275\n",
      "i: 154400, s: 5920505, Loss: 1.0203099250793457\n",
      "Average Episode Reward: 22.88749277513085\n",
      "i: 154600, s: 5935617, Loss: 1.0206001996994019\n",
      "i: 154800, s: 5935617, Loss: 1.0208791494369507\n",
      "Average Episode Reward: -26.454296342207382\n",
      "i: 155000, s: 5957275, Loss: 1.0211747884750366\n",
      "i: 155200, s: 5957275, Loss: 1.0214558839797974\n",
      "i: 155400, s: 5957275, Loss: 1.021735668182373\n",
      "Average Episode Reward: 2.170139369579126\n",
      "i: 155600, s: 5977646, Loss: 1.0220142602920532\n",
      "i: 155800, s: 5977646, Loss: 1.022295594215393\n",
      "Average Episode Reward: -5.853891064833599\n",
      "i: 156000, s: 5994174, Loss: 1.022578239440918\n",
      "i: 156200, s: 5994174, Loss: 1.0228211879730225\n",
      "i: 156400, s: 5994174, Loss: 1.0230861902236938\n",
      "Average Episode Reward: 47.68049840336581\n",
      "i: 156600, s: 6016677, Loss: 1.0233711004257202\n",
      "i: 156800, s: 6016677, Loss: 1.023638129234314\n",
      "Average Episode Reward: -191.10395876933592\n",
      "i: 157000, s: 6047537, Loss: 1.02388596534729\n",
      "i: 157200, s: 6047537, Loss: 1.02415931224823\n",
      "i: 157400, s: 6047537, Loss: 1.0244139432907104\n",
      "Average Episode Reward: -64.54284263954632\n",
      "i: 157600, s: 6070783, Loss: 1.0246739387512207\n",
      "i: 157800, s: 6070783, Loss: 1.0249475240707397\n",
      "Average Episode Reward: -21.24213394693288\n",
      "i: 158000, s: 6088452, Loss: 1.0252267122268677\n",
      "i: 158200, s: 6088452, Loss: 1.0254874229431152\n",
      "i: 158400, s: 6088452, Loss: 1.0257301330566406\n",
      "Average Episode Reward: 56.37286639957492\n",
      "i: 158600, s: 6112373, Loss: 1.025979995727539\n",
      "i: 158800, s: 6112373, Loss: 1.0262174606323242\n",
      "Average Episode Reward: 19.168278110144804\n",
      "i: 159000, s: 6128874, Loss: 1.0264586210250854\n",
      "i: 159200, s: 6128874, Loss: 1.0267152786254883\n",
      "i: 159400, s: 6128874, Loss: 1.0269542932510376\n",
      "Average Episode Reward: -7.268390943960128\n",
      "i: 159600, s: 6145236, Loss: 1.0271885395050049\n",
      "i: 159800, s: 6145236, Loss: 1.0274200439453125\n",
      "Average Episode Reward: 5.4273522151707345\n",
      "i: 160000, s: 6162054, Loss: 1.0276399850845337\n",
      "i: 160200, s: 6162054, Loss: 1.0278609991073608\n",
      "i: 160400, s: 6162054, Loss: 1.028091311454773\n",
      "Average Episode Reward: 27.359225735492664\n",
      "i: 160600, s: 6185954, Loss: 1.0283286571502686\n",
      "i: 160800, s: 6185954, Loss: 1.0285612344741821\n",
      "Average Episode Reward: 18.249402445057026\n",
      "i: 161000, s: 6204874, Loss: 1.0287749767303467\n",
      "i: 161200, s: 6204874, Loss: 1.0290067195892334\n",
      "i: 161400, s: 6204874, Loss: 1.0292240381240845\n",
      "Average Episode Reward: 22.565636671599794\n",
      "i: 161600, s: 6232396, Loss: 1.0294384956359863\n",
      "i: 161800, s: 6232396, Loss: 1.029653549194336\n",
      "Average Episode Reward: -26.05311052110548\n",
      "i: 162000, s: 6251066, Loss: 1.0298957824707031\n",
      "i: 162200, s: 6251066, Loss: 1.0301332473754883\n",
      "i: 162400, s: 6251066, Loss: 1.030362606048584\n",
      "Average Episode Reward: -34.21687104222201\n",
      "i: 162600, s: 6273559, Loss: 1.0305713415145874\n",
      "i: 162800, s: 6273559, Loss: 1.030794382095337\n",
      "Average Episode Reward: -33.81144562047693\n",
      "i: 163000, s: 6289305, Loss: 1.0309985876083374\n",
      "i: 163200, s: 6289305, Loss: 1.0312074422836304\n",
      "i: 163400, s: 6289305, Loss: 1.0314159393310547\n",
      "Average Episode Reward: -4.925182965644005\n",
      "i: 163600, s: 6305109, Loss: 1.0316325426101685\n",
      "i: 163800, s: 6305109, Loss: 1.0318375825881958\n",
      "Average Episode Reward: -48.58743657000854\n",
      "i: 164000, s: 6321175, Loss: 1.0320496559143066\n",
      "i: 164200, s: 6321175, Loss: 1.032246708869934\n",
      "i: 164400, s: 6321175, Loss: 1.0324430465698242\n",
      "Average Episode Reward: 16.302530164901274\n",
      "i: 164600, s: 6344801, Loss: 1.0326405763626099\n",
      "i: 164800, s: 6344801, Loss: 1.032829999923706\n",
      "Average Episode Reward: 15.338349091758424\n",
      "i: 165000, s: 6362998, Loss: 1.0330383777618408\n",
      "i: 165200, s: 6362998, Loss: 1.0332348346710205\n",
      "i: 165400, s: 6362998, Loss: 1.0334241390228271\n",
      "Average Episode Reward: 1.1234572325771905\n",
      "i: 165600, s: 6375975, Loss: 1.0336121320724487\n",
      "i: 165800, s: 6375975, Loss: 1.0338045358657837\n",
      "Average Episode Reward: 16.761527016167744\n",
      "i: 166000, s: 6389751, Loss: 1.0339901447296143\n",
      "i: 166200, s: 6389751, Loss: 1.0341696739196777\n",
      "i: 166400, s: 6389751, Loss: 1.0343506336212158\n",
      "Average Episode Reward: -77.02366809191602\n",
      "i: 166600, s: 6415019, Loss: 1.0345251560211182\n",
      "i: 166800, s: 6415019, Loss: 1.0347063541412354\n",
      "Average Episode Reward: 52.3240576345184\n",
      "i: 167000, s: 6437189, Loss: 1.0348999500274658\n",
      "i: 167200, s: 6437189, Loss: 1.0350756645202637\n",
      "i: 167400, s: 6437189, Loss: 1.035261869430542\n",
      "Average Episode Reward: -15.087694795724406\n",
      "i: 167600, s: 6465759, Loss: 1.0354506969451904\n",
      "i: 167800, s: 6465759, Loss: 1.0356248617172241\n",
      "Average Episode Reward: -1.250872707666376\n",
      "i: 168000, s: 6476959, Loss: 1.0358072519302368\n",
      "i: 168200, s: 6476959, Loss: 1.0359777212142944\n",
      "i: 168400, s: 6476959, Loss: 1.036163330078125\n",
      "Average Episode Reward: 2.5316737354792327\n",
      "i: 168600, s: 6499886, Loss: 1.0363343954086304\n",
      "i: 168800, s: 6499886, Loss: 1.0365008115768433\n",
      "Average Episode Reward: -13.859074473349155\n",
      "i: 169000, s: 6519648, Loss: 1.0366817712783813\n",
      "i: 169200, s: 6519648, Loss: 1.0368602275848389\n",
      "i: 169400, s: 6519648, Loss: 1.0370395183563232\n",
      "Average Episode Reward: -80.50665819104911\n",
      "i: 169600, s: 6540854, Loss: 1.0372041463851929\n",
      "i: 169800, s: 6540854, Loss: 1.0373631715774536\n",
      "Average Episode Reward: 0.36460593709908795\n",
      "i: 170000, s: 6560476, Loss: 1.0375311374664307\n",
      "i: 170200, s: 6560476, Loss: 1.0376967191696167\n",
      "i: 170400, s: 6560476, Loss: 1.0378550291061401\n",
      "Average Episode Reward: -11.160394817082393\n",
      "i: 170600, s: 6578186, Loss: 1.0380098819732666\n",
      "i: 170800, s: 6578186, Loss: 1.0381746292114258\n",
      "Average Episode Reward: 35.364447185292285\n",
      "i: 171000, s: 6595668, Loss: 1.0383392572402954\n",
      "i: 171200, s: 6595668, Loss: 1.0385050773620605\n",
      "i: 171400, s: 6595668, Loss: 1.0386643409729004\n",
      "Average Episode Reward: 43.46346319055564\n",
      "i: 171600, s: 6617168, Loss: 1.0388221740722656\n",
      "i: 171800, s: 6617168, Loss: 1.0389798879623413\n",
      "Average Episode Reward: -39.84835870677498\n",
      "i: 172000, s: 6633548, Loss: 1.0391325950622559\n",
      "i: 172200, s: 6633548, Loss: 1.039284110069275\n",
      "i: 172400, s: 6633548, Loss: 1.0394521951675415\n",
      "Average Episode Reward: 29.705991377652907\n",
      "i: 172600, s: 6649403, Loss: 1.0396101474761963\n",
      "i: 172800, s: 6649403, Loss: 1.0397635698318481\n",
      "Average Episode Reward: -28.3366466929681\n",
      "i: 173000, s: 6666629, Loss: 1.0399121046066284\n",
      "i: 173200, s: 6666629, Loss: 1.0400686264038086\n",
      "i: 173400, s: 6666629, Loss: 1.040218710899353\n",
      "Average Episode Reward: 13.070252635897164\n",
      "i: 173600, s: 6689665, Loss: 1.0403807163238525\n",
      "i: 173800, s: 6689665, Loss: 1.0405362844467163\n",
      "Average Episode Reward: 14.200530120048779\n",
      "i: 174000, s: 6710076, Loss: 1.0406869649887085\n",
      "i: 174200, s: 6710076, Loss: 1.0408447980880737\n",
      "i: 174400, s: 6710076, Loss: 1.0409986972808838\n",
      "Average Episode Reward: 22.424150860064447\n",
      "i: 174600, s: 6728915, Loss: 1.0411502122879028\n",
      "i: 174800, s: 6728915, Loss: 1.0412979125976562\n",
      "Average Episode Reward: 90.50166505608013\n",
      "i: 175000, s: 6758201, Loss: 1.0414432287216187\n",
      "i: 175200, s: 6758201, Loss: 1.0415863990783691\n",
      "i: 175400, s: 6758201, Loss: 1.0417416095733643\n",
      "Average Episode Reward: 30.16075212752383\n",
      "i: 175600, s: 6786561, Loss: 1.041892409324646\n",
      "i: 175800, s: 6786561, Loss: 1.0420647859573364\n",
      "Average Episode Reward: 72.83930262833255\n",
      "i: 176000, s: 6812455, Loss: 1.0422110557556152\n",
      "i: 176200, s: 6812455, Loss: 1.0423587560653687\n",
      "i: 176400, s: 6812455, Loss: 1.0425148010253906\n",
      "Average Episode Reward: 20.824497462851692\n",
      "i: 176600, s: 6829482, Loss: 1.0426673889160156\n",
      "i: 176800, s: 6829482, Loss: 1.042827844619751\n",
      "Average Episode Reward: -35.609445149016594\n",
      "i: 177000, s: 6852751, Loss: 1.0429834127426147\n",
      "i: 177200, s: 6852751, Loss: 1.043123483657837\n",
      "i: 177400, s: 6852751, Loss: 1.0432634353637695\n",
      "Average Episode Reward: 56.087502249004814\n",
      "i: 177600, s: 6874152, Loss: 1.0434112548828125\n",
      "i: 177800, s: 6874152, Loss: 1.043564796447754\n",
      "Average Episode Reward: 14.065559068140853\n",
      "i: 178000, s: 6893342, Loss: 1.0437095165252686\n",
      "i: 178200, s: 6893342, Loss: 1.0438544750213623\n",
      "i: 178400, s: 6893342, Loss: 1.0439947843551636\n",
      "Average Episode Reward: 28.717283128100835\n",
      "i: 178600, s: 6908269, Loss: 1.0441428422927856\n",
      "i: 178800, s: 6908269, Loss: 1.0442864894866943\n",
      "Average Episode Reward: 10.291296874369056\n",
      "i: 179000, s: 6923243, Loss: 1.0444393157958984\n",
      "i: 179200, s: 6923243, Loss: 1.0445778369903564\n",
      "i: 179400, s: 6923243, Loss: 1.0447213649749756\n",
      "Average Episode Reward: 71.87037615817364\n",
      "i: 179600, s: 6950371, Loss: 1.044859528541565\n",
      "i: 179800, s: 6950371, Loss: 1.0449934005737305\n",
      "Average Episode Reward: 2.4776988606973287\n",
      "i: 180000, s: 6971375, Loss: 1.045135259628296\n",
      "i: 180200, s: 6971375, Loss: 1.0452730655670166\n",
      "i: 180400, s: 6971375, Loss: 1.0454163551330566\n",
      "Average Episode Reward: 92.98757815767495\n",
      "i: 180600, s: 6997362, Loss: 1.0455571413040161\n",
      "i: 180800, s: 6997362, Loss: 1.045689344406128\n",
      "Average Episode Reward: 74.62976627021435\n",
      "i: 181000, s: 7025714, Loss: 1.0458215475082397\n",
      "i: 181200, s: 7025714, Loss: 1.0459532737731934\n",
      "i: 181400, s: 7025714, Loss: 1.046087622642517\n",
      "Average Episode Reward: 24.08895763730056\n",
      "i: 181600, s: 7041359, Loss: 1.0462225675582886\n",
      "i: 181800, s: 7041359, Loss: 1.0463515520095825\n",
      "Average Episode Reward: 7.282611791364166\n",
      "i: 182000, s: 7056424, Loss: 1.0464892387390137\n",
      "i: 182200, s: 7056424, Loss: 1.046630859375\n",
      "i: 182400, s: 7056424, Loss: 1.046754002571106\n",
      "Average Episode Reward: -11.388157488888812\n",
      "i: 182600, s: 7071803, Loss: 1.0468825101852417\n",
      "i: 182800, s: 7071803, Loss: 1.0470136404037476\n",
      "Average Episode Reward: 29.91986120711111\n",
      "i: 183000, s: 7089890, Loss: 1.0471328496932983\n",
      "i: 183200, s: 7089890, Loss: 1.047256588935852\n",
      "i: 183400, s: 7089890, Loss: 1.047372817993164\n",
      "Average Episode Reward: 17.271017129727305\n",
      "i: 183600, s: 7103036, Loss: 1.0474853515625\n",
      "i: 183800, s: 7103036, Loss: 1.047613263130188\n",
      "Average Episode Reward: 25.87752578660625\n",
      "i: 184000, s: 7125661, Loss: 1.0477299690246582\n",
      "i: 184200, s: 7125661, Loss: 1.047848105430603\n",
      "i: 184400, s: 7125661, Loss: 1.0479663610458374\n",
      "Average Episode Reward: 9.35711558824329\n",
      "i: 184600, s: 7148706, Loss: 1.0480880737304688\n",
      "i: 184800, s: 7148706, Loss: 1.0482041835784912\n",
      "Average Episode Reward: 26.584568098858853\n",
      "i: 185000, s: 7165305, Loss: 1.0483309030532837\n",
      "i: 185200, s: 7165305, Loss: 1.0484538078308105\n",
      "i: 185400, s: 7165305, Loss: 1.0485745668411255\n",
      "Average Episode Reward: 3.7422855953544762\n",
      "i: 185600, s: 7182049, Loss: 1.0486863851547241\n",
      "i: 185800, s: 7182049, Loss: 1.0487927198410034\n",
      "Average Episode Reward: 47.992389220761936\n",
      "i: 186000, s: 7203009, Loss: 1.0489113330841064\n",
      "i: 186200, s: 7203009, Loss: 1.0490307807922363\n",
      "i: 186400, s: 7203009, Loss: 1.0491517782211304\n",
      "Average Episode Reward: 7.423465096188558\n",
      "i: 186600, s: 7217780, Loss: 1.0492664575576782\n",
      "i: 186800, s: 7217780, Loss: 1.049381136894226\n",
      "Average Episode Reward: 49.05865693848636\n",
      "i: 187000, s: 7239794, Loss: 1.0494964122772217\n",
      "i: 187200, s: 7239794, Loss: 1.0496139526367188\n",
      "i: 187400, s: 7239794, Loss: 1.049730896949768\n",
      "Average Episode Reward: 19.739294332497845\n",
      "i: 187600, s: 7257411, Loss: 1.0498489141464233\n",
      "i: 187800, s: 7257411, Loss: 1.0499666929244995\n",
      "Average Episode Reward: -60.74934083163293\n",
      "i: 188000, s: 7279323, Loss: 1.0500808954238892\n",
      "i: 188200, s: 7279323, Loss: 1.0501996278762817\n",
      "i: 188400, s: 7279323, Loss: 1.0503185987472534\n",
      "Average Episode Reward: 30.843806796266175\n",
      "i: 188600, s: 7301580, Loss: 1.0504286289215088\n",
      "i: 188800, s: 7301580, Loss: 1.0505377054214478\n",
      "Average Episode Reward: 25.14189581958632\n",
      "i: 189000, s: 7318580, Loss: 1.0506587028503418\n",
      "i: 189200, s: 7318580, Loss: 1.0507720708847046\n",
      "i: 189400, s: 7318580, Loss: 1.0508748292922974\n",
      "Average Episode Reward: 62.54735560701138\n",
      "i: 189600, s: 7343174, Loss: 1.050982117652893\n",
      "i: 189800, s: 7343174, Loss: 1.0510923862457275\n",
      "Average Episode Reward: 12.937329580567269\n",
      "i: 190000, s: 7357085, Loss: 1.051188349723816\n",
      "i: 190200, s: 7357085, Loss: 1.0512977838516235\n",
      "i: 190400, s: 7357085, Loss: 1.0514028072357178\n",
      "Average Episode Reward: -0.6236898366500034\n",
      "i: 190600, s: 7370215, Loss: 1.0515145063400269\n",
      "i: 190800, s: 7370215, Loss: 1.051619052886963\n",
      "Average Episode Reward: 39.13191822496234\n",
      "i: 191000, s: 7387249, Loss: 1.0517218112945557\n",
      "i: 191200, s: 7387249, Loss: 1.051827311515808\n",
      "i: 191400, s: 7387249, Loss: 1.0519269704818726\n",
      "Average Episode Reward: 37.69571951715509\n",
      "i: 191600, s: 7402174, Loss: 1.0520328283309937\n",
      "i: 191800, s: 7402174, Loss: 1.0521347522735596\n",
      "Average Episode Reward: 51.11572330733771\n",
      "i: 192000, s: 7421843, Loss: 1.0522336959838867\n",
      "i: 192200, s: 7421843, Loss: 1.0523369312286377\n",
      "i: 192400, s: 7421843, Loss: 1.052437424659729\n",
      "Average Episode Reward: -8.872247209766899\n",
      "i: 192600, s: 7437375, Loss: 1.0525357723236084\n",
      "i: 192800, s: 7437375, Loss: 1.052636742591858\n",
      "Average Episode Reward: 53.264421007613784\n",
      "i: 193000, s: 7454016, Loss: 1.0527294874191284\n",
      "i: 193200, s: 7454016, Loss: 1.0528255701065063\n",
      "i: 193400, s: 7454016, Loss: 1.0529215335845947\n",
      "Average Episode Reward: 57.0080399501189\n",
      "i: 193600, s: 7472548, Loss: 1.0530146360397339\n",
      "i: 193800, s: 7472548, Loss: 1.053105354309082\n",
      "Average Episode Reward: 24.865767695240155\n",
      "i: 194000, s: 7486033, Loss: 1.0531980991363525\n",
      "i: 194200, s: 7486033, Loss: 1.053293228149414\n",
      "i: 194400, s: 7486033, Loss: 1.0533759593963623\n",
      "Average Episode Reward: 20.87113412381427\n",
      "i: 194600, s: 7511718, Loss: 1.0534683465957642\n",
      "i: 194800, s: 7511718, Loss: 1.0535643100738525\n",
      "Average Episode Reward: 77.008143402317\n",
      "i: 195000, s: 7532281, Loss: 1.0536515712738037\n",
      "i: 195200, s: 7532281, Loss: 1.0537426471710205\n",
      "i: 195400, s: 7532281, Loss: 1.0538339614868164\n",
      "Average Episode Reward: 31.140458282907936\n",
      "i: 195600, s: 7546579, Loss: 1.0539268255233765\n",
      "i: 195800, s: 7546579, Loss: 1.054023027420044\n",
      "Average Episode Reward: 51.8222798246143\n",
      "i: 196000, s: 7573026, Loss: 1.0541166067123413\n",
      "i: 196200, s: 7573026, Loss: 1.0542116165161133\n",
      "i: 196400, s: 7573026, Loss: 1.0543056726455688\n",
      "Average Episode Reward: 51.24940307567833\n",
      "i: 196600, s: 7591456, Loss: 1.0544013977050781\n",
      "i: 196800, s: 7591456, Loss: 1.054497241973877\n",
      "Average Episode Reward: 34.73214141453511\n",
      "i: 197000, s: 7609852, Loss: 1.054592490196228\n",
      "i: 197200, s: 7609852, Loss: 1.0546857118606567\n",
      "i: 197400, s: 7609852, Loss: 1.0547786951065063\n",
      "Average Episode Reward: -1.0747716488905537\n",
      "i: 197600, s: 7628839, Loss: 1.0548688173294067\n",
      "i: 197800, s: 7628839, Loss: 1.0549535751342773\n",
      "Average Episode Reward: 0.4117508961041547\n",
      "i: 198000, s: 7643532, Loss: 1.055039882659912\n",
      "i: 198200, s: 7643532, Loss: 1.0551292896270752\n",
      "i: 198400, s: 7643532, Loss: 1.0552148818969727\n",
      "Average Episode Reward: 32.56829822187614\n",
      "i: 198600, s: 7659501, Loss: 1.055312156677246\n",
      "i: 198800, s: 7659501, Loss: 1.0554049015045166\n",
      "Average Episode Reward: -41.28309890228593\n",
      "i: 199000, s: 7686889, Loss: 1.0554996728897095\n",
      "i: 199200, s: 7686889, Loss: 1.0555914640426636\n",
      "i: 199400, s: 7686889, Loss: 1.0556827783584595\n",
      "Average Episode Reward: -16.580098327253225\n",
      "i: 199600, s: 7708074, Loss: 1.055770754814148\n",
      "i: 199800, s: 7708074, Loss: 1.055856466293335\n",
      "Average Episode Reward: 7.271399605209606\n",
      "i: 200000, s: 7722458, Loss: 1.0559455156326294\n",
      "i: 200200, s: 7722458, Loss: 1.0560282468795776\n",
      "i: 200400, s: 7722458, Loss: 1.056117296218872\n",
      "Average Episode Reward: -3.1179543916354864\n",
      "i: 200600, s: 7741894, Loss: 1.0562039613723755\n",
      "i: 200800, s: 7741894, Loss: 1.056298851966858\n",
      "Average Episode Reward: 35.33373638111897\n",
      "i: 201000, s: 7757265, Loss: 1.0563814640045166\n",
      "i: 201200, s: 7757265, Loss: 1.0564674139022827\n",
      "i: 201400, s: 7757265, Loss: 1.0565563440322876\n",
      "Average Episode Reward: 16.26418724209884\n",
      "i: 201600, s: 7774451, Loss: 1.056640386581421\n",
      "i: 201800, s: 7774451, Loss: 1.0567182302474976\n",
      "Average Episode Reward: -43.86083412707953\n",
      "i: 202000, s: 7791000, Loss: 1.0567952394485474\n",
      "i: 202200, s: 7791000, Loss: 1.0568830966949463\n",
      "i: 202400, s: 7791000, Loss: 1.0569700002670288\n",
      "Average Episode Reward: 21.625821106685702\n",
      "i: 202600, s: 7806586, Loss: 1.0570566654205322\n",
      "i: 202800, s: 7806586, Loss: 1.0571345090866089\n",
      "Average Episode Reward: -46.94251403106951\n",
      "i: 203000, s: 7825352, Loss: 1.0572154521942139\n",
      "i: 203200, s: 7825352, Loss: 1.0572938919067383\n",
      "i: 203400, s: 7825352, Loss: 1.0573768615722656\n",
      "Average Episode Reward: 33.96043645333583\n",
      "i: 203600, s: 7839504, Loss: 1.0574560165405273\n",
      "i: 203800, s: 7839504, Loss: 1.0575343370437622\n",
      "Average Episode Reward: 47.29313028803242\n",
      "i: 204000, s: 7858035, Loss: 1.0576144456863403\n",
      "i: 204200, s: 7858035, Loss: 1.0576939582824707\n",
      "i: 204400, s: 7858035, Loss: 1.0577722787857056\n",
      "Average Episode Reward: 11.53040347488872\n",
      "i: 204600, s: 7875139, Loss: 1.0578505992889404\n",
      "i: 204800, s: 7875139, Loss: 1.0579296350479126\n",
      "Average Episode Reward: -85.93251764432675\n",
      "i: 205000, s: 7896904, Loss: 1.0580079555511475\n",
      "i: 205200, s: 7896904, Loss: 1.058083176612854\n",
      "i: 205400, s: 7896904, Loss: 1.058156132698059\n",
      "Average Episode Reward: -20.420954636707584\n",
      "i: 205600, s: 7918450, Loss: 1.0582302808761597\n",
      "i: 205800, s: 7918450, Loss: 1.0582988262176514\n",
      "Average Episode Reward: 5.901448256398867\n",
      "i: 206000, s: 7940898, Loss: 1.0583727359771729\n",
      "i: 206200, s: 7940898, Loss: 1.0584462881088257\n",
      "i: 206400, s: 7940898, Loss: 1.058520793914795\n",
      "Average Episode Reward: -32.647436422050276\n",
      "i: 206600, s: 7958601, Loss: 1.0585896968841553\n",
      "i: 206800, s: 7958601, Loss: 1.0586634874343872\n",
      "Average Episode Reward: -1.912993015760153\n",
      "i: 207000, s: 7977026, Loss: 1.0587328672409058\n",
      "i: 207200, s: 7977026, Loss: 1.0587968826293945\n",
      "i: 207400, s: 7977026, Loss: 1.058862328529358\n",
      "Average Episode Reward: 0.759564488580812\n",
      "i: 207600, s: 7998338, Loss: 1.058925747871399\n",
      "i: 207800, s: 7998338, Loss: 1.0589927434921265\n",
      "Average Episode Reward: 63.16029753860446\n",
      "i: 208000, s: 8020524, Loss: 1.0590590238571167\n",
      "i: 208200, s: 8020524, Loss: 1.059126853942871\n",
      "i: 208400, s: 8020524, Loss: 1.0591932535171509\n",
      "Average Episode Reward: 14.960591700491516\n",
      "i: 208600, s: 8035885, Loss: 1.0592559576034546\n",
      "i: 208800, s: 8035885, Loss: 1.0593228340148926\n",
      "Average Episode Reward: -74.159897095699\n",
      "i: 209000, s: 8057986, Loss: 1.0593838691711426\n",
      "i: 209200, s: 8057986, Loss: 1.059447169303894\n",
      "i: 209400, s: 8057986, Loss: 1.0595098733901978\n",
      "Average Episode Reward: 19.098498629954303\n",
      "i: 209600, s: 8071976, Loss: 1.059576153755188\n",
      "i: 209800, s: 8071976, Loss: 1.0596481561660767\n",
      "Average Episode Reward: 29.064152610447273\n",
      "i: 210000, s: 8090579, Loss: 1.0597110986709595\n",
      "i: 210200, s: 8090579, Loss: 1.059775710105896\n",
      "i: 210400, s: 8090579, Loss: 1.0598359107971191\n",
      "Average Episode Reward: 19.762624761589024\n",
      "i: 210600, s: 8111285, Loss: 1.0599019527435303\n",
      "i: 210800, s: 8111285, Loss: 1.0599592924118042\n",
      "Average Episode Reward: -3.847891997588674\n",
      "i: 211000, s: 8122677, Loss: 1.0600184202194214\n",
      "i: 211200, s: 8122677, Loss: 1.060076355934143\n",
      "i: 211400, s: 8122677, Loss: 1.0601351261138916\n",
      "Average Episode Reward: -8.719180495907246\n",
      "i: 211600, s: 8144352, Loss: 1.0601941347122192\n",
      "i: 211800, s: 8144352, Loss: 1.060250997543335\n",
      "Average Episode Reward: 24.131485519368447\n",
      "i: 212000, s: 8158115, Loss: 1.0603049993515015\n",
      "i: 212200, s: 8158115, Loss: 1.0603631734848022\n",
      "i: 212400, s: 8158115, Loss: 1.0604220628738403\n",
      "Average Episode Reward: 32.077709853059844\n",
      "i: 212600, s: 8172193, Loss: 1.060484766960144\n",
      "i: 212800, s: 8172193, Loss: 1.0605419874191284\n",
      "Average Episode Reward: 15.08681385941411\n",
      "i: 213000, s: 8185844, Loss: 1.060607671737671\n",
      "i: 213200, s: 8185844, Loss: 1.0606656074523926\n",
      "i: 213400, s: 8185844, Loss: 1.0607144832611084\n",
      "Average Episode Reward: 25.569804214690343\n",
      "i: 213600, s: 8204898, Loss: 1.0607714653015137\n",
      "i: 213800, s: 8204898, Loss: 1.060821294784546\n",
      "Average Episode Reward: 21.700627019097684\n",
      "i: 214000, s: 8222360, Loss: 1.0608808994293213\n",
      "i: 214200, s: 8222360, Loss: 1.0609384775161743\n",
      "i: 214400, s: 8222360, Loss: 1.0609976053237915\n",
      "Average Episode Reward: -32.180719416344296\n",
      "i: 214600, s: 8242147, Loss: 1.0610536336898804\n",
      "i: 214800, s: 8242147, Loss: 1.0611114501953125\n",
      "Average Episode Reward: 8.84101563974185\n",
      "i: 215000, s: 8256635, Loss: 1.0611679553985596\n",
      "i: 215200, s: 8256635, Loss: 1.0612181425094604\n",
      "i: 215400, s: 8256635, Loss: 1.0612726211547852\n",
      "Average Episode Reward: 31.268228604311002\n",
      "i: 215600, s: 8276497, Loss: 1.0613209009170532\n",
      "i: 215800, s: 8276497, Loss: 1.0613737106323242\n",
      "Average Episode Reward: 12.70285327158655\n",
      "i: 216000, s: 8299601, Loss: 1.0614274740219116\n",
      "i: 216200, s: 8299601, Loss: 1.0614826679229736\n",
      "i: 216400, s: 8299601, Loss: 1.0615322589874268\n",
      "Average Episode Reward: -8.223297951087657\n",
      "i: 216600, s: 8323601, Loss: 1.0615885257720947\n",
      "i: 216800, s: 8323601, Loss: 1.0616405010223389\n",
      "Average Episode Reward: -21.01025737710484\n",
      "i: 217000, s: 8347002, Loss: 1.0616955757141113\n",
      "i: 217200, s: 8347002, Loss: 1.0617493391036987\n",
      "i: 217400, s: 8347002, Loss: 1.06179678440094\n",
      "Average Episode Reward: -4.8244343064050925\n",
      "i: 217600, s: 8367176, Loss: 1.0618456602096558\n",
      "i: 217800, s: 8367176, Loss: 1.061896800994873\n",
      "Average Episode Reward: 24.28859950526371\n",
      "i: 218000, s: 8383766, Loss: 1.0619463920593262\n",
      "i: 218200, s: 8383766, Loss: 1.0619921684265137\n",
      "i: 218400, s: 8383766, Loss: 1.062047004699707\n",
      "Average Episode Reward: 23.111471421090023\n",
      "i: 218600, s: 8404064, Loss: 1.062096357345581\n",
      "i: 218800, s: 8404064, Loss: 1.0621391534805298\n",
      "Average Episode Reward: -17.367503885797753\n",
      "i: 219000, s: 8422392, Loss: 1.062187671661377\n",
      "i: 219200, s: 8422392, Loss: 1.0622447729110718\n",
      "i: 219400, s: 8422392, Loss: 1.0622990131378174\n",
      "Average Episode Reward: 8.006089631710855\n",
      "i: 219600, s: 8435566, Loss: 1.0623499155044556\n",
      "i: 219800, s: 8435566, Loss: 1.0624024868011475\n",
      "Average Episode Reward: -18.07732840615732\n",
      "i: 220000, s: 8449448, Loss: 1.0624487400054932\n",
      "i: 220200, s: 8449448, Loss: 1.062490701675415\n",
      "i: 220400, s: 8449448, Loss: 1.0625379085540771\n",
      "Average Episode Reward: 24.33359658459242\n",
      "i: 220600, s: 8465281, Loss: 1.0625840425491333\n",
      "i: 220800, s: 8465281, Loss: 1.0626335144042969\n",
      "Average Episode Reward: 17.549563297093954\n",
      "i: 221000, s: 8486152, Loss: 1.0626786947250366\n",
      "i: 221200, s: 8486152, Loss: 1.0627169609069824\n",
      "i: 221400, s: 8486152, Loss: 1.0627610683441162\n",
      "Average Episode Reward: 30.820695352680033\n",
      "i: 221600, s: 8503659, Loss: 1.0627994537353516\n",
      "i: 221800, s: 8503659, Loss: 1.0628424882888794\n",
      "Average Episode Reward: -37.56064105872272\n",
      "i: 222000, s: 8522811, Loss: 1.0628827810287476\n",
      "i: 222200, s: 8522811, Loss: 1.06292724609375\n",
      "i: 222400, s: 8522811, Loss: 1.062968134880066\n",
      "Average Episode Reward: 26.7132173018336\n",
      "i: 222600, s: 8537289, Loss: 1.0630085468292236\n",
      "i: 222800, s: 8537289, Loss: 1.063050389289856\n",
      "Average Episode Reward: 21.93104500894503\n",
      "i: 223000, s: 8550787, Loss: 1.0630881786346436\n",
      "i: 223200, s: 8550787, Loss: 1.0631271600723267\n",
      "i: 223400, s: 8550787, Loss: 1.0631718635559082\n",
      "Average Episode Reward: 59.390714375958666\n",
      "i: 223600, s: 8573963, Loss: 1.0632102489471436\n",
      "i: 223800, s: 8573963, Loss: 1.0632517337799072\n",
      "Average Episode Reward: 33.904029042344895\n",
      "i: 224000, s: 8592049, Loss: 1.063292145729065\n",
      "i: 224200, s: 8592049, Loss: 1.063326358795166\n",
      "i: 224400, s: 8592049, Loss: 1.0633652210235596\n",
      "Average Episode Reward: 30.495839733512284\n",
      "i: 224600, s: 8609572, Loss: 1.063401222229004\n",
      "i: 224800, s: 8609572, Loss: 1.0634404420852661\n",
      "Average Episode Reward: -8.86458533475888\n",
      "i: 225000, s: 8625205, Loss: 1.0634797811508179\n",
      "i: 225200, s: 8625205, Loss: 1.0635180473327637\n",
      "i: 225400, s: 8625205, Loss: 1.06355619430542\n",
      "Average Episode Reward: -12.728285928787319\n",
      "i: 225600, s: 8648704, Loss: 1.063593864440918\n",
      "i: 225800, s: 8648704, Loss: 1.063634991645813\n",
      "Average Episode Reward: -28.36556019957338\n",
      "i: 226000, s: 8666157, Loss: 1.0636738538742065\n",
      "i: 226200, s: 8666157, Loss: 1.0637143850326538\n",
      "i: 226400, s: 8666157, Loss: 1.063754677772522\n",
      "Average Episode Reward: 42.660629184655136\n",
      "i: 226600, s: 8684749, Loss: 1.0637898445129395\n",
      "i: 226800, s: 8684749, Loss: 1.0638338327407837\n",
      "Average Episode Reward: 12.770858981803126\n",
      "i: 227000, s: 8697565, Loss: 1.0638715028762817\n",
      "i: 227200, s: 8697565, Loss: 1.0639082193374634\n",
      "i: 227400, s: 8697565, Loss: 1.063949704170227\n",
      "Average Episode Reward: -51.774862138354955\n",
      "i: 227600, s: 8714566, Loss: 1.0639909505844116\n",
      "i: 227800, s: 8714566, Loss: 1.0640288591384888\n",
      "Average Episode Reward: -135.4960350651457\n",
      "i: 228000, s: 8735459, Loss: 1.0640684366226196\n",
      "i: 228200, s: 8735459, Loss: 1.06410551071167\n",
      "i: 228400, s: 8735459, Loss: 1.0641425848007202\n",
      "Average Episode Reward: -15.616523147176867\n",
      "i: 228600, s: 8757239, Loss: 1.0641803741455078\n",
      "i: 228800, s: 8757239, Loss: 1.064219355583191\n",
      "Average Episode Reward: 0.347821781832624\n",
      "i: 229000, s: 8774641, Loss: 1.0642555952072144\n",
      "i: 229200, s: 8774641, Loss: 1.0642906427383423\n",
      "i: 229400, s: 8774641, Loss: 1.064324975013733\n",
      "Average Episode Reward: -5.318199583891403\n",
      "i: 229600, s: 8793120, Loss: 1.0643620491027832\n",
      "i: 229800, s: 8793120, Loss: 1.064400315284729\n",
      "Average Episode Reward: 17.768386430109402\n",
      "i: 230000, s: 8807101, Loss: 1.0644330978393555\n",
      "i: 230200, s: 8807101, Loss: 1.0644662380218506\n",
      "i: 230400, s: 8807101, Loss: 1.0645008087158203\n",
      "Average Episode Reward: 20.965565390168063\n",
      "i: 230600, s: 8820321, Loss: 1.06453537940979\n",
      "i: 230800, s: 8820321, Loss: 1.0645716190338135\n",
      "Average Episode Reward: 29.251052369022048\n",
      "i: 231000, s: 8835386, Loss: 1.0646047592163086\n",
      "i: 231200, s: 8835386, Loss: 1.0646404027938843\n",
      "i: 231400, s: 8835386, Loss: 1.064677119255066\n",
      "Average Episode Reward: -48.59174072199862\n",
      "i: 231600, s: 8851758, Loss: 1.0647122859954834\n",
      "i: 231800, s: 8851758, Loss: 1.06475031375885\n",
      "Average Episode Reward: 19.077605143970963\n",
      "i: 232000, s: 8865958, Loss: 1.0647832155227661\n",
      "i: 232200, s: 8865958, Loss: 1.064818024635315\n",
      "i: 232400, s: 8865958, Loss: 1.0648561716079712\n",
      "Average Episode Reward: -14.41413214263925\n",
      "i: 232600, s: 8882063, Loss: 1.0648877620697021\n",
      "i: 232800, s: 8882063, Loss: 1.064916729927063\n",
      "Average Episode Reward: -38.17269849527859\n",
      "i: 233000, s: 8905728, Loss: 1.0649504661560059\n",
      "i: 233200, s: 8905728, Loss: 1.0649850368499756\n",
      "i: 233400, s: 8905728, Loss: 1.0650204420089722\n",
      "Average Episode Reward: 12.74375725302923\n",
      "i: 233600, s: 8921196, Loss: 1.0650475025177002\n",
      "i: 233800, s: 8921196, Loss: 1.0650759935379028\n",
      "Average Episode Reward: -15.764888700699917\n",
      "i: 234000, s: 8940233, Loss: 1.0651085376739502\n",
      "i: 234200, s: 8940233, Loss: 1.0651369094848633\n",
      "i: 234400, s: 8940233, Loss: 1.0651718378067017\n",
      "Average Episode Reward: 23.214436628585712\n",
      "i: 234600, s: 8953201, Loss: 1.0652003288269043\n",
      "i: 234800, s: 8953201, Loss: 1.065233826637268\n",
      "Average Episode Reward: 9.31023604779167\n",
      "i: 235000, s: 8967696, Loss: 1.065262794494629\n",
      "i: 235200, s: 8967696, Loss: 1.065297245979309\n",
      "i: 235400, s: 8967696, Loss: 1.065333366394043\n",
      "Average Episode Reward: 1.4300581621863049\n",
      "i: 235600, s: 8985024, Loss: 1.065361738204956\n",
      "i: 235800, s: 8985024, Loss: 1.06538987159729\n",
      "Average Episode Reward: 26.17842118963565\n",
      "i: 236000, s: 9003959, Loss: 1.0654208660125732\n",
      "i: 236200, s: 9003959, Loss: 1.0654512643814087\n",
      "i: 236400, s: 9003959, Loss: 1.0654819011688232\n",
      "Average Episode Reward: 7.6538679623314625\n",
      "i: 236600, s: 9025674, Loss: 1.0655053853988647\n",
      "i: 236800, s: 9025674, Loss: 1.0655324459075928\n",
      "Average Episode Reward: 12.196100521057911\n",
      "i: 237000, s: 9039298, Loss: 1.0655560493469238\n",
      "i: 237200, s: 9039298, Loss: 1.065584421157837\n",
      "i: 237400, s: 9039298, Loss: 1.0656166076660156\n",
      "Average Episode Reward: 26.184545311327437\n",
      "i: 237600, s: 9058727, Loss: 1.0656450986862183\n",
      "i: 237800, s: 9058727, Loss: 1.0656765699386597\n",
      "Average Episode Reward: 0.08412216233198506\n",
      "i: 238000, s: 9078399, Loss: 1.06570565700531\n",
      "i: 238200, s: 9078399, Loss: 1.0657285451889038\n",
      "i: 238400, s: 9078399, Loss: 1.0657552480697632\n",
      "Average Episode Reward: 0.6239064926058413\n",
      "i: 238600, s: 9092125, Loss: 1.0657833814620972\n",
      "i: 238800, s: 9092125, Loss: 1.0658113956451416\n",
      "Average Episode Reward: 55.30259150644586\n",
      "i: 239000, s: 9111739, Loss: 1.0658371448516846\n",
      "i: 239200, s: 9111739, Loss: 1.0658681392669678\n",
      "i: 239400, s: 9111739, Loss: 1.065894365310669\n",
      "Average Episode Reward: 25.843678609505385\n",
      "i: 239600, s: 9134307, Loss: 1.0659242868423462\n",
      "i: 239800, s: 9134307, Loss: 1.0659514665603638\n",
      "Average Episode Reward: 12.970991260419616\n",
      "i: 240000, s: 9152515, Loss: 1.0659793615341187\n",
      "i: 240200, s: 9152515, Loss: 1.0660035610198975\n",
      "i: 240400, s: 9152515, Loss: 1.0660285949707031\n",
      "Average Episode Reward: 1.4248959126162788\n",
      "i: 240600, s: 9170834, Loss: 1.0660524368286133\n",
      "i: 240800, s: 9170834, Loss: 1.0660810470581055\n",
      "Average Episode Reward: 59.84899806786327\n",
      "i: 241000, s: 9195601, Loss: 1.0661126375198364\n",
      "i: 241200, s: 9195601, Loss: 1.0661365985870361\n",
      "i: 241400, s: 9195601, Loss: 1.0661653280258179\n",
      "Average Episode Reward: 25.250678010412322\n",
      "i: 241600, s: 9213860, Loss: 1.0661919116973877\n",
      "i: 241800, s: 9213860, Loss: 1.0662152767181396\n",
      "Average Episode Reward: -24.366994592171217\n",
      "i: 242000, s: 9241750, Loss: 1.066247582435608\n",
      "i: 242200, s: 9241750, Loss: 1.0662778615951538\n",
      "i: 242400, s: 9241750, Loss: 1.0663036108016968\n",
      "Average Episode Reward: 35.54244454710518\n",
      "i: 242600, s: 9258297, Loss: 1.0663295984268188\n",
      "i: 242800, s: 9258297, Loss: 1.0663543939590454\n",
      "Average Episode Reward: 62.894941004288995\n",
      "i: 243000, s: 9276797, Loss: 1.0663820505142212\n",
      "i: 243200, s: 9276797, Loss: 1.0664054155349731\n",
      "i: 243400, s: 9276797, Loss: 1.0664315223693848\n",
      "Average Episode Reward: 27.459816881724873\n",
      "i: 243600, s: 9290912, Loss: 1.0664563179016113\n",
      "i: 243800, s: 9290912, Loss: 1.066479206085205\n",
      "Average Episode Reward: 23.041259429577114\n",
      "i: 244000, s: 9313145, Loss: 1.0665028095245361\n",
      "i: 244200, s: 9313145, Loss: 1.066524863243103\n",
      "i: 244400, s: 9313145, Loss: 1.0665476322174072\n",
      "Average Episode Reward: 26.1206284249031\n",
      "i: 244600, s: 9330515, Loss: 1.066572904586792\n",
      "i: 244800, s: 9330515, Loss: 1.0665959119796753\n",
      "Average Episode Reward: 48.604895121734096\n",
      "i: 245000, s: 9352293, Loss: 1.0666199922561646\n",
      "i: 245200, s: 9352293, Loss: 1.0666403770446777\n",
      "i: 245400, s: 9352293, Loss: 1.066665768623352\n",
      "Average Episode Reward: 37.46736556221508\n",
      "i: 245600, s: 9367276, Loss: 1.066684365272522\n",
      "i: 245800, s: 9367276, Loss: 1.0667142868041992\n",
      "Average Episode Reward: 45.67049833220247\n",
      "i: 246000, s: 9383582, Loss: 1.0667413473129272\n",
      "i: 246200, s: 9383582, Loss: 1.0667661428451538\n",
      "i: 246400, s: 9383582, Loss: 1.0667879581451416\n",
      "Average Episode Reward: 41.92995741869152\n",
      "i: 246600, s: 9398423, Loss: 1.0668106079101562\n",
      "i: 246800, s: 9398423, Loss: 1.0668326616287231\n",
      "Average Episode Reward: -45.44425440925374\n",
      "i: 247000, s: 9417928, Loss: 1.0668485164642334\n",
      "i: 247200, s: 9417928, Loss: 1.066864013671875\n",
      "i: 247400, s: 9417928, Loss: 1.0668894052505493\n",
      "Average Episode Reward: -26.529433786522205\n",
      "i: 247600, s: 9441340, Loss: 1.0669126510620117\n",
      "i: 247800, s: 9441340, Loss: 1.0669326782226562\n",
      "Average Episode Reward: 0.46126307887350065\n",
      "i: 248000, s: 9465232, Loss: 1.066954255104065\n",
      "i: 248200, s: 9465232, Loss: 1.0669779777526855\n",
      "i: 248400, s: 9465232, Loss: 1.0669984817504883\n",
      "Average Episode Reward: 9.304751101748106\n",
      "i: 248600, s: 9484435, Loss: 1.067022681236267\n",
      "i: 248800, s: 9484435, Loss: 1.0670462846755981\n",
      "Average Episode Reward: 41.809757620505586\n",
      "i: 249000, s: 9500596, Loss: 1.0670723915100098\n",
      "i: 249200, s: 9500596, Loss: 1.067092776298523\n",
      "i: 249400, s: 9500596, Loss: 1.0671162605285645\n",
      "Average Episode Reward: 31.966766314662955\n",
      "i: 249600, s: 9532713, Loss: 1.0671355724334717\n",
      "i: 249800, s: 9532713, Loss: 1.0671569108963013\n",
      "Average Episode Reward: 70.63590894961422\n",
      "i: 250000, s: 9557181, Loss: 1.0671800374984741\n",
      "i: 250200, s: 9557181, Loss: 1.0672008991241455\n",
      "i: 250400, s: 9557181, Loss: 1.067220687866211\n",
      "Average Episode Reward: 14.784521866515188\n",
      "i: 250600, s: 9571504, Loss: 1.067244052886963\n",
      "i: 250800, s: 9571504, Loss: 1.0672634840011597\n",
      "Average Episode Reward: 23.707174607038315\n",
      "i: 251000, s: 9589460, Loss: 1.0672801733016968\n",
      "i: 251200, s: 9589460, Loss: 1.067299723625183\n",
      "i: 251400, s: 9589460, Loss: 1.0673210620880127\n",
      "Average Episode Reward: 7.179748306962431\n",
      "i: 251600, s: 9607248, Loss: 1.0673447847366333\n",
      "i: 251800, s: 9607248, Loss: 1.067366600036621\n",
      "Average Episode Reward: 28.135754061375334\n",
      "i: 252000, s: 9623745, Loss: 1.067388892173767\n",
      "i: 252200, s: 9623745, Loss: 1.0674054622650146\n",
      "i: 252400, s: 9623745, Loss: 1.067421555519104\n",
      "Average Episode Reward: 86.44495057812335\n",
      "i: 252600, s: 9650683, Loss: 1.0674386024475098\n",
      "i: 252800, s: 9650683, Loss: 1.0674558877944946\n",
      "Average Episode Reward: 5.272262265876845\n",
      "i: 253000, s: 9664583, Loss: 1.0674744844436646\n",
      "i: 253200, s: 9664583, Loss: 1.067492127418518\n",
      "i: 253400, s: 9664583, Loss: 1.0675075054168701\n",
      "Average Episode Reward: 29.69469980810862\n",
      "i: 253600, s: 9690627, Loss: 1.0675289630889893\n",
      "i: 253800, s: 9690627, Loss: 1.0675421953201294\n",
      "Average Episode Reward: -47.85873366712874\n",
      "i: 254000, s: 9714542, Loss: 1.0675606727600098\n",
      "i: 254200, s: 9714542, Loss: 1.0675791501998901\n",
      "i: 254400, s: 9714542, Loss: 1.0675956010818481\n",
      "Average Episode Reward: 22.050050959555364\n",
      "i: 254600, s: 9727051, Loss: 1.0676153898239136\n",
      "i: 254800, s: 9727051, Loss: 1.067631721496582\n",
      "Average Episode Reward: 20.949219410963195\n",
      "i: 255000, s: 9746650, Loss: 1.067650556564331\n",
      "i: 255200, s: 9746650, Loss: 1.0676705837249756\n",
      "i: 255400, s: 9746650, Loss: 1.0676907300949097\n",
      "Average Episode Reward: 53.67602241314123\n",
      "i: 255600, s: 9762957, Loss: 1.0677067041397095\n",
      "i: 255800, s: 9762957, Loss: 1.0677223205566406\n",
      "Average Episode Reward: 28.820773834937714\n",
      "i: 256000, s: 9785261, Loss: 1.067739486694336\n",
      "i: 256200, s: 9785261, Loss: 1.0677539110183716\n",
      "i: 256400, s: 9785261, Loss: 1.0677739381790161\n",
      "Average Episode Reward: -6.703884914516712\n",
      "i: 256600, s: 9811242, Loss: 1.0677882432937622\n",
      "i: 256800, s: 9811242, Loss: 1.0678082704544067\n",
      "Average Episode Reward: 9.284981480220996\n",
      "i: 257000, s: 9826554, Loss: 1.0678259134292603\n",
      "i: 257200, s: 9826554, Loss: 1.0678399801254272\n",
      "i: 257400, s: 9826554, Loss: 1.0678561925888062\n",
      "Average Episode Reward: 24.56368497105193\n",
      "i: 257600, s: 9849364, Loss: 1.0678750276565552\n",
      "i: 257800, s: 9849364, Loss: 1.0678932666778564\n",
      "Average Episode Reward: 29.81756672763858\n",
      "i: 258000, s: 9871599, Loss: 1.067911148071289\n",
      "i: 258200, s: 9871599, Loss: 1.0679289102554321\n",
      "i: 258400, s: 9871599, Loss: 1.0679442882537842\n",
      "Average Episode Reward: -59.57700468987858\n",
      "i: 258600, s: 9888368, Loss: 1.0679608583450317\n",
      "i: 258800, s: 9888368, Loss: 1.0679818391799927\n",
      "Average Episode Reward: -14.30979654403094\n",
      "i: 259000, s: 9915466, Loss: 1.0680004358291626\n",
      "i: 259200, s: 9915466, Loss: 1.068015694618225\n",
      "i: 259400, s: 9915466, Loss: 1.068032145500183\n",
      "Average Episode Reward: -58.72864298943117\n",
      "i: 259600, s: 9942069, Loss: 1.0680477619171143\n",
      "i: 259800, s: 9942069, Loss: 1.0680612325668335\n",
      "Average Episode Reward: 19.629593166802273\n",
      "i: 260000, s: 9962730, Loss: 1.0680781602859497\n",
      "i: 260200, s: 9962730, Loss: 1.06809401512146\n",
      "i: 260400, s: 9962730, Loss: 1.068110704421997\n",
      "Average Episode Reward: 13.021959610141732\n",
      "i: 260600, s: 9979610, Loss: 1.0681263208389282\n",
      "i: 260800, s: 9979610, Loss: 1.0681427717208862\n",
      "Average Episode Reward: -25.447609774157353\n",
      "i: 261000, s: 9997300, Loss: 1.068155288696289\n",
      "i: 261200, s: 9997300, Loss: 1.0681694746017456\n",
      "i: 261400, s: 9997300, Loss: 1.0681860446929932\n",
      "Average Episode Reward: -51.39546318119112\n",
      "i: 261600, s: 10018072, Loss: 1.0682001113891602\n",
      "i: 261800, s: 10018072, Loss: 1.0682181119918823\n",
      "Average Episode Reward: 19.789783853452338\n",
      "i: 262000, s: 10036307, Loss: 1.0682326555252075\n",
      "i: 262200, s: 10036307, Loss: 1.0682507753372192\n",
      "i: 262400, s: 10036307, Loss: 1.0682636499404907\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-59b7a73d0385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         trajectories, mean_reward, length = rollout(100, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n\u001b[1;32m---> 21\u001b[1;33m                               device=device, action_fn=action_fn)\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\experiment.py\u001b[0m in \u001b[0;36mrollout\u001b[1;34m(episodes, env, model, sample_action, cmd, render, replay_buffer, device, action_fn)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         t, reward = rollout_episode(env=env, model=model, sample_action=sample_action, cmd=cmd,\n\u001b[1;32m---> 62\u001b[1;33m                             render=render, device=device, action_fn=action_fn)            \n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\experiment.py\u001b[0m in \u001b[0;36mrollout_episode\u001b[1;34m(env, model, sample_action, cmd, render, device, action_fn)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-31a24c78cfa2>\u001b[0m in \u001b[0;36maction_fn\u001b[1;34m(model, inputs, sample_action)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0maction_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0maction_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-65f91478025c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, i)\n",
    "    \n",
    "    if i % 500 == 0:        \n",
    "        trajectories, mean_reward, length = rollout(100, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn)\n",
    "        rb.add(trajectories)\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Mean_Reward', mean_reward, steps)\n",
    "        \n",
    "\n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (280.085, 318.91295076177306)\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
