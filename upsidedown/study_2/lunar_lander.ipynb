{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from itertools import count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_v0_lunar_lander_v2'\n",
    "HIDDEN = 32\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# class Behavior(torch.nn.Module):\n",
    "#     def __init__(self, input_shape, num_actions):\n",
    "#         super(Behavior, self).__init__()\n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(input_shape, HIDDEN), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, HIDDEN),\n",
    "#             nn.ReLU(),            \n",
    "#             nn.Linear(HIDDEN, HIDDEN), \n",
    "#             nn.ReLU(),            \n",
    "#             nn.Linear(HIDDEN, num_actions)\n",
    "#         )        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.classifier(x)\n",
    "\n",
    "\n",
    "class Behavior(nn.Module):\n",
    "    def __init__(self, state_shape, cmd_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc_state = nn.Linear(state_shape,HIDDEN)\n",
    "        self.fc_cmd = nn.Linear(cmd_shape,HIDDEN)\n",
    "        \n",
    "        self.fc1 = nn.Linear(HIDDEN,HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN,num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_spate = self.fc_state(x[0])\n",
    "        output_cmd = torch.sigmoid(self.fc_cmd(x[1]))\n",
    "        \n",
    "        output = output_spate * output_cmd\n",
    "        \n",
    "        output = torch.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "#         self.classifier = torch.nn.Sequential(\n",
    "#             nn.Linear(input_shape, HIDDEN), \n",
    "# #             nn.Dropout(0.1),\n",
    "#             torch.nn.LayerNorm(HIDDEN),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, HIDDEN),\n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "#             nn.ReLU(),            \n",
    "# #             nn.Linear(HIDDEN, HIDDEN), \n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "# #             nn.ReLU(),            \n",
    "# #             nn.Linear(HIDDEN, HIDDEN), \n",
    "# #             torch.nn.LayerNorm(HIDDEN),\n",
    "# #             nn.Dropout(0.1),\n",
    "# #             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN, num_actions)\n",
    "#         )        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(state_shape=env.observation_space.shape[0], cmd_shape=2, num_actions=env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=50, last_few=50)\n",
    "\n",
    "n_warmup_episodes = 30\n",
    "# Random rollout\n",
    "trajectories, mean_reward, length = rollout(episodes=n_warmup_episodes, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "# Keep track of steps used during random rollout!\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "steps += length\n",
    "save_model(MODEL_NAME, epoch, model_sample, optimizer, loss, steps)\n",
    "\n",
    "# Plot initial values\n",
    "writer.add_scalar('Steps/reward', mean_reward, steps)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model([inputs[:, :-2], inputs[:, -2:]])\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model([inputs[:, :-2], inputs[:, -2:]])\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:        \n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())        \n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 0, steps 3676 with loss: 0.0\n",
      "3676\n",
      "Average Episode Reward: -156.60252454335253\n",
      "i: 0, s: 4603, Loss: 1.3939533233642578\n",
      "Average Episode Reward: -140.97323759529982\n",
      "Average Episode Reward: -144.61891011867456\n",
      "Average Episode Reward: -126.9948052598018\n",
      "Average Episode Reward: -113.97513915697087\n",
      "i: 200, s: 8095, Loss: 1.3779411315917969\n",
      "Average Episode Reward: -105.62407844282163\n",
      "Average Episode Reward: -147.17655294931035\n",
      "Average Episode Reward: -114.46937486127533\n",
      "Average Episode Reward: -97.74574513351453\n",
      "i: 400, s: 11807, Loss: 1.3705745935440063\n",
      "Average Episode Reward: -105.22635558690949\n",
      "Average Episode Reward: -106.18434431837773\n",
      "Average Episode Reward: -70.79296625740696\n",
      "Average Episode Reward: -93.05582327119569\n",
      "i: 600, s: 16132, Loss: 1.3629987239837646\n",
      "Average Episode Reward: -123.43925537953706\n",
      "Average Episode Reward: -67.5068043105255\n",
      "Average Episode Reward: -113.11475356968904\n",
      "Average Episode Reward: -95.63659784455245\n",
      "i: 800, s: 19965, Loss: 1.3565438985824585\n",
      "Average Episode Reward: -88.49082341941093\n",
      "Average Episode Reward: -105.77639795456852\n",
      "Average Episode Reward: -83.67285398855351\n",
      "Average Episode Reward: -129.98084235182228\n",
      "i: 1000, s: 24168, Loss: 1.3497085571289062\n",
      "Average Episode Reward: -99.21514533547996\n",
      "Average Episode Reward: -77.9833337881989\n",
      "Average Episode Reward: -105.62113826160389\n",
      "Average Episode Reward: -124.09122093540618\n",
      "i: 1200, s: 28989, Loss: 1.3425729274749756\n",
      "Average Episode Reward: -130.18809026101525\n",
      "Average Episode Reward: -109.13560553870707\n",
      "Average Episode Reward: -87.9173489176944\n",
      "Average Episode Reward: -115.84540006364973\n",
      "i: 1400, s: 34116, Loss: 1.3345097303390503\n",
      "Average Episode Reward: -101.27031192891879\n",
      "Average Episode Reward: -79.01358502576535\n",
      "Average Episode Reward: -100.58365595819\n",
      "Average Episode Reward: -86.42139466923588\n",
      "i: 1600, s: 39530, Loss: 1.3262532949447632\n",
      "Average Episode Reward: -107.51130477328763\n",
      "Average Episode Reward: -75.40690234999225\n",
      "Average Episode Reward: -108.6545030713465\n",
      "Average Episode Reward: -121.33416035750152\n",
      "i: 1800, s: 44949, Loss: 1.3172624111175537\n",
      "Average Episode Reward: -78.96259484587793\n",
      "Average Episode Reward: -79.25337367476797\n",
      "Average Episode Reward: -79.11391813236064\n",
      "Average Episode Reward: -110.77658251004837\n",
      "i: 2000, s: 50557, Loss: 1.3085988759994507\n",
      "Average Episode Reward: -140.93002770289496\n",
      "Average Episode Reward: -118.03410777122622\n",
      "Average Episode Reward: -74.88140401882023\n",
      "Average Episode Reward: -105.1825584529922\n",
      "i: 2200, s: 56942, Loss: 1.3005074262619019\n",
      "Average Episode Reward: -94.2980943615137\n",
      "Average Episode Reward: -136.02082148906\n",
      "Average Episode Reward: -50.92831566640238\n",
      "Average Episode Reward: -109.62799781453596\n",
      "i: 2400, s: 64201, Loss: 1.2929096221923828\n",
      "Average Episode Reward: -91.66387212743511\n",
      "Average Episode Reward: -85.69082076289088\n",
      "Average Episode Reward: -117.60963101591412\n",
      "Average Episode Reward: -61.04056161322317\n",
      "i: 2600, s: 72259, Loss: 1.2849704027175903\n",
      "Average Episode Reward: -172.621295178871\n",
      "Average Episode Reward: -124.71305675777103\n",
      "Average Episode Reward: -186.59893676097528\n",
      "Average Episode Reward: -102.66777650345799\n",
      "i: 2800, s: 80680, Loss: 1.2771508693695068\n",
      "Average Episode Reward: -109.86126114122422\n",
      "Average Episode Reward: -145.63613531750508\n",
      "Average Episode Reward: -126.66652275897559\n",
      "Average Episode Reward: -102.0001254233099\n",
      "i: 3000, s: 90209, Loss: 1.270028829574585\n",
      "Average Episode Reward: -167.8654681998666\n",
      "Average Episode Reward: -135.50957231185666\n",
      "Average Episode Reward: -162.9421840686021\n",
      "Average Episode Reward: -108.94100357577403\n",
      "i: 3200, s: 100023, Loss: 1.2631982564926147\n",
      "Average Episode Reward: -147.7111997581075\n",
      "Average Episode Reward: -140.63456223697875\n",
      "Average Episode Reward: -222.62789554011584\n",
      "Average Episode Reward: -83.66128923346692\n",
      "i: 3400, s: 110054, Loss: 1.2567200660705566\n",
      "Average Episode Reward: -170.96875957660637\n",
      "Average Episode Reward: -129.77635135941415\n",
      "Average Episode Reward: -144.1112905837336\n",
      "Average Episode Reward: -70.91317552123621\n",
      "i: 3600, s: 119705, Loss: 1.2506060600280762\n",
      "Average Episode Reward: -100.53814461238001\n",
      "Average Episode Reward: -100.34138423341696\n",
      "Average Episode Reward: -175.8764644505661\n",
      "Average Episode Reward: -260.35844780275465\n",
      "i: 3800, s: 132649, Loss: 1.245043158531189\n",
      "Average Episode Reward: -64.31867312530031\n",
      "Average Episode Reward: -66.47238924681152\n",
      "Average Episode Reward: -172.2989079636912\n",
      "Average Episode Reward: -106.3739610224657\n",
      "i: 4000, s: 145557, Loss: 1.2395634651184082\n",
      "Average Episode Reward: -103.52468400413375\n",
      "Average Episode Reward: -253.43196274316023\n",
      "Average Episode Reward: -207.25745970648873\n",
      "Average Episode Reward: -109.12223600681008\n",
      "i: 4200, s: 160930, Loss: 1.2340019941329956\n",
      "Average Episode Reward: -98.55778791203208\n",
      "Average Episode Reward: -83.42801559910706\n",
      "Average Episode Reward: -235.5426872295633\n",
      "Average Episode Reward: -60.84238658345597\n",
      "i: 4400, s: 170518, Loss: 1.2289259433746338\n",
      "Average Episode Reward: -113.9250721816505\n",
      "Average Episode Reward: -216.7192490605611\n",
      "Average Episode Reward: -248.35668746256383\n",
      "Average Episode Reward: -192.49354336291483\n",
      "i: 4600, s: 181318, Loss: 1.2240490913391113\n",
      "Average Episode Reward: -175.43061483495012\n",
      "Average Episode Reward: -27.902230429023906\n",
      "Average Episode Reward: -375.0765437255833\n",
      "Average Episode Reward: -226.26633071346032\n",
      "i: 4800, s: 193913, Loss: 1.2193973064422607\n",
      "Average Episode Reward: -72.05909631619409\n",
      "Average Episode Reward: -230.33435347764834\n",
      "Average Episode Reward: -343.5745204907168\n",
      "Average Episode Reward: -114.73431130717498\n",
      "i: 5000, s: 204745, Loss: 1.214975118637085\n",
      "Average Episode Reward: -25.973284853039747\n",
      "Average Episode Reward: -224.74162307823934\n",
      "Average Episode Reward: -70.11582373555702\n",
      "Average Episode Reward: -192.19365922640978\n",
      "i: 5200, s: 218807, Loss: 1.2104629278182983\n",
      "Average Episode Reward: -61.98143134012025\n",
      "Average Episode Reward: -7.678854051742005\n",
      "Average Episode Reward: -88.55720790034022\n",
      "Average Episode Reward: -39.34252560743829\n",
      "i: 5400, s: 230065, Loss: 1.2061681747436523\n",
      "Average Episode Reward: -96.03891426997653\n",
      "Average Episode Reward: -60.81413728927517\n",
      "Average Episode Reward: -113.67918408602218\n",
      "Average Episode Reward: -300.17113811798004\n",
      "i: 5600, s: 243666, Loss: 1.2019561529159546\n",
      "Average Episode Reward: -54.436365705258105\n",
      "Average Episode Reward: -33.427328134077314\n",
      "Average Episode Reward: -6.357462957104582\n",
      "Average Episode Reward: -18.81918979746746\n",
      "i: 5800, s: 255579, Loss: 1.198236346244812\n",
      "Average Episode Reward: -15.415833529255384\n",
      "Average Episode Reward: -0.3627765876687931\n",
      "Average Episode Reward: -166.34378082024824\n",
      "Average Episode Reward: -49.23440648222038\n",
      "i: 6000, s: 271726, Loss: 1.1945528984069824\n",
      "Average Episode Reward: -91.23458624426031\n",
      "Average Episode Reward: 8.579139725749037\n",
      "Average Episode Reward: -48.343681832683714\n",
      "Average Episode Reward: -9.589708685150562\n",
      "i: 6200, s: 284166, Loss: 1.1908472776412964\n",
      "Average Episode Reward: -91.10411694347387\n",
      "Average Episode Reward: -15.09197671248828\n",
      "Average Episode Reward: 25.292772941676095\n",
      "Average Episode Reward: -81.14986269261792\n",
      "i: 6400, s: 302346, Loss: 1.1870931386947632\n",
      "Average Episode Reward: -9.945274240220849\n",
      "Average Episode Reward: 36.66659824336724\n",
      "Average Episode Reward: -56.108907939630356\n",
      "Average Episode Reward: 47.97291502554157\n",
      "i: 6600, s: 321824, Loss: 1.1832307577133179\n",
      "Average Episode Reward: 5.105373437479274\n",
      "Average Episode Reward: -50.15874702540918\n",
      "Average Episode Reward: -11.837945302248896\n",
      "Average Episode Reward: -29.367298710516785\n",
      "i: 6800, s: 336192, Loss: 1.1796350479125977\n",
      "Average Episode Reward: -23.947743107372567\n",
      "Average Episode Reward: -27.02952041114917\n",
      "Average Episode Reward: -120.10424847735472\n",
      "Average Episode Reward: 16.091989601636584\n",
      "i: 7000, s: 351474, Loss: 1.1761791706085205\n",
      "Average Episode Reward: -17.08717673120797\n",
      "Average Episode Reward: -85.62948495283887\n",
      "Average Episode Reward: 52.84238477493365\n",
      "Average Episode Reward: -82.55961163346333\n",
      "i: 7200, s: 368603, Loss: 1.1727617979049683\n",
      "Average Episode Reward: 4.486352238101023\n",
      "Average Episode Reward: 28.106465829589194\n",
      "Average Episode Reward: -31.363523644335704\n",
      "Average Episode Reward: 43.784505259282945\n",
      "i: 7400, s: 390165, Loss: 1.169477105140686\n",
      "Average Episode Reward: -44.48900652569956\n",
      "Average Episode Reward: 26.330103316029316\n",
      "Average Episode Reward: -45.06520524279889\n",
      "Average Episode Reward: -38.28482431740282\n",
      "i: 7600, s: 410319, Loss: 1.166179895401001\n",
      "Average Episode Reward: 50.77501227578587\n",
      "Average Episode Reward: -4.389524762604367\n",
      "Average Episode Reward: 61.401848484462924\n",
      "Average Episode Reward: 18.862956995803124\n",
      "i: 7800, s: 435919, Loss: 1.1626993417739868\n",
      "Average Episode Reward: -0.606275281923861\n",
      "Average Episode Reward: 14.357022162588924\n",
      "Average Episode Reward: 35.410528898467035\n",
      "Average Episode Reward: -24.644594082188142\n",
      "i: 8000, s: 459460, Loss: 1.1593064069747925\n",
      "Average Episode Reward: 47.2972825140299\n",
      "Average Episode Reward: -5.472650251299011\n",
      "Average Episode Reward: 37.0679408310243\n",
      "Average Episode Reward: 42.80991256308839\n",
      "i: 8200, s: 482634, Loss: 1.1561167240142822\n",
      "Average Episode Reward: -32.42181526103101\n",
      "Average Episode Reward: -15.582613226528787\n",
      "Average Episode Reward: 10.425034759552341\n",
      "Average Episode Reward: 42.01932671094306\n",
      "i: 8400, s: 506438, Loss: 1.1531440019607544\n",
      "Average Episode Reward: 32.08980881659511\n",
      "Average Episode Reward: -25.910288156178517\n",
      "Average Episode Reward: 50.44566280124055\n",
      "Average Episode Reward: -58.43706557622861\n",
      "i: 8600, s: 525738, Loss: 1.1501898765563965\n",
      "Average Episode Reward: 44.96955469374963\n",
      "Average Episode Reward: 84.01454554195693\n",
      "Average Episode Reward: 3.7042761392081447\n",
      "Average Episode Reward: 27.13442473950038\n",
      "i: 8800, s: 550024, Loss: 1.1474356651306152\n",
      "Average Episode Reward: 49.68698523678213\n",
      "Average Episode Reward: 12.344919420409752\n",
      "Average Episode Reward: 6.58833164620925\n",
      "Average Episode Reward: 31.273571281306687\n",
      "i: 9000, s: 570214, Loss: 1.1447722911834717\n",
      "Average Episode Reward: -21.13913715618218\n",
      "Average Episode Reward: -23.633154829969612\n",
      "Average Episode Reward: 64.10348939199066\n",
      "Average Episode Reward: 53.98411237943678\n",
      "i: 9200, s: 592105, Loss: 1.142127513885498\n",
      "Average Episode Reward: 78.94427410418386\n",
      "Average Episode Reward: -36.568126568580155\n",
      "Average Episode Reward: 30.123666286770607\n",
      "Average Episode Reward: 17.46785952460891\n",
      "i: 9400, s: 614566, Loss: 1.1396876573562622\n",
      "Average Episode Reward: 6.7590399252246955\n",
      "Average Episode Reward: -21.94737292244801\n",
      "Average Episode Reward: 59.90975655686295\n",
      "Average Episode Reward: 9.919647399553726\n",
      "i: 9600, s: 636190, Loss: 1.1374001502990723\n",
      "Average Episode Reward: 21.351425469041857\n",
      "Average Episode Reward: 37.62659442401829\n",
      "Average Episode Reward: 20.507161685904794\n",
      "Average Episode Reward: 74.82521547482904\n",
      "i: 9800, s: 662535, Loss: 1.1352429389953613\n",
      "Average Episode Reward: 84.57131309225686\n",
      "Average Episode Reward: 51.50731660378377\n",
      "Average Episode Reward: 7.569792723683605\n",
      "Average Episode Reward: 57.324535559809284\n",
      "i: 10000, s: 687779, Loss: 1.133338451385498\n",
      "Average Episode Reward: 13.639111836080431\n",
      "Average Episode Reward: 76.02698590285841\n",
      "Average Episode Reward: 6.133843423044802\n",
      "Average Episode Reward: 43.30482968933777\n",
      "i: 10200, s: 713732, Loss: 1.1314599514007568\n",
      "Average Episode Reward: 48.86350221834907\n",
      "Average Episode Reward: 74.9287527995488\n",
      "Average Episode Reward: 35.8426099989296\n",
      "Average Episode Reward: 41.09937653115933\n",
      "i: 10400, s: 743315, Loss: 1.12956964969635\n",
      "Average Episode Reward: 71.79218902258168\n",
      "Average Episode Reward: -12.170345865564475\n",
      "Average Episode Reward: 75.42438057253028\n",
      "Average Episode Reward: 18.692058101867133\n",
      "i: 10600, s: 766712, Loss: 1.1276962757110596\n",
      "Average Episode Reward: -23.18832094047938\n",
      "Average Episode Reward: 59.06886054199738\n",
      "Average Episode Reward: 15.89261014827835\n",
      "Average Episode Reward: -5.738058511716079\n",
      "i: 10800, s: 789920, Loss: 1.1258968114852905\n",
      "Average Episode Reward: 37.44726654779887\n",
      "Average Episode Reward: 13.71173736304963\n",
      "Average Episode Reward: 32.217344674102506\n",
      "Average Episode Reward: 45.65229237275876\n",
      "i: 11000, s: 813612, Loss: 1.1241481304168701\n",
      "Average Episode Reward: 59.28259869413455\n",
      "Average Episode Reward: 36.741695217750134\n",
      "Average Episode Reward: 76.81430360937082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fcacf6dd1aeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mn_updates_per_iter\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n\u001b[1;32m---> 30\u001b[1;33m                               device=device, action_fn=action_fn)\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout\u001b[1;34m(episodes, env, model, sample_action, cmd, render, replay_buffer, device, action_fn, evaluation)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         t, reward = rollout_episode(env=env, model=model, sample_action=sample_action, cmd=cmd,\n\u001b[1;32m---> 69\u001b[1;33m                             render=render, device=device, action_fn=action_fn)            \n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\upsidedown\\upsidedown\\study_2\\experiment.py\u001b[0m in \u001b[0;36mrollout_episode\u001b[1;34m(env, model, sample_action, cmd, render, device, action_fn)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-61c6aeb174ca>\u001b[0m in \u001b[0;36maction_fn\u001b[1;34m(model, inputs, sample_action)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0maction_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0maction_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a2fbdb5dda70>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moutput_spate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0moutput_cmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_cmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\upsidedown\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "SOLVED_MEAN_REWARD = 200\n",
    "MAX_STEPS = 10**7\n",
    "rewards = []\n",
    "\n",
    "epoch, model_sample, optimizer, loss, steps = load_model(MODEL_NAME, model_sample, optimizer, device, train=True)\n",
    "print(steps)\n",
    "\n",
    "EVAL_EVERY = 1000\n",
    "\n",
    "for i in count(start=epoch):\n",
    "    x, y = rb.sample(batch_size, device)    \n",
    "    loss = train_step(model_sample, x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    writer.add_scalar('Loss/loss', loss, i)\n",
    "    \n",
    "    (dh, dr) = rb.sample_command()\n",
    "    writer.add_scalar('Epoch/dh', dh, i)\n",
    "    writer.add_scalar('Epoch/dr', dr, i)\n",
    "\n",
    "    n_episodes_per_iter = 10\n",
    "    n_updates_per_iter = 50\n",
    "    if i % n_updates_per_iter == 0:\n",
    "        trajectories, mean_reward, length = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                              device=device, action_fn=action_fn)\n",
    "        rb.add(trajectories)\n",
    "        rewards.append(mean_reward)\n",
    "        rewards = rewards[-50:] # Keep only last  rewards\n",
    "        \n",
    "        steps += length\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)        \n",
    "        print(f\"Average Episode Reward: {mean_reward}\")        \n",
    "        writer.add_scalar('Steps/reward', mean_reward, steps)\n",
    "        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Steps/length', mean_length, steps)\n",
    "        \n",
    "        if np.mean(rewards) >= SOLVED_MEAN_REWARD:\n",
    "            print(\"Task considered solved! Stopping.\")\n",
    "            break\n",
    "        \n",
    "        if steps >= MAX_STEPS:\n",
    "            print(f\"Steps {steps} exceeds max env steps {MAX_STEPS}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "    if i % EVAL_EVERY == 0:\n",
    "        eval_episodes = 10\n",
    "        _, mean_reward, length = rollout(eval_episodes, env=env, model=model_sample, \n",
    "                            sample_action=True, replay_buffer=rb, \n",
    "                            device=device, action_fn=action_fn, evaluation=True)\n",
    "        \n",
    "        writer.add_scalar('Eval/reward', mean_reward, i)        \n",
    "        mean_length = length*1.0/n_episodes_per_iter\n",
    "        writer.add_scalar('Eval/length', mean_length, i)\n",
    "        \n",
    "    if i % 200 == 0:\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, s: {steps}, Loss: {avg_loss}')\n",
    "        \n",
    "        save_model(MODEL_NAME, i, model_sample, optimizer, avg_loss, steps)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 1223800, steps 851509 with loss: 0.35655415058135986\n",
      "Average Episode Reward: -107.85978643501417\n"
     ]
    }
   ],
   "source": [
    "       # dh ,dr\n",
    "cmd = rb.sample_command()\n",
    "rb.sample_command()\n",
    "env = gym.make('LunarLander-v2')\n",
    "e, model, _, l,_ = load_model(name=MODEL_NAME, train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "_, mean_reward, _ = rollout(episodes=5, env=env, model=model_sample, sample_action=True, \n",
    "                      cmd=cmd, render=True, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
