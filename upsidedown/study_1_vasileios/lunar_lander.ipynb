{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.relu(self.fc1(x))\n",
    "        output = torch.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss() #torch.nn.BCEWithLogitsLoss() #torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device) #env.action_space.n\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: -181.25503152411505\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=50, last_few=50)\n",
    "\n",
    "# Random rollout\n",
    "trajectories, avg_reward = rollout(episodes=30, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "print(f\"Average Episode Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:\n",
    "        global steps_done\n",
    "        sample = random.random()\n",
    "        eps_threshold = 0.0 #EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        \n",
    "                \n",
    "        #if sample > eps_threshold:\n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "#         else:\n",
    "#             action = random.randrange(env.action_space.n)\n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().cpu().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n",
      "Average Episode Reward: -147.18674007991683\n",
      "i: 0, Loss: 1.375089406967163\n",
      "Average Episode Reward: -84.40132595158063\n",
      "i: 10, Loss: 1.3183801174163818\n",
      "Average Episode Reward: -65.3593440563526\n",
      "i: 20, Loss: 1.2512346506118774\n",
      "Average Episode Reward: -67.05461776566904\n",
      "i: 30, Loss: 1.1951897144317627\n",
      "Average Episode Reward: -67.7842011585556\n",
      "i: 40, Loss: 1.1479456424713135\n",
      "Average Episode Reward: -58.62704704470716\n",
      "i: 50, Loss: 1.1075485944747925\n",
      "Average Episode Reward: -51.91007518795973\n",
      "i: 60, Loss: 1.0711078643798828\n",
      "Average Episode Reward: -49.52578837194524\n",
      "i: 70, Loss: 1.0378433465957642\n",
      "Average Episode Reward: -59.69706923607364\n",
      "i: 80, Loss: 1.0077588558197021\n",
      "Average Episode Reward: -54.09257578525548\n",
      "i: 90, Loss: 0.9800236225128174\n",
      "Average Episode Reward: -45.0265501385662\n",
      "i: 100, Loss: 0.9555590748786926\n",
      "Average Episode Reward: -50.63211749443112\n",
      "i: 110, Loss: 0.9341796040534973\n",
      "Average Episode Reward: -19.107380199369107\n",
      "i: 120, Loss: 0.913959801197052\n",
      "Average Episode Reward: -23.49087205490222\n",
      "i: 130, Loss: 0.893052875995636\n",
      "Average Episode Reward: -21.85625355741579\n",
      "i: 140, Loss: 0.8727523684501648\n",
      "Average Episode Reward: -28.19309463863192\n",
      "i: 150, Loss: 0.853732168674469\n",
      "Average Episode Reward: -38.88414830962478\n",
      "i: 160, Loss: 0.8368619084358215\n",
      "Average Episode Reward: -10.320447129128167\n",
      "i: 170, Loss: 0.8211418390274048\n",
      "Average Episode Reward: -33.096594158829475\n",
      "i: 180, Loss: 0.8058328032493591\n",
      "Average Episode Reward: -11.571660709578257\n",
      "i: 190, Loss: 0.7916955947875977\n",
      "Average Episode Reward: -39.293105535008884\n",
      "i: 200, Loss: 0.7786027789115906\n",
      "Average Episode Reward: -35.32890623877593\n",
      "i: 210, Loss: 0.7658966779708862\n",
      "Average Episode Reward: -76.53950703385915\n",
      "i: 220, Loss: 0.7532650828361511\n",
      "Average Episode Reward: -39.18381247190689\n",
      "i: 230, Loss: 0.7416301369667053\n",
      "Average Episode Reward: -33.99123178427157\n",
      "i: 240, Loss: 0.7304731011390686\n",
      "Average Episode Reward: -60.23221642878374\n",
      "i: 250, Loss: 0.7205936908721924\n",
      "Average Episode Reward: -24.70343099205794\n",
      "i: 260, Loss: 0.7109599709510803\n",
      "Average Episode Reward: -36.33084304323996\n",
      "i: 270, Loss: 0.7019508481025696\n",
      "Average Episode Reward: -12.154522288473412\n",
      "i: 280, Loss: 0.6934489607810974\n",
      "Average Episode Reward: -57.39838373921763\n",
      "i: 290, Loss: 0.6852986216545105\n",
      "Average Episode Reward: -34.134786534591214\n",
      "i: 300, Loss: 0.6774958372116089\n",
      "Average Episode Reward: -55.2319473302061\n",
      "i: 310, Loss: 0.6694477796554565\n",
      "Average Episode Reward: -9.467287923762994\n",
      "i: 320, Loss: 0.661149263381958\n",
      "Average Episode Reward: -48.222582193824124\n",
      "i: 330, Loss: 0.6526783108711243\n",
      "Average Episode Reward: -38.4549433824891\n",
      "i: 340, Loss: 0.6446506977081299\n",
      "Average Episode Reward: -37.087582055842105\n",
      "i: 350, Loss: 0.6371144652366638\n",
      "Average Episode Reward: -29.653069007232027\n",
      "i: 360, Loss: 0.6297183036804199\n",
      "Average Episode Reward: -21.627436802824338\n",
      "i: 370, Loss: 0.6221747398376465\n",
      "Average Episode Reward: -17.391843473881718\n",
      "i: 380, Loss: 0.6158190369606018\n",
      "Average Episode Reward: -52.55695584239429\n",
      "i: 390, Loss: 0.6104885935783386\n",
      "Average Episode Reward: -17.36046398895875\n",
      "i: 400, Loss: 0.604874312877655\n",
      "Average Episode Reward: -9.13992501334666\n",
      "i: 410, Loss: 0.600223183631897\n",
      "Average Episode Reward: -78.38348199699412\n",
      "i: 420, Loss: 0.5953087210655212\n",
      "Average Episode Reward: -20.81355239650216\n",
      "i: 430, Loss: 0.5903075933456421\n",
      "Average Episode Reward: -55.66974894585085\n",
      "i: 440, Loss: 0.5849156379699707\n",
      "Average Episode Reward: -49.51733288688048\n",
      "i: 450, Loss: 0.579095184803009\n",
      "Average Episode Reward: -60.95705718404948\n",
      "i: 460, Loss: 0.5734695792198181\n",
      "Average Episode Reward: -11.756401791387196\n",
      "i: 470, Loss: 0.5680624842643738\n",
      "Average Episode Reward: -62.562151352737736\n",
      "i: 480, Loss: 0.5624455213546753\n",
      "Average Episode Reward: -89.84242382787559\n",
      "i: 490, Loss: 0.5571715831756592\n",
      "Average Episode Reward: -25.965487021360822\n",
      "i: 500, Loss: 0.5517038702964783\n",
      "Average Episode Reward: -19.904978607743864\n",
      "i: 510, Loss: 0.5464836359024048\n",
      "Average Episode Reward: -3.7393259036547177\n",
      "i: 520, Loss: 0.5408200025558472\n",
      "Average Episode Reward: -62.01421042955002\n",
      "i: 530, Loss: 0.534904956817627\n",
      "Average Episode Reward: 12.576474476224002\n",
      "i: 540, Loss: 0.5292620658874512\n",
      "Average Episode Reward: -17.739658029286083\n",
      "i: 550, Loss: 0.5234290957450867\n",
      "Average Episode Reward: -64.15240887700025\n",
      "i: 560, Loss: 0.5177133083343506\n",
      "Average Episode Reward: -39.06204700004952\n",
      "i: 570, Loss: 0.512162983417511\n",
      "Average Episode Reward: 9.742436286892902\n",
      "i: 580, Loss: 0.5072721838951111\n",
      "Average Episode Reward: -14.06866019097745\n",
      "i: 590, Loss: 0.5029382705688477\n",
      "Average Episode Reward: 17.373288211791163\n",
      "i: 600, Loss: 0.4988548159599304\n",
      "Average Episode Reward: -11.342188709123505\n",
      "i: 610, Loss: 0.4952142536640167\n",
      "Average Episode Reward: -12.979022947374782\n",
      "i: 620, Loss: 0.49184760451316833\n",
      "Average Episode Reward: -66.89419526718237\n",
      "i: 630, Loss: 0.48861101269721985\n",
      "Average Episode Reward: 33.06970302560633\n",
      "i: 640, Loss: 0.4854361116886139\n",
      "Average Episode Reward: 19.774406143882906\n",
      "i: 650, Loss: 0.4826323390007019\n",
      "Average Episode Reward: 51.189815285822775\n",
      "i: 660, Loss: 0.4798046350479126\n",
      "Average Episode Reward: 22.58336688403549\n",
      "i: 670, Loss: 0.477100133895874\n",
      "Average Episode Reward: 19.15258736010731\n",
      "i: 680, Loss: 0.47455185651779175\n",
      "Average Episode Reward: 87.32917430881862\n",
      "i: 690, Loss: 0.4720606803894043\n",
      "Average Episode Reward: 15.095573210985822\n",
      "i: 700, Loss: 0.4694267809391022\n",
      "Average Episode Reward: 44.41563430480191\n",
      "i: 710, Loss: 0.46680325269699097\n",
      "Average Episode Reward: 31.538288133670914\n",
      "i: 720, Loss: 0.46428182721138\n",
      "Average Episode Reward: 50.64400791261551\n",
      "i: 730, Loss: 0.46188151836395264\n",
      "Average Episode Reward: 27.139458933284594\n",
      "i: 740, Loss: 0.459628164768219\n",
      "Average Episode Reward: 32.08891100546108\n",
      "i: 750, Loss: 0.45737937092781067\n",
      "Average Episode Reward: 36.473420742031564\n",
      "i: 760, Loss: 0.4551907181739807\n",
      "Average Episode Reward: 109.0624503101149\n",
      "i: 770, Loss: 0.45299452543258667\n",
      "Average Episode Reward: 87.902540614228\n",
      "i: 780, Loss: 0.4508517384529114\n",
      "Average Episode Reward: 64.722575235888\n",
      "i: 790, Loss: 0.4487755596637726\n",
      "Average Episode Reward: 46.177958402378124\n",
      "i: 800, Loss: 0.4466499388217926\n",
      "Average Episode Reward: 64.58978930897108\n",
      "i: 810, Loss: 0.4444768726825714\n",
      "Average Episode Reward: 19.99107999219161\n",
      "i: 820, Loss: 0.44228577613830566\n",
      "Average Episode Reward: 48.713621390190056\n",
      "i: 830, Loss: 0.4400557577610016\n",
      "Average Episode Reward: 42.760768019958014\n",
      "i: 840, Loss: 0.4378359615802765\n",
      "Average Episode Reward: 51.61566911231606\n",
      "i: 850, Loss: 0.4356883466243744\n",
      "Average Episode Reward: 15.02283436731002\n",
      "i: 860, Loss: 0.4336147904396057\n",
      "Average Episode Reward: 124.25100678120243\n",
      "i: 870, Loss: 0.4315519332885742\n",
      "Average Episode Reward: 38.19478962202989\n",
      "i: 880, Loss: 0.4295397996902466\n",
      "Average Episode Reward: -7.822645056290815\n",
      "i: 890, Loss: 0.42755213379859924\n",
      "Average Episode Reward: 117.67916277989434\n",
      "i: 900, Loss: 0.425598680973053\n",
      "Average Episode Reward: -14.504389856535699\n",
      "i: 910, Loss: 0.42369312047958374\n",
      "Average Episode Reward: 85.90254992073024\n",
      "i: 920, Loss: 0.42178115248680115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-be23a158db10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     trajectories, mean_reward = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n\u001b[0;32m---> 24\u001b[0;31m                           device=device, action_fn=action_fn)\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/885f402c-7221-4331-a50f-8a04f96c2300/andriy/upsidedown/upsidedown/study_1/experiment.py\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(episodes, env, model, sample_action, cmd, render, replay_buffer, device, action_fn)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         t, reward = rollout_episode(env=env, model=model, sample_action=sample_action, cmd=cmd,\n\u001b[0;32m---> 64\u001b[0;31m                             render=render, device=device, action_fn=action_fn)            \n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/885f402c-7221-4331-a50f-8a04f96c2300/andriy/upsidedown/upsidedown/study_1/experiment.py\u001b[0m in \u001b[0;36mrollout_episode\u001b[0;34m(env, model, sample_action, cmd, render, device, action_fn)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6a55a8ab7df9>\u001b[0m in \u001b[0;36maction_fn\u001b[0;34m(model, inputs, sample_action)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0maction_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt13/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss = load_model('lunar_lander_sample_actions', model_sample, optimizer, device, train=True)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    for _ in range(50):\n",
    "        x, y = rb.sample(batch_size, device)    \n",
    "        loss = train_step(model_sample, x, y)\n",
    "        loss_sum += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "    if i == 0:\n",
    "        n_episodes_per_iter = 10\n",
    "    else:\n",
    "        n_episodes_per_iter = 10\n",
    "        \n",
    "    trajectories, mean_reward = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                          device=device, action_fn=action_fn)\n",
    "    rb.add(trajectories)\n",
    "    \n",
    "    if i % 20:\n",
    "        steps_done += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Average Episode Reward: {mean_reward}\")\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, Loss: {avg_loss}') #'\\t Accuracy: {accuracy_m.result()}')\n",
    "        save_model('lunar_lander_sample_actions', i, model_sample, optimizer, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-101.11198134691003, 58),\n",
       " (-103.64699052774198, 68),\n",
       " (-127.58791857723014, 84),\n",
       " (-131.89672730059758, 155),\n",
       " (-163.5911319642045, 124),\n",
       " (-210.31447991516205, 82),\n",
       " (-275.18922879202216, 102),\n",
       " (-305.9108282899752, 96),\n",
       " (-351.6562497742343, 109),\n",
       " (-410.746399174143, 93)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(rb, open(\"buffer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-62cc408c9ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrbbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"buffer.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "rbbb = pickle.load(open(\"buffer.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(310.66825831675976, 381),\n",
       " (307.26196994702906, 356),\n",
       " (303.24897175129075, 434),\n",
       " (302.4825435649981, 422),\n",
       " (302.08153217515587, 443),\n",
       " (301.62343453588, 450),\n",
       " (301.336795776998, 374),\n",
       " (301.0824957187764, 401),\n",
       " (300.30239077159666, 393),\n",
       " (300.00413043635956, 376),\n",
       " (299.5551268182258, 403),\n",
       " (299.4571607913125, 401),\n",
       " (299.3745826281496, 439),\n",
       " (299.27842296736947, 832),\n",
       " (298.9785312032605, 431),\n",
       " (298.9744925975774, 368),\n",
       " (298.8005255482776, 377),\n",
       " (298.2905025908745, 374),\n",
       " (298.25199484161163, 327),\n",
       " (298.1082917441395, 426),\n",
       " (298.01858655378317, 380),\n",
       " (297.86267734638704, 387),\n",
       " (297.44062399672396, 222),\n",
       " (297.167313218092, 372),\n",
       " (296.8036351123287, 346),\n",
       " (296.40188358973296, 677),\n",
       " (296.3219972655071, 392),\n",
       " (296.0394593078145, 377),\n",
       " (295.79209420786935, 187),\n",
       " (295.5912289208195, 415),\n",
       " (295.2668316842348, 389),\n",
       " (295.1391259645944, 284),\n",
       " (295.12430763923635, 409),\n",
       " (295.10259048325526, 365),\n",
       " (294.76372007622194, 365),\n",
       " (294.67945424467996, 425),\n",
       " (294.2226482587089, 367),\n",
       " (293.3617890483258, 420),\n",
       " (292.5447532344713, 360),\n",
       " (292.42745334494384, 363),\n",
       " (292.2048495687508, 371),\n",
       " (291.89594233247914, 194),\n",
       " (291.7759414623758, 386),\n",
       " (291.76475823960226, 328),\n",
       " (291.6613137109741, 380),\n",
       " (291.4135601366796, 385),\n",
       " (291.205157072951, 402),\n",
       " (291.0094553445198, 463),\n",
       " (291.00806211006966, 378),\n",
       " (290.8770827395257, 527)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rbbb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(305.6909235090742, 225),\n",
       " (302.0056402212026, 208),\n",
       " (301.7944143213541, 269),\n",
       " (300.71844923634444, 224),\n",
       " (300.65080857502187, 224),\n",
       " (298.2907459656101, 222),\n",
       " (296.1363344579555, 217),\n",
       " (295.11447143257334, 237),\n",
       " (295.01556728710614, 219),\n",
       " (293.1993229567172, 212),\n",
       " (293.05844646098535, 348),\n",
       " (289.4628917571782, 190),\n",
       " (289.20918684615805, 184),\n",
       " (282.6850227305037, 196),\n",
       " (280.199684161463, 162),\n",
       " (279.1794201095174, 193),\n",
       " (278.78865415341886, 211),\n",
       " (272.22855976041046, 209),\n",
       " (271.5107908143556, 215),\n",
       " (269.0895324255778, 209),\n",
       " (268.8088351096112, 190),\n",
       " (266.1338566930086, 227),\n",
       " (265.524661876984, 203),\n",
       " (262.22279884775435, 343),\n",
       " (260.5030123683403, 209),\n",
       " (258.43169595399274, 225),\n",
       " (254.9498661720394, 195),\n",
       " (254.78018048064902, 386),\n",
       " (253.8927482723219, 276),\n",
       " (253.66807584372114, 190),\n",
       " (253.6187685539958, 180),\n",
       " (253.49713835959105, 172),\n",
       " (252.58063975061393, 183),\n",
       " (252.29265489999935, 228),\n",
       " (250.91490513950285, 205),\n",
       " (250.41154765153993, 183),\n",
       " (248.50605915924794, 192),\n",
       " (247.22072380685617, 200),\n",
       " (246.99935073050054, 213),\n",
       " (246.97439714292065, 184),\n",
       " (244.68541568546854, 450),\n",
       " (243.65345306341987, 184),\n",
       " (239.08654579553945, 190),\n",
       " (237.6718367336331, 198),\n",
       " (235.88090564504827, 191),\n",
       " (222.30144667188446, 244),\n",
       " (222.09185215128173, 319),\n",
       " (-9.019525522150943, 395),\n",
       " (-113.24850799013994, 348),\n",
       " (-135.79300254428543, 374)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394.48, 297.9185157945218)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 920 with loss: 0.42178115248680115\n",
      "Average Episode Reward: 88.46099208472887\n"
     ]
    }
   ],
   "source": [
    "cmd = rb.sample_command() #(200, 200)\n",
    "rb.sample_command()\n",
    "#env = gym.make('MountainCar-v0')\n",
    "e, model, _, l = load_model(name='lunar_lander_sample_actions', train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "# _, mean_reward = rollout(episodes=1, env=env, model=model, sample_action=False, \n",
    "#                       replay_buffer=rb, render=True, device=device, action_fn=action_fn)\n",
    "_, mean_reward = rollout(episodes=100, env=env, model=model_sample, sample_action=False, \n",
    "                      cmd=cmd, render=False, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
