{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trajectory = []\n",
    "        self.total_return = 0\n",
    "        self.length = 0\n",
    "        \n",
    "    def add(self, state, action, reward, state_prime):\n",
    "        self.trajectory.append((state, action, reward, state_prime))\n",
    "        self.total_return += reward\n",
    "        self.length += 1\n",
    "        \n",
    "    def sample_segment(self):\n",
    "        T = len(self.trajectory)\n",
    "\n",
    "        t1 = np.random.randint(1, T+1)\n",
    "        t2 = np.random.randint(t1, T+1)\n",
    "\n",
    "        state = self.trajectory[t1-1][0]\n",
    "        action = self.trajectory[t1-1][1]\n",
    "\n",
    "        d_r = 0.0\n",
    "        for i in range(t1, t2 + 1):\n",
    "            d_r += self.trajectory[i-1][2]\n",
    "\n",
    "        d_h = t2 - t1 + 1.0\n",
    "\n",
    "        return ((state,d_r,d_h),action)\n",
    "    \n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size, last_few):\n",
    "        \"\"\"\n",
    "        @param last_few: Number of episodes from the end of the replay buffer\n",
    "        used for sampling exploratory commands.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.cur_size = 0\n",
    "        self.buffer = []\n",
    "        \n",
    "        self.last_few = last_few\n",
    "        \n",
    "    def add(self, trajectory):\n",
    "        self.buffer.append(trajectory)\n",
    "        \n",
    "        self.buffer = sorted(self.buffer, key=lambda x: x.total_return, reverse=True)\n",
    "        self.buffer = self.buffer[:self.max_size]\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        trajectories = np.random.choice(self.buffer, batch_size, replace=True)\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        for t in trajectories:\n",
    "            segments.append(t.sample_segment())\n",
    "            \n",
    "        return segments\n",
    "    \n",
    "    def sample_command(self):\n",
    "        eps = self.buffer[:self.last_few]\n",
    "        \n",
    "        dh_0 = np.mean([e.length for e in eps])\n",
    "        \n",
    "        m = np.mean([e.total_return for e in eps])\n",
    "        s = np.std([e.total_return for e in eps])\n",
    "        \n",
    "        dr_0 = np.random.uniform(m, m+s)\n",
    "        \n",
    "        return dh_0, dr_0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-1. Initialize replay buffer with warm-up episodes using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(1000, 100)\n",
    "\n",
    "avg_rewards = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    t = Trajectory()\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        s_old = s\n",
    "        action = env.action_space.sample()\n",
    "        s, reward, done, info = env.step(action)\n",
    "        t.add(s_old, action, reward, s)\n",
    "        ep_reward += reward\n",
    "    avg_rewards.append(ep_reward)    \n",
    "#     print(f'Episode reward: {ep_reward}')\n",
    "    rb.add(t)\n",
    "    \n",
    "    \n",
    "env.close()\n",
    "print(f\"Average Episode Reward: {np.mean(avg_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-2 Initialize a behavior function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "class Behavior(tf.keras.Model):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super().__init__()        \n",
    "        print(input_shape)\n",
    "        self.d = tf.keras.layers.Dense(32, input_shape=input_shape, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.classifier = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.d(inputs)\n",
    "        x = self.d2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "(d,) = env.observation_space.shape\n",
    "model = Behavior(input_shape=(d+2,), num_actions=env.action_space.n)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "batch_size = 16\n",
    "\n",
    "loss_m = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "model.run_eagerly = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A1-3: while stopping criteria is not reached do:\n",
    "### A1-4:   Improve the behavior function by training on replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1326848268508911\n",
      "Loss: 1.132562279701233\n",
      "Loss: 1.132436990737915\n",
      "Loss: 1.1323360204696655\n",
      "Loss: 1.132219672203064\n",
      "Loss: 1.1321113109588623\n",
      "Loss: 1.1319878101348877\n",
      "Loss: 1.1318769454956055\n",
      "Loss: 1.1317698955535889\n",
      "Loss: 1.1316936016082764\n",
      "Loss: 1.1315803527832031\n",
      "Loss: 1.1314586400985718\n",
      "Loss: 1.1313440799713135\n",
      "Loss: 1.1312594413757324\n",
      "Loss: 1.1311520338058472\n",
      "Loss: 1.1310617923736572\n",
      "Loss: 1.1309516429901123\n",
      "Loss: 1.1308298110961914\n",
      "Loss: 1.1307485103607178\n",
      "Loss: 1.1306666135787964\n",
      "Loss: 1.1305646896362305\n",
      "Loss: 1.1305086612701416\n",
      "Loss: 1.1304153203964233\n",
      "Loss: 1.130315899848938\n",
      "Loss: 1.1302450895309448\n",
      "Loss: 1.1301460266113281\n",
      "Loss: 1.1300417184829712\n",
      "Loss: 1.1299362182617188\n",
      "Loss: 1.1298493146896362\n",
      "Loss: 1.1297376155853271\n",
      "Loss: 1.1296405792236328\n",
      "Loss: 1.1295446157455444\n",
      "Loss: 1.129443645477295\n",
      "Loss: 1.1293468475341797\n",
      "Loss: 1.1292541027069092\n",
      "Loss: 1.1291635036468506\n",
      "Loss: 1.1290628910064697\n",
      "Loss: 1.1289750337600708\n",
      "Loss: 1.1288809776306152\n",
      "Loss: 1.1287952661514282\n",
      "Loss: 1.1287755966186523\n",
      "Loss: 1.1286801099777222\n",
      "Loss: 1.1285834312438965\n",
      "Loss: 1.1284884214401245\n",
      "Loss: 1.1284033060073853\n",
      "Loss: 1.1283156871795654\n",
      "Loss: 1.1282082796096802\n",
      "Loss: 1.1281087398529053\n",
      "Loss: 1.1280466318130493\n",
      "Loss: 1.127949595451355\n"
     ]
    }
   ],
   "source": [
    "def to_training(s, dr, dh):\n",
    "    l = s.tolist()\n",
    "    l.append(dr)\n",
    "    l.append(dh)\n",
    "    return l\n",
    "\n",
    "def segments_to_training(segments):\n",
    "    x = []\n",
    "    y = []\n",
    "    for (s, dr, dh), action in segments:\n",
    "        l = to_training(s, dr, dh)\n",
    "        x.append(l)\n",
    "        y.append(action)\n",
    "    x = tf.constant(x, dtype=tf.float32)\n",
    "    y = tf.constant(y)\n",
    "    \n",
    "    return x, y\n",
    "        \n",
    "# accuracy_m = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "#         print(predictions)\n",
    "        loss = loss_object(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    loss_m(loss)\n",
    "#     accuracy_m(targets, predictions)\n",
    "    \n",
    "    \n",
    "epochs = 1000\n",
    "for i in range(1, epochs+1):\n",
    "    segments = rb.sample(batch_size)\n",
    "    segments = np.array(segments)\n",
    "    x, y = segments_to_training(segments)\n",
    "    train_step(x, y)\n",
    "    if i % 20 == 0:\n",
    "        print(f'Loss: {loss_m.result()}') #'\\t Accuracy: {accuracy_m.result()}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A1:5 Sample exploratory commands based on replay buffer\n",
    "cmd = rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A1:6 Generate episodes using Alg 2 and add to replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "avg_rewards = []\n",
    "\n",
    "def generate_episode(cmd):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    \n",
    "    t = Trajectory()\n",
    "    while not done:\n",
    "        (dh, dr) = cmd\n",
    "        inputs = tf.constant([to_training(s, dr, dh)], dtype=tf.float32)\n",
    "        \n",
    "        action_probs = model(inputs)\n",
    "        action = np.argmax(action_probs)\n",
    "        \n",
    "#         env.render()\n",
    "        s_old = s\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        t.add(s_old, action, reward, s)\n",
    "        ep_reward += reward\n",
    "        dh = dh - 1\n",
    "        dr = dr - reward\n",
    "        cmd = (dh, dr)\n",
    "    \n",
    "#     print(f'Episode reward: {ep_reward}')\n",
    "    rb.add(t)\n",
    "    return ep_reward\n",
    "\n",
    "rewards = [] \n",
    "for e in range(10):\n",
    "    rewards.append(generate_episode(cmd))\n",
    "\n",
    "# env.close()\n",
    "print(f\"Average Episode Reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"behavior_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  160       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  99        \n",
      "=================================================================\n",
      "Total params: 1,315\n",
      "Trainable params: 1,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
