{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.relu(self.fc1(x))\n",
    "        output = torch.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss() #torch.nn.BCEWithLogitsLoss() #torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device) #env.action_space.n\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: -181.25503152411505\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=50, last_few=50)\n",
    "\n",
    "# Random rollout\n",
    "trajectories, avg_reward = rollout(episodes=30, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "print(f\"Average Episode Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:\n",
    "        global steps_done\n",
    "        sample = random.random()\n",
    "        eps_threshold = 0.0 #EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        \n",
    "                \n",
    "        #if sample > eps_threshold:\n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "#         else:\n",
    "#             action = random.randrange(env.action_space.n)\n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().cpu().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n",
      "Average Episode Reward: -147.18674007991683\n",
      "i: 0, Loss: 1.375089406967163\n",
      "Average Episode Reward: -84.40132595158063\n",
      "i: 10, Loss: 1.3183801174163818\n",
      "Average Episode Reward: -65.3593440563526\n",
      "i: 20, Loss: 1.2512346506118774\n",
      "Average Episode Reward: -67.05461776566904\n",
      "i: 30, Loss: 1.1951897144317627\n",
      "Average Episode Reward: -67.7842011585556\n",
      "i: 40, Loss: 1.1479456424713135\n",
      "Average Episode Reward: -58.62704704470716\n",
      "i: 50, Loss: 1.1075485944747925\n",
      "Average Episode Reward: -51.91007518795973\n",
      "i: 60, Loss: 1.0711078643798828\n",
      "Average Episode Reward: -49.52578837194524\n",
      "i: 70, Loss: 1.0378433465957642\n",
      "Average Episode Reward: -59.69706923607364\n",
      "i: 80, Loss: 1.0077588558197021\n",
      "Average Episode Reward: -54.09257578525548\n",
      "i: 90, Loss: 0.9800236225128174\n",
      "Average Episode Reward: -45.0265501385662\n",
      "i: 100, Loss: 0.9555590748786926\n",
      "Average Episode Reward: -50.63211749443112\n",
      "i: 110, Loss: 0.9341796040534973\n",
      "Average Episode Reward: -19.107380199369107\n",
      "i: 120, Loss: 0.913959801197052\n",
      "Average Episode Reward: -23.49087205490222\n",
      "i: 130, Loss: 0.893052875995636\n",
      "Average Episode Reward: -21.85625355741579\n",
      "i: 140, Loss: 0.8727523684501648\n",
      "Average Episode Reward: -28.19309463863192\n",
      "i: 150, Loss: 0.853732168674469\n",
      "Average Episode Reward: -38.88414830962478\n",
      "i: 160, Loss: 0.8368619084358215\n",
      "Average Episode Reward: -10.320447129128167\n",
      "i: 170, Loss: 0.8211418390274048\n",
      "Average Episode Reward: -33.096594158829475\n",
      "i: 180, Loss: 0.8058328032493591\n",
      "Average Episode Reward: -11.571660709578257\n",
      "i: 190, Loss: 0.7916955947875977\n",
      "Average Episode Reward: -39.293105535008884\n",
      "i: 200, Loss: 0.7786027789115906\n",
      "Average Episode Reward: -35.32890623877593\n",
      "i: 210, Loss: 0.7658966779708862\n",
      "Average Episode Reward: -76.53950703385915\n",
      "i: 220, Loss: 0.7532650828361511\n",
      "Average Episode Reward: -39.18381247190689\n",
      "i: 230, Loss: 0.7416301369667053\n",
      "Average Episode Reward: -33.99123178427157\n",
      "i: 240, Loss: 0.7304731011390686\n",
      "Average Episode Reward: -60.23221642878374\n",
      "i: 250, Loss: 0.7205936908721924\n",
      "Average Episode Reward: -24.70343099205794\n",
      "i: 260, Loss: 0.7109599709510803\n",
      "Average Episode Reward: -36.33084304323996\n",
      "i: 270, Loss: 0.7019508481025696\n",
      "Average Episode Reward: -12.154522288473412\n",
      "i: 280, Loss: 0.6934489607810974\n",
      "Average Episode Reward: -57.39838373921763\n",
      "i: 290, Loss: 0.6852986216545105\n",
      "Average Episode Reward: -34.134786534591214\n",
      "i: 300, Loss: 0.6774958372116089\n",
      "Average Episode Reward: -55.2319473302061\n",
      "i: 310, Loss: 0.6694477796554565\n",
      "Average Episode Reward: -9.467287923762994\n",
      "i: 320, Loss: 0.661149263381958\n",
      "Average Episode Reward: -48.222582193824124\n",
      "i: 330, Loss: 0.6526783108711243\n",
      "Average Episode Reward: -38.4549433824891\n",
      "i: 340, Loss: 0.6446506977081299\n",
      "Average Episode Reward: -37.087582055842105\n",
      "i: 350, Loss: 0.6371144652366638\n",
      "Average Episode Reward: -29.653069007232027\n",
      "i: 360, Loss: 0.6297183036804199\n",
      "Average Episode Reward: -21.627436802824338\n",
      "i: 370, Loss: 0.6221747398376465\n",
      "Average Episode Reward: -17.391843473881718\n",
      "i: 380, Loss: 0.6158190369606018\n",
      "Average Episode Reward: -52.55695584239429\n",
      "i: 390, Loss: 0.6104885935783386\n",
      "Average Episode Reward: -17.36046398895875\n",
      "i: 400, Loss: 0.604874312877655\n",
      "Average Episode Reward: -9.13992501334666\n",
      "i: 410, Loss: 0.600223183631897\n",
      "Average Episode Reward: -78.38348199699412\n",
      "i: 420, Loss: 0.5953087210655212\n",
      "Average Episode Reward: -20.81355239650216\n",
      "i: 430, Loss: 0.5903075933456421\n",
      "Average Episode Reward: -55.66974894585085\n",
      "i: 440, Loss: 0.5849156379699707\n",
      "Average Episode Reward: -49.51733288688048\n",
      "i: 450, Loss: 0.579095184803009\n",
      "Average Episode Reward: -60.95705718404948\n",
      "i: 460, Loss: 0.5734695792198181\n",
      "Average Episode Reward: -11.756401791387196\n",
      "i: 470, Loss: 0.5680624842643738\n",
      "Average Episode Reward: -62.562151352737736\n",
      "i: 480, Loss: 0.5624455213546753\n",
      "Average Episode Reward: -89.84242382787559\n",
      "i: 490, Loss: 0.5571715831756592\n",
      "Average Episode Reward: -25.965487021360822\n",
      "i: 500, Loss: 0.5517038702964783\n",
      "Average Episode Reward: -19.904978607743864\n",
      "i: 510, Loss: 0.5464836359024048\n",
      "Average Episode Reward: -3.7393259036547177\n",
      "i: 520, Loss: 0.5408200025558472\n",
      "Average Episode Reward: -62.01421042955002\n",
      "i: 530, Loss: 0.534904956817627\n",
      "Average Episode Reward: 12.576474476224002\n",
      "i: 540, Loss: 0.5292620658874512\n",
      "Average Episode Reward: -17.739658029286083\n",
      "i: 550, Loss: 0.5234290957450867\n",
      "Average Episode Reward: -64.15240887700025\n",
      "i: 560, Loss: 0.5177133083343506\n",
      "Average Episode Reward: -39.06204700004952\n",
      "i: 570, Loss: 0.512162983417511\n",
      "Average Episode Reward: 9.742436286892902\n",
      "i: 580, Loss: 0.5072721838951111\n",
      "Average Episode Reward: -14.06866019097745\n",
      "i: 590, Loss: 0.5029382705688477\n",
      "Average Episode Reward: 17.373288211791163\n",
      "i: 600, Loss: 0.4988548159599304\n",
      "Average Episode Reward: -11.342188709123505\n",
      "i: 610, Loss: 0.4952142536640167\n",
      "Average Episode Reward: -12.979022947374782\n",
      "i: 620, Loss: 0.49184760451316833\n",
      "Average Episode Reward: -66.89419526718237\n",
      "i: 630, Loss: 0.48861101269721985\n",
      "Average Episode Reward: 33.06970302560633\n",
      "i: 640, Loss: 0.4854361116886139\n",
      "Average Episode Reward: 19.774406143882906\n",
      "i: 650, Loss: 0.4826323390007019\n",
      "Average Episode Reward: 51.189815285822775\n",
      "i: 660, Loss: 0.4798046350479126\n",
      "Average Episode Reward: 22.58336688403549\n",
      "i: 670, Loss: 0.477100133895874\n",
      "Average Episode Reward: 19.15258736010731\n",
      "i: 680, Loss: 0.47455185651779175\n",
      "Average Episode Reward: 87.32917430881862\n",
      "i: 690, Loss: 0.4720606803894043\n",
      "Average Episode Reward: 15.095573210985822\n",
      "i: 700, Loss: 0.4694267809391022\n",
      "Average Episode Reward: 44.41563430480191\n",
      "i: 710, Loss: 0.46680325269699097\n",
      "Average Episode Reward: 31.538288133670914\n",
      "i: 720, Loss: 0.46428182721138\n",
      "Average Episode Reward: 50.64400791261551\n",
      "i: 730, Loss: 0.46188151836395264\n",
      "Average Episode Reward: 27.139458933284594\n",
      "i: 740, Loss: 0.459628164768219\n",
      "Average Episode Reward: 32.08891100546108\n",
      "i: 750, Loss: 0.45737937092781067\n",
      "Average Episode Reward: 36.473420742031564\n",
      "i: 760, Loss: 0.4551907181739807\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss = load_model('lunar_lander_sample_actions', model_sample, optimizer, device, train=True)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    for _ in range(50):\n",
    "        x, y = rb.sample(batch_size, device)    \n",
    "        loss = train_step(model_sample, x, y)\n",
    "        loss_sum += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "    if i == 0:\n",
    "        n_episodes_per_iter = 10\n",
    "    else:\n",
    "        n_episodes_per_iter = 10\n",
    "        \n",
    "    trajectories, mean_reward = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                          device=device, action_fn=action_fn)\n",
    "    rb.add(trajectories)\n",
    "    \n",
    "    if i % 20:\n",
    "        steps_done += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Average Episode Reward: {mean_reward}\")\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, Loss: {avg_loss}') #'\\t Accuracy: {accuracy_m.result()}')\n",
    "        save_model('lunar_lander_sample_actions', i, model_sample, optimizer, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-101.11198134691003, 58),\n",
       " (-103.64699052774198, 68),\n",
       " (-127.58791857723014, 84),\n",
       " (-131.89672730059758, 155),\n",
       " (-163.5911319642045, 124),\n",
       " (-210.31447991516205, 82),\n",
       " (-275.18922879202216, 102),\n",
       " (-305.9108282899752, 96),\n",
       " (-351.6562497742343, 109),\n",
       " (-410.746399174143, 93)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(rb, open(\"buffer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbbb = pickle.load(open(\"buffer.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(310.66825831675976, 381),\n",
       " (307.26196994702906, 356),\n",
       " (303.24897175129075, 434),\n",
       " (302.4825435649981, 422),\n",
       " (302.08153217515587, 443),\n",
       " (301.62343453588, 450),\n",
       " (301.336795776998, 374),\n",
       " (301.0824957187764, 401),\n",
       " (300.30239077159666, 393),\n",
       " (300.00413043635956, 376),\n",
       " (299.5551268182258, 403),\n",
       " (299.4571607913125, 401),\n",
       " (299.3745826281496, 439),\n",
       " (299.27842296736947, 832),\n",
       " (298.9785312032605, 431),\n",
       " (298.9744925975774, 368),\n",
       " (298.8005255482776, 377),\n",
       " (298.2905025908745, 374),\n",
       " (298.25199484161163, 327),\n",
       " (298.1082917441395, 426),\n",
       " (298.01858655378317, 380),\n",
       " (297.86267734638704, 387),\n",
       " (297.44062399672396, 222),\n",
       " (297.167313218092, 372),\n",
       " (296.8036351123287, 346),\n",
       " (296.40188358973296, 677),\n",
       " (296.3219972655071, 392),\n",
       " (296.0394593078145, 377),\n",
       " (295.79209420786935, 187),\n",
       " (295.5912289208195, 415),\n",
       " (295.2668316842348, 389),\n",
       " (295.1391259645944, 284),\n",
       " (295.12430763923635, 409),\n",
       " (295.10259048325526, 365),\n",
       " (294.76372007622194, 365),\n",
       " (294.67945424467996, 425),\n",
       " (294.2226482587089, 367),\n",
       " (293.3617890483258, 420),\n",
       " (292.5447532344713, 360),\n",
       " (292.42745334494384, 363),\n",
       " (292.2048495687508, 371),\n",
       " (291.89594233247914, 194),\n",
       " (291.7759414623758, 386),\n",
       " (291.76475823960226, 328),\n",
       " (291.6613137109741, 380),\n",
       " (291.4135601366796, 385),\n",
       " (291.205157072951, 402),\n",
       " (291.0094553445198, 463),\n",
       " (291.00806211006966, 378),\n",
       " (290.8770827395257, 527)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rbbb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(305.6909235090742, 225),\n",
       " (302.0056402212026, 208),\n",
       " (301.7944143213541, 269),\n",
       " (300.71844923634444, 224),\n",
       " (300.65080857502187, 224),\n",
       " (298.2907459656101, 222),\n",
       " (296.1363344579555, 217),\n",
       " (295.11447143257334, 237),\n",
       " (295.01556728710614, 219),\n",
       " (293.1993229567172, 212),\n",
       " (293.05844646098535, 348),\n",
       " (289.4628917571782, 190),\n",
       " (289.20918684615805, 184),\n",
       " (282.6850227305037, 196),\n",
       " (280.199684161463, 162),\n",
       " (279.1794201095174, 193),\n",
       " (278.78865415341886, 211),\n",
       " (272.22855976041046, 209),\n",
       " (271.5107908143556, 215),\n",
       " (269.0895324255778, 209),\n",
       " (268.8088351096112, 190),\n",
       " (266.1338566930086, 227),\n",
       " (265.524661876984, 203),\n",
       " (262.22279884775435, 343),\n",
       " (260.5030123683403, 209),\n",
       " (258.43169595399274, 225),\n",
       " (254.9498661720394, 195),\n",
       " (254.78018048064902, 386),\n",
       " (253.8927482723219, 276),\n",
       " (253.66807584372114, 190),\n",
       " (253.6187685539958, 180),\n",
       " (253.49713835959105, 172),\n",
       " (252.58063975061393, 183),\n",
       " (252.29265489999935, 228),\n",
       " (250.91490513950285, 205),\n",
       " (250.41154765153993, 183),\n",
       " (248.50605915924794, 192),\n",
       " (247.22072380685617, 200),\n",
       " (246.99935073050054, 213),\n",
       " (246.97439714292065, 184),\n",
       " (244.68541568546854, 450),\n",
       " (243.65345306341987, 184),\n",
       " (239.08654579553945, 190),\n",
       " (237.6718367336331, 198),\n",
       " (235.88090564504827, 191),\n",
       " (222.30144667188446, 244),\n",
       " (222.09185215128173, 319),\n",
       " (-9.019525522150943, 395),\n",
       " (-113.24850799013994, 348),\n",
       " (-135.79300254428543, 374)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394.48, 297.9185157945218)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 1820 with loss: 0.600623369216919\n",
      "Average Episode Reward: 29.77999965700948\n"
     ]
    }
   ],
   "source": [
    "cmd = rb.sample_command() #(200, 200)\n",
    "rb.sample_command()\n",
    "#env = gym.make('MountainCar-v0')\n",
    "e, model, _, l = load_model(name='lunar_lander_sample_actions', train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "# _, mean_reward = rollout(episodes=1, env=env, model=model, sample_action=False, \n",
    "#                       replay_buffer=rb, render=True, device=device, action_fn=action_fn)\n",
    "_, mean_reward = rollout(episodes=100, env=env, model=model_sample, sample_action=False, \n",
    "                      cmd=cmd, render=False, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
