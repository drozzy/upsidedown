{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from experiment import rollout, ReplayBuffer, Trajectory, load_model, save_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.relu(self.fc1(x))\n",
    "        output = torch.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = torch.nn.CrossEntropyLoss() #torch.nn.BCEWithLogitsLoss() #torch.nn.CrossEntropyLoss().to(device)\n",
    "model_sample = Behavior(input_shape=env.observation_space.shape[0]+2, num_actions=env.action_space.n).to(device) #env.action_space.n\n",
    "optimizer = torch.optim.Adam(model_sample.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: -240.4364181122449\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=50, last_few=50)\n",
    "\n",
    "# Random rollout\n",
    "trajectories, avg_reward = rollout(episodes=10, env=env, render=False)\n",
    "rb.add(trajectories)\n",
    "\n",
    "print(f\"Average Episode Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    optimizer.zero_grad()    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_object(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def action_fn(model, inputs, sample_action=True):\n",
    "    action_logits = model(inputs)\n",
    "    action_probs = torch.softmax(action_logits, axis=-1)\n",
    "\n",
    "    if sample_action:\n",
    "        global steps_done\n",
    "        sample = random.random()\n",
    "        eps_threshold = 0.0 #EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        \n",
    "                \n",
    "        #if sample > eps_threshold:\n",
    "        m = torch.distributions.categorical.Categorical(logits=action_logits)             \n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "#         else:\n",
    "#             action = random.randrange(env.action_space.n)\n",
    "    else:\n",
    "        action = int(np.argmax(action_probs.detach().squeeze().cpu().numpy()))\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Creating new model.\n",
      "Average Episode Reward: -204.67461045033377\n",
      "i: 0, Loss: 1.3611431121826172\n",
      "Average Episode Reward: -118.59136830172977\n",
      "i: 10, Loss: 1.3151540756225586\n",
      "Average Episode Reward: -83.52454752224455\n",
      "i: 20, Loss: 1.2771064043045044\n",
      "Average Episode Reward: -66.55097213741024\n",
      "i: 30, Loss: 1.2326279878616333\n",
      "Average Episode Reward: -55.06935499634218\n",
      "i: 40, Loss: 1.1914715766906738\n",
      "Average Episode Reward: -56.87821724604019\n",
      "i: 50, Loss: 1.1493827104568481\n",
      "Average Episode Reward: -49.57860491065807\n",
      "i: 60, Loss: 1.1125949621200562\n",
      "Average Episode Reward: -51.07264866424924\n",
      "i: 70, Loss: 1.0807398557662964\n",
      "Average Episode Reward: -43.71636288326839\n",
      "i: 80, Loss: 1.04901123046875\n",
      "Average Episode Reward: -35.72996024661258\n",
      "i: 90, Loss: 1.0212032794952393\n",
      "Average Episode Reward: -33.46904584220375\n",
      "i: 100, Loss: 0.9997531175613403\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE ACTIONS\n",
    "\n",
    "loss_sum = 0\n",
    "loss_count = 0\n",
    "\n",
    "epochs = 1000000\n",
    "epoch, model_sample, optimizer, loss = load_model('lunar_lander_sample_actions', model_sample, optimizer, device, train=True)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "for i in range(epoch, epochs+epoch):\n",
    "    for _ in range(50):\n",
    "        x, y = rb.sample(batch_size, device)    \n",
    "        loss = train_step(model_sample, x, y)\n",
    "        loss_sum += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "    if i == 0:\n",
    "        n_episodes_per_iter = 10\n",
    "    else:\n",
    "        n_episodes_per_iter = 10\n",
    "        \n",
    "    trajectories, mean_reward = rollout(n_episodes_per_iter, env=env, model=model_sample, sample_action=True, replay_buffer=rb, \n",
    "                          device=device, action_fn=action_fn)\n",
    "    rb.add(trajectories)\n",
    "    \n",
    "    if i % 20:\n",
    "        steps_done += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Average Episode Reward: {mean_reward}\")\n",
    "        avg_loss = loss_sum/loss_count\n",
    "        print(f'i: {i}, Loss: {avg_loss}') #'\\t Accuracy: {accuracy_m.result()}')\n",
    "        save_model('lunar_lander_sample_actions', i, model_sample, optimizer, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-101.11198134691003, 58),\n",
       " (-103.64699052774198, 68),\n",
       " (-127.58791857723014, 84),\n",
       " (-131.89672730059758, 155),\n",
       " (-163.5911319642045, 124),\n",
       " (-210.31447991516205, 82),\n",
       " (-275.18922879202216, 102),\n",
       " (-305.9108282899752, 96),\n",
       " (-351.6562497742343, 109),\n",
       " (-410.746399174143, 93)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(rb, open(\"buffer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbbb = pickle.load(open(\"buffer.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(310.66825831675976, 381),\n",
       " (307.26196994702906, 356),\n",
       " (303.24897175129075, 434),\n",
       " (302.4825435649981, 422),\n",
       " (302.08153217515587, 443),\n",
       " (301.62343453588, 450),\n",
       " (301.336795776998, 374),\n",
       " (301.0824957187764, 401),\n",
       " (300.30239077159666, 393),\n",
       " (300.00413043635956, 376),\n",
       " (299.5551268182258, 403),\n",
       " (299.4571607913125, 401),\n",
       " (299.3745826281496, 439),\n",
       " (299.27842296736947, 832),\n",
       " (298.9785312032605, 431),\n",
       " (298.9744925975774, 368),\n",
       " (298.8005255482776, 377),\n",
       " (298.2905025908745, 374),\n",
       " (298.25199484161163, 327),\n",
       " (298.1082917441395, 426),\n",
       " (298.01858655378317, 380),\n",
       " (297.86267734638704, 387),\n",
       " (297.44062399672396, 222),\n",
       " (297.167313218092, 372),\n",
       " (296.8036351123287, 346),\n",
       " (296.40188358973296, 677),\n",
       " (296.3219972655071, 392),\n",
       " (296.0394593078145, 377),\n",
       " (295.79209420786935, 187),\n",
       " (295.5912289208195, 415),\n",
       " (295.2668316842348, 389),\n",
       " (295.1391259645944, 284),\n",
       " (295.12430763923635, 409),\n",
       " (295.10259048325526, 365),\n",
       " (294.76372007622194, 365),\n",
       " (294.67945424467996, 425),\n",
       " (294.2226482587089, 367),\n",
       " (293.3617890483258, 420),\n",
       " (292.5447532344713, 360),\n",
       " (292.42745334494384, 363),\n",
       " (292.2048495687508, 371),\n",
       " (291.89594233247914, 194),\n",
       " (291.7759414623758, 386),\n",
       " (291.76475823960226, 328),\n",
       " (291.6613137109741, 380),\n",
       " (291.4135601366796, 385),\n",
       " (291.205157072951, 402),\n",
       " (291.0094553445198, 463),\n",
       " (291.00806211006966, 378),\n",
       " (290.8770827395257, 527)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rbbb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(305.6909235090742, 225),\n",
       " (302.0056402212026, 208),\n",
       " (301.7944143213541, 269),\n",
       " (300.71844923634444, 224),\n",
       " (300.65080857502187, 224),\n",
       " (298.2907459656101, 222),\n",
       " (296.1363344579555, 217),\n",
       " (295.11447143257334, 237),\n",
       " (295.01556728710614, 219),\n",
       " (293.1993229567172, 212),\n",
       " (293.05844646098535, 348),\n",
       " (289.4628917571782, 190),\n",
       " (289.20918684615805, 184),\n",
       " (282.6850227305037, 196),\n",
       " (280.199684161463, 162),\n",
       " (279.1794201095174, 193),\n",
       " (278.78865415341886, 211),\n",
       " (272.22855976041046, 209),\n",
       " (271.5107908143556, 215),\n",
       " (269.0895324255778, 209),\n",
       " (268.8088351096112, 190),\n",
       " (266.1338566930086, 227),\n",
       " (265.524661876984, 203),\n",
       " (262.22279884775435, 343),\n",
       " (260.5030123683403, 209),\n",
       " (258.43169595399274, 225),\n",
       " (254.9498661720394, 195),\n",
       " (254.78018048064902, 386),\n",
       " (253.8927482723219, 276),\n",
       " (253.66807584372114, 190),\n",
       " (253.6187685539958, 180),\n",
       " (253.49713835959105, 172),\n",
       " (252.58063975061393, 183),\n",
       " (252.29265489999935, 228),\n",
       " (250.91490513950285, 205),\n",
       " (250.41154765153993, 183),\n",
       " (248.50605915924794, 192),\n",
       " (247.22072380685617, 200),\n",
       " (246.99935073050054, 213),\n",
       " (246.97439714292065, 184),\n",
       " (244.68541568546854, 450),\n",
       " (243.65345306341987, 184),\n",
       " (239.08654579553945, 190),\n",
       " (237.6718367336331, 198),\n",
       " (235.88090564504827, 191),\n",
       " (222.30144667188446, 244),\n",
       " (222.09185215128173, 319),\n",
       " (-9.019525522150943, 395),\n",
       " (-113.24850799013994, 348),\n",
       " (-135.79300254428543, 374)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xxx.total_return, xxx.length) for xxx in rb.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394.48, 297.9185157945218)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading from epoch 1820 with loss: 0.600623369216919\n",
      "Average Episode Reward: 29.77999965700948\n"
     ]
    }
   ],
   "source": [
    "cmd = rb.sample_command() #(200, 200)\n",
    "rb.sample_command()\n",
    "#env = gym.make('MountainCar-v0')\n",
    "e, model, _, l = load_model(name='lunar_lander_sample_actions', train=False, model=model_sample, optimizer=optimizer, device=device)\n",
    "\n",
    "# _, mean_reward = rollout(episodes=1, env=env, model=model, sample_action=False, \n",
    "#                       replay_buffer=rb, render=True, device=device, action_fn=action_fn)\n",
    "_, mean_reward = rollout(episodes=100, env=env, model=model_sample, sample_action=False, \n",
    "                      cmd=cmd, render=False, device=device, action_fn=action_fn)\n",
    "\n",
    "\n",
    "print(f\"Average Episode Reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
