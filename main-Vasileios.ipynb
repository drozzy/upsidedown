{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trajectory = []\n",
    "        self.total_return = 0\n",
    "        self.length = 0\n",
    "        \n",
    "    def add(self, state, action, reward, state_prime):\n",
    "        self.trajectory.append((state, action, reward, state_prime))\n",
    "        self.total_return += reward\n",
    "        self.length += 1\n",
    "        \n",
    "    def sample_segment(self):\n",
    "        T = len(self.trajectory)\n",
    "\n",
    "        t1 = np.random.randint(1, T+1)\n",
    "        t2 = np.random.randint(t1, T+1)\n",
    "\n",
    "        state = self.trajectory[t1-1][0]\n",
    "        action = self.trajectory[t1-1][1]\n",
    "\n",
    "        d_r = 0.0\n",
    "        for i in range(t1, t2 + 1):\n",
    "            d_r += self.trajectory[i-1][2]\n",
    "\n",
    "        d_h = t2 - t1 + 1.0\n",
    "\n",
    "        return ((state,d_r,d_h),action)\n",
    "    \n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size, last_few):\n",
    "        \"\"\"\n",
    "        @param last_few: Number of episodes from the end of the replay buffer\n",
    "        used for sampling exploratory commands.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.cur_size = 0\n",
    "        self.buffer = []\n",
    "        \n",
    "        self.last_few = last_few\n",
    "        \n",
    "    def add(self, trajectory):\n",
    "        self.buffer.append(trajectory)\n",
    "        \n",
    "        self.buffer = sorted(self.buffer, key=lambda x: x.total_return, reverse=True)\n",
    "        self.buffer = self.buffer[:self.max_size]\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        trajectories = np.random.choice(self.buffer, batch_size, replace=True)\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        for t in trajectories:\n",
    "            segments.append(t.sample_segment())\n",
    "            \n",
    "        return segments\n",
    "    \n",
    "    def sample_command(self):\n",
    "        eps = self.buffer[:self.last_few]\n",
    "        \n",
    "        dh_0 = np.mean([e.length for e in eps])\n",
    "        \n",
    "        m = np.mean([e.total_return for e in eps])\n",
    "        s = np.std([e.total_return for e in eps])\n",
    "        \n",
    "        dr_0 = np.random.uniform(m, m+s)\n",
    "        \n",
    "        return dh_0, dr_0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-1. Initialize replay buffer with warm-up episodes using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 22.2368\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(5000, 100)\n",
    "\n",
    "avg_rewards = []\n",
    "\n",
    "for _ in range(5000):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    t = Trajectory()\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        s_old = s\n",
    "        action = env.action_space.sample()\n",
    "        s, reward, done, info = env.step(action)\n",
    "        t.add(s_old, action, reward, s)\n",
    "        ep_reward += reward\n",
    "    avg_rewards.append(ep_reward)    \n",
    "    # print(f'Episode reward: {ep_reward}')\n",
    "    rb.add(t)\n",
    "    \n",
    "    \n",
    "env.close()\n",
    "print(f\"Average Episode Reward: {np.mean(avg_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-2 Initialize a behavior function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate\n",
    "    linearly for the first ``warmup_steps`` training steps, and decreasing it thereafter proportionally\n",
    "    to the inverse square root of the step number, scaled by the inverse square root of the\n",
    "    dimensionality of the model. Time will tell if this is just madness or it's actually important.\n",
    "    Parameters\n",
    "    ----------\n",
    "    warmup_steps: ``int``, required.\n",
    "        The number of steps to linearly increase the learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        last_epoch = max(1, self.last_epoch)\n",
    "        scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n",
    "        return [base_lr * scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.fc3 = nn.Linear(512,512)\n",
    "        self.fc4 = nn.Linear(512,512)\n",
    "        self.fc5 = nn.Linear(512,num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.relu(self.fc1(x))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = F.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "d = env.observation_space.shape[0]\n",
    "model = Behavior(input_shape=d+2, num_actions=1).to(device) # env.action_space.n\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "lr_scheduler = NoamLR(optimizer, 50000)\n",
    "\n",
    "loss_object = torch.nn.BCEWithLogitsLoss().to(device) #CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A1-3: while stopping criteria is not reached do:\n",
    "### A1-4:   Improve the behavior function by training on replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sum = 0\n",
    "loss_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 200, Loss: 0.6454012393951416\n",
      "i: 400, Loss: 0.64532470703125\n",
      "i: 600, Loss: 0.6452473402023315\n",
      "i: 800, Loss: 0.6451734900474548\n",
      "Average Episode Reward: 95.887\n",
      "i: 1000, Loss: 0.6451003551483154\n",
      "i: 1200, Loss: 0.6450269222259521\n",
      "i: 1400, Loss: 0.6449548006057739\n",
      "i: 1600, Loss: 0.644880473613739\n",
      "i: 1800, Loss: 0.6448032855987549\n",
      "Average Episode Reward: 152.697\n",
      "i: 2000, Loss: 0.6447285413742065\n",
      "i: 2200, Loss: 0.6446495056152344\n",
      "i: 2400, Loss: 0.644574761390686\n",
      "i: 2600, Loss: 0.6445000767707825\n",
      "i: 2800, Loss: 0.6444293260574341\n",
      "Average Episode Reward: 108.545\n",
      "i: 3000, Loss: 0.6443573832511902\n",
      "i: 3200, Loss: 0.6442849636077881\n",
      "i: 3400, Loss: 0.6442137360572815\n",
      "i: 3600, Loss: 0.6441410183906555\n",
      "i: 3800, Loss: 0.6440678238868713\n",
      "Average Episode Reward: 144.435\n",
      "i: 4000, Loss: 0.6439946889877319\n",
      "i: 4200, Loss: 0.6439268589019775\n",
      "i: 4400, Loss: 0.6438541412353516\n",
      "i: 4600, Loss: 0.6437812447547913\n",
      "i: 4800, Loss: 0.6437135338783264\n",
      "Average Episode Reward: 144.913\n",
      "i: 5000, Loss: 0.6436429619789124\n",
      "i: 5200, Loss: 0.6435762643814087\n",
      "i: 5400, Loss: 0.6435075402259827\n",
      "i: 5600, Loss: 0.6434403657913208\n",
      "i: 5800, Loss: 0.6433693170547485\n",
      "Average Episode Reward: 112.179\n",
      "i: 6000, Loss: 0.6433051228523254\n",
      "i: 6200, Loss: 0.6432350873947144\n",
      "i: 6400, Loss: 0.6431713104248047\n",
      "i: 6600, Loss: 0.6431050300598145\n",
      "i: 6800, Loss: 0.643038809299469\n",
      "Average Episode Reward: 131.508\n",
      "i: 7000, Loss: 0.6429746747016907\n",
      "i: 7200, Loss: 0.6429107785224915\n"
     ]
    }
   ],
   "source": [
    "def to_training(s, dr, dh):\n",
    "    l = s.tolist()\n",
    "    l.append(dr)\n",
    "    l.append(dh)\n",
    "    return l\n",
    "\n",
    "def segments_to_training(segments):\n",
    "    x = []\n",
    "    y = []\n",
    "    for (s, dr, dh), action in segments:\n",
    "        l = to_training(s, dr, dh)\n",
    "        x.append(l)\n",
    "        y.append(action)\n",
    "        \n",
    "    x = torch.tensor(x).float().to(device)\n",
    "    y = torch.tensor(y).float().to(device)\n",
    "    \n",
    "    return x, y\n",
    "        \n",
    "# accuracy_m = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    #print(predictions, targets)\n",
    "    \n",
    "    loss = loss_object(predictions, targets.unsqueeze(1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def generate_episode(cmd):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    \n",
    "    t = Trajectory()\n",
    "    while not done:\n",
    "        (dh, dr) = cmd\n",
    "        inputs = torch.tensor([to_training(s, dr, dh)]).float().to(device)\n",
    "        \n",
    "        action_probs = model(inputs)\n",
    "        action_probs = F.sigmoid(action_probs) #, dim=-1)\n",
    "        \n",
    "        m = torch.distributions.bernoulli.Bernoulli(probs=action_probs) #categorical.Categorical(probs=action_probs)\n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "        \n",
    "        # env.render()\n",
    "        s_old = s\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        t.add(s_old, action, reward, s)\n",
    "        \n",
    "        ep_reward += reward\n",
    "        dh = dh - 1\n",
    "        dr = dr - reward\n",
    "        cmd = (dh, dr)\n",
    "    \n",
    "    # print(f'Episode reward: {ep_reward}')\n",
    "    rb.add(t)\n",
    "    return ep_reward\n",
    "    \n",
    "    \n",
    "epochs = 1000000\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    segments = rb.sample(batch_size)\n",
    "    segments = np.array(segments)\n",
    "    x, y = segments_to_training(segments)\n",
    "    loss = train_step(x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    #if i % 1000 == 0:\n",
    "    lr_scheduler.step()\n",
    "    #print(lr_scheduler.get_lr())\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        rewards = [] \n",
    "        for e in range(1000):\n",
    "            cmd = rb.sample_command()\n",
    "            rewards.append(generate_episode(cmd))\n",
    "        \n",
    "        print(f\"Average Episode Reward: {np.mean(rewards)}\")\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(f'i: {i}, Loss: {loss_sum/loss_count}') #'\\t Accuracy: {accuracy_m.result()}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A1:6 Generate episodes using Alg 2 and add to replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (500, 500) #rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 22.03\n"
     ]
    }
   ],
   "source": [
    "avg_rewards = []\n",
    "\n",
    "def test(cmd):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        (dh, dr) = cmd\n",
    "        inputs = torch.tensor([to_training(s, dr, dh)]).float().to(device)\n",
    "        \n",
    "        action_probs = model(inputs)\n",
    "        action_probs = F.sigmoid(action_probs) #, dim=-1)\n",
    "        \n",
    "        m = torch.distributions.bernoulli.Bernoulli(probs=action_probs) #torch.distributions.categorical.Categorical(probs=action_probs)\n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "        \n",
    "        #env.render()\n",
    "        s_old = s\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        \n",
    "        ep_reward += reward\n",
    "        dh = dh - 1\n",
    "        dr = dr - reward\n",
    "        cmd = (dh, dr)\n",
    "    \n",
    "    # print(f'Episode reward: {ep_reward}')\n",
    "    return ep_reward\n",
    "\n",
    "rewards = [] \n",
    "for e in range(100):\n",
    "    rewards.append(test(cmd))\n",
    "\n",
    "# env.close()\n",
    "print(f\"Average Episode Reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
