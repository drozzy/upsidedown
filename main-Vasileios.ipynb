{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trajectory = []\n",
    "        self.total_return = 0\n",
    "        self.length = 0\n",
    "        \n",
    "    def add(self, state, action, reward, state_prime):\n",
    "        self.trajectory.append((state, action, reward, state_prime))\n",
    "        self.total_return += reward\n",
    "        self.length += 1\n",
    "        \n",
    "    def sample_segment(self):\n",
    "        T = len(self.trajectory)\n",
    "\n",
    "        t1 = np.random.randint(1, T+1)\n",
    "        t2 = np.random.randint(t1, T+1)\n",
    "\n",
    "        state = self.trajectory[t1-1][0]\n",
    "        action = self.trajectory[t1-1][1]\n",
    "\n",
    "        d_r = 0.0\n",
    "        for i in range(t1, t2 + 1):\n",
    "            d_r += self.trajectory[i-1][2]\n",
    "\n",
    "        d_h = t2 - t1 + 1.0\n",
    "\n",
    "        return ((state,d_r,d_h),action)\n",
    "    \n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size, last_few):\n",
    "        \"\"\"\n",
    "        @param last_few: Number of episodes from the end of the replay buffer\n",
    "        used for sampling exploratory commands.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.cur_size = 0\n",
    "        self.buffer = []\n",
    "        \n",
    "        self.last_few = last_few\n",
    "        \n",
    "    def add(self, trajectory):\n",
    "        self.buffer.append(trajectory)\n",
    "        \n",
    "        self.buffer = sorted(self.buffer, key=lambda x: x.total_return, reverse=True)\n",
    "        self.buffer = self.buffer[:self.max_size]\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        trajectories = np.random.choice(self.buffer, batch_size, replace=True)\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        for t in trajectories:\n",
    "            segments.append(t.sample_segment())\n",
    "            \n",
    "        return segments\n",
    "    \n",
    "    def sample_command(self):\n",
    "        eps = self.buffer[:self.last_few]\n",
    "        \n",
    "        dh_0 = np.mean([e.length for e in eps])\n",
    "        \n",
    "        m = np.mean([e.total_return for e in eps])\n",
    "        s = np.std([e.total_return for e in eps])\n",
    "        \n",
    "        dr_0 = np.random.uniform(m, m+s)\n",
    "        \n",
    "        return dh_0, dr_0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-1. Initialize replay buffer with warm-up episodes using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 22.2368\n"
     ]
    }
   ],
   "source": [
    "rb = ReplayBuffer(5000, 100)\n",
    "\n",
    "avg_rewards = []\n",
    "\n",
    "for _ in range(5000):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    t = Trajectory()\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        s_old = s\n",
    "        action = env.action_space.sample()\n",
    "        s, reward, done, info = env.step(action)\n",
    "        t.add(s_old, action, reward, s)\n",
    "        ep_reward += reward\n",
    "    avg_rewards.append(ep_reward)    \n",
    "    # print(f'Episode reward: {ep_reward}')\n",
    "    rb.add(t)\n",
    "    \n",
    "    \n",
    "env.close()\n",
    "print(f\"Average Episode Reward: {np.mean(avg_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1-2 Initialize a behavior function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate\n",
    "    linearly for the first ``warmup_steps`` training steps, and decreasing it thereafter proportionally\n",
    "    to the inverse square root of the step number, scaled by the inverse square root of the\n",
    "    dimensionality of the model. Time will tell if this is just madness or it's actually important.\n",
    "    Parameters\n",
    "    ----------\n",
    "    warmup_steps: ``int``, required.\n",
    "        The number of steps to linearly increase the learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        last_epoch = max(1, self.last_epoch)\n",
    "        scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n",
    "        return [base_lr * scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Behavior, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.fc3 = nn.Linear(512,512)\n",
    "        self.fc4 = nn.Linear(512,512)\n",
    "        self.fc5 = nn.Linear(512,num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.relu(self.fc1(x))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = F.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "d = env.observation_space.shape[0]\n",
    "model = Behavior(input_shape=d+2, num_actions=1).to(device) # env.action_space.n\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "lr_scheduler = NoamLR(optimizer, 50000)\n",
    "\n",
    "loss_object = torch.nn.BCEWithLogitsLoss().to(device) #CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A1-3: while stopping criteria is not reached do:\n",
    "### A1-4:   Improve the behavior function by training on replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sum = 0\n",
    "loss_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 200, Loss: 0.6266692876815796\n",
      "i: 400, Loss: 0.6266453266143799\n",
      "i: 600, Loss: 0.6266225576400757\n",
      "i: 800, Loss: 0.626600980758667\n",
      "Average Episode Reward: 330.419\n",
      "i: 1000, Loss: 0.6265760660171509\n",
      "i: 1200, Loss: 0.6265539526939392\n",
      "i: 1400, Loss: 0.6265305280685425\n",
      "i: 1600, Loss: 0.6265065670013428\n",
      "i: 1800, Loss: 0.6264835000038147\n",
      "Average Episode Reward: 363.18\n",
      "i: 2000, Loss: 0.6264581680297852\n",
      "i: 2200, Loss: 0.6264347434043884\n",
      "i: 2400, Loss: 0.626413106918335\n",
      "i: 2600, Loss: 0.6263912916183472\n",
      "i: 2800, Loss: 0.6263686418533325\n",
      "Average Episode Reward: 344.886\n",
      "i: 3000, Loss: 0.6263457536697388\n",
      "i: 3200, Loss: 0.6263212561607361\n",
      "i: 3400, Loss: 0.6262988448143005\n",
      "i: 3600, Loss: 0.6262755990028381\n",
      "i: 3800, Loss: 0.6262524127960205\n",
      "Average Episode Reward: 272.719\n",
      "i: 4000, Loss: 0.6262301802635193\n",
      "i: 4200, Loss: 0.6262065768241882\n",
      "i: 4400, Loss: 0.626183271408081\n",
      "i: 4600, Loss: 0.6261605620384216\n",
      "i: 4800, Loss: 0.6261381506919861\n",
      "Average Episode Reward: 183.407\n",
      "i: 5000, Loss: 0.6261146664619446\n",
      "i: 5200, Loss: 0.6260920166969299\n",
      "i: 5400, Loss: 0.6260697841644287\n",
      "i: 5600, Loss: 0.6260475516319275\n",
      "i: 5800, Loss: 0.6260241866111755\n",
      "Average Episode Reward: 377.066\n",
      "i: 6000, Loss: 0.6260029077529907\n",
      "i: 6200, Loss: 0.625980019569397\n",
      "i: 6400, Loss: 0.6259568929672241\n",
      "i: 6600, Loss: 0.6259350776672363\n",
      "i: 6800, Loss: 0.6259133815765381\n",
      "Average Episode Reward: 354.008\n",
      "i: 7000, Loss: 0.6258895993232727\n",
      "i: 7200, Loss: 0.6258673071861267\n",
      "i: 7400, Loss: 0.6258441805839539\n",
      "i: 7600, Loss: 0.6258242726325989\n",
      "i: 7800, Loss: 0.6258018016815186\n",
      "Average Episode Reward: 345.68\n",
      "i: 8000, Loss: 0.6257798671722412\n",
      "i: 8200, Loss: 0.6257592439651489\n",
      "i: 8400, Loss: 0.6257385015487671\n",
      "i: 8600, Loss: 0.6257164478302002\n",
      "i: 8800, Loss: 0.6256963014602661\n",
      "Average Episode Reward: 344.567\n",
      "i: 9000, Loss: 0.6256747841835022\n",
      "i: 9200, Loss: 0.6256538033485413\n",
      "i: 9400, Loss: 0.6256313920021057\n",
      "i: 9600, Loss: 0.6256118416786194\n",
      "i: 9800, Loss: 0.6255900263786316\n",
      "Average Episode Reward: 347.167\n",
      "i: 10000, Loss: 0.6255695223808289\n",
      "i: 10200, Loss: 0.625551164150238\n",
      "i: 10400, Loss: 0.6255313158035278\n",
      "i: 10600, Loss: 0.6255086064338684\n",
      "i: 10800, Loss: 0.6254874467849731\n",
      "Average Episode Reward: 356.738\n",
      "i: 11000, Loss: 0.6254662275314331\n",
      "i: 11200, Loss: 0.6254459023475647\n",
      "i: 11400, Loss: 0.6254234910011292\n",
      "i: 11600, Loss: 0.6254023909568787\n",
      "i: 11800, Loss: 0.6253827214241028\n",
      "Average Episode Reward: 386.103\n",
      "i: 12000, Loss: 0.6253604292869568\n",
      "i: 12200, Loss: 0.6253385543823242\n",
      "i: 12400, Loss: 0.625317394733429\n",
      "i: 12600, Loss: 0.6252992153167725\n",
      "i: 12800, Loss: 0.6252787113189697\n",
      "Average Episode Reward: 322.555\n",
      "i: 13000, Loss: 0.6252593994140625\n",
      "i: 13200, Loss: 0.6252385973930359\n",
      "i: 13400, Loss: 0.6252179145812988\n",
      "i: 13600, Loss: 0.6251981258392334\n",
      "i: 13800, Loss: 0.6251779198646545\n",
      "Average Episode Reward: 322.603\n",
      "i: 14000, Loss: 0.6251567006111145\n",
      "i: 14200, Loss: 0.6251342296600342\n",
      "i: 14400, Loss: 0.625113844871521\n",
      "i: 14600, Loss: 0.625093400478363\n",
      "i: 14800, Loss: 0.6250728964805603\n",
      "Average Episode Reward: 251.514\n",
      "i: 15000, Loss: 0.6250529885292053\n",
      "i: 15200, Loss: 0.625032365322113\n",
      "i: 15400, Loss: 0.6250128746032715\n",
      "i: 15600, Loss: 0.6249903440475464\n",
      "i: 15800, Loss: 0.6249719262123108\n",
      "Average Episode Reward: 268.901\n",
      "i: 16000, Loss: 0.6249523162841797\n",
      "i: 16200, Loss: 0.6249319911003113\n",
      "i: 16400, Loss: 0.6249101161956787\n",
      "i: 16600, Loss: 0.6248906850814819\n",
      "i: 16800, Loss: 0.6248713135719299\n",
      "Average Episode Reward: 307.122\n",
      "i: 17000, Loss: 0.6248524785041809\n",
      "i: 17200, Loss: 0.6248323917388916\n",
      "i: 17400, Loss: 0.6248124241828918\n",
      "i: 17600, Loss: 0.6247929334640503\n",
      "i: 17800, Loss: 0.6247744560241699\n",
      "Average Episode Reward: 259.673\n",
      "i: 18000, Loss: 0.6247535943984985\n",
      "i: 18200, Loss: 0.6247347593307495\n",
      "i: 18400, Loss: 0.6247154474258423\n",
      "i: 18600, Loss: 0.6246960163116455\n",
      "i: 18800, Loss: 0.6246784925460815\n",
      "Average Episode Reward: 268.801\n",
      "i: 19000, Loss: 0.624660074710846\n",
      "i: 19200, Loss: 0.6246418952941895\n",
      "i: 19400, Loss: 0.6246212720870972\n",
      "i: 19600, Loss: 0.6246024966239929\n",
      "i: 19800, Loss: 0.6245825290679932\n",
      "Average Episode Reward: 305.482\n",
      "i: 20000, Loss: 0.6245629787445068\n",
      "i: 20200, Loss: 0.6245442628860474\n",
      "i: 20400, Loss: 0.6245261430740356\n",
      "i: 20600, Loss: 0.6245089173316956\n",
      "i: 20800, Loss: 0.6244900822639465\n",
      "Average Episode Reward: 297.74\n",
      "i: 21000, Loss: 0.6244714856147766\n",
      "i: 21200, Loss: 0.624451756477356\n",
      "i: 21400, Loss: 0.6244329214096069\n",
      "i: 21600, Loss: 0.6244139671325684\n",
      "i: 21800, Loss: 0.6243957877159119\n",
      "Average Episode Reward: 374.148\n",
      "i: 22000, Loss: 0.6243771910667419\n",
      "i: 22200, Loss: 0.6243577003479004\n",
      "i: 22400, Loss: 0.6243381500244141\n",
      "i: 22600, Loss: 0.6243197917938232\n",
      "i: 22800, Loss: 0.6242998838424683\n",
      "Average Episode Reward: 363.998\n",
      "i: 23000, Loss: 0.6242809891700745\n",
      "i: 23200, Loss: 0.6242620944976807\n",
      "i: 23400, Loss: 0.6242437958717346\n",
      "i: 23600, Loss: 0.6242268681526184\n",
      "i: 23800, Loss: 0.6242085099220276\n",
      "Average Episode Reward: 357.764\n",
      "i: 24000, Loss: 0.6241896748542786\n",
      "i: 24200, Loss: 0.6241724491119385\n",
      "i: 24400, Loss: 0.6241539120674133\n",
      "i: 24600, Loss: 0.6241347193717957\n",
      "i: 24800, Loss: 0.6241156458854675\n",
      "Average Episode Reward: 341.31\n",
      "i: 25000, Loss: 0.6240977644920349\n",
      "i: 25200, Loss: 0.6240801811218262\n",
      "i: 25400, Loss: 0.624061107635498\n",
      "i: 25600, Loss: 0.6240437030792236\n",
      "i: 25800, Loss: 0.6240247488021851\n",
      "Average Episode Reward: 348.426\n",
      "i: 26000, Loss: 0.6240046620368958\n",
      "i: 26200, Loss: 0.6239858865737915\n",
      "i: 26400, Loss: 0.6239696145057678\n",
      "i: 26600, Loss: 0.6239514350891113\n",
      "i: 26800, Loss: 0.6239352226257324\n",
      "Average Episode Reward: 241.488\n",
      "i: 27000, Loss: 0.6239168047904968\n",
      "i: 27200, Loss: 0.6238986253738403\n",
      "i: 27400, Loss: 0.6238813400268555\n",
      "i: 27600, Loss: 0.6238637566566467\n",
      "i: 27800, Loss: 0.6238465309143066\n",
      "Average Episode Reward: 356.066\n",
      "i: 28000, Loss: 0.6238271594047546\n",
      "i: 28200, Loss: 0.6238108277320862\n",
      "i: 28400, Loss: 0.6237928867340088\n",
      "i: 28600, Loss: 0.6237773299217224\n",
      "i: 28800, Loss: 0.6237599849700928\n",
      "Average Episode Reward: 323.286\n",
      "i: 29000, Loss: 0.6237419247627258\n",
      "i: 29200, Loss: 0.623724639415741\n",
      "i: 29400, Loss: 0.6237061619758606\n",
      "i: 29600, Loss: 0.6236879229545593\n",
      "i: 29800, Loss: 0.6236714124679565\n",
      "Average Episode Reward: 316.958\n",
      "i: 30000, Loss: 0.6236546039581299\n",
      "i: 30200, Loss: 0.6236360669136047\n",
      "i: 30400, Loss: 0.6236183047294617\n",
      "i: 30600, Loss: 0.6235997676849365\n",
      "i: 30800, Loss: 0.6235826015472412\n",
      "Average Episode Reward: 363.72\n",
      "i: 31000, Loss: 0.6235658526420593\n",
      "i: 31200, Loss: 0.623549222946167\n",
      "i: 31400, Loss: 0.6235330700874329\n",
      "i: 31600, Loss: 0.6235161423683167\n",
      "i: 31800, Loss: 0.6234988570213318\n",
      "Average Episode Reward: 327.002\n",
      "i: 32000, Loss: 0.6234812140464783\n",
      "i: 32200, Loss: 0.6234654188156128\n",
      "i: 32400, Loss: 0.6234481334686279\n",
      "i: 32600, Loss: 0.6234318017959595\n",
      "i: 32800, Loss: 0.6234148144721985\n",
      "Average Episode Reward: 307.561\n",
      "i: 33000, Loss: 0.6233967542648315\n",
      "i: 33200, Loss: 0.6233804225921631\n",
      "i: 33400, Loss: 0.6233639121055603\n",
      "i: 33600, Loss: 0.6233471632003784\n",
      "i: 33800, Loss: 0.6233296394348145\n",
      "Average Episode Reward: 366.099\n",
      "i: 34000, Loss: 0.6233129501342773\n",
      "i: 34200, Loss: 0.6232970952987671\n",
      "i: 34400, Loss: 0.6232796907424927\n",
      "i: 34600, Loss: 0.6232624053955078\n",
      "i: 34800, Loss: 0.6232455968856812\n",
      "Average Episode Reward: 220.333\n",
      "i: 35000, Loss: 0.6232273578643799\n",
      "i: 35200, Loss: 0.6232106685638428\n",
      "i: 35400, Loss: 0.6231937408447266\n",
      "i: 35600, Loss: 0.6231779456138611\n",
      "i: 35800, Loss: 0.6231628656387329\n",
      "Average Episode Reward: 260.355\n",
      "i: 36000, Loss: 0.623146653175354\n",
      "i: 36200, Loss: 0.6231297254562378\n",
      "i: 36400, Loss: 0.6231132745742798\n",
      "i: 36600, Loss: 0.6230971217155457\n",
      "i: 36800, Loss: 0.623079776763916\n",
      "Average Episode Reward: 403.304\n",
      "i: 37000, Loss: 0.6230633854866028\n",
      "i: 37200, Loss: 0.6230463981628418\n",
      "i: 37400, Loss: 0.6230289936065674\n",
      "i: 37600, Loss: 0.6230124235153198\n",
      "i: 37800, Loss: 0.6229976415634155\n",
      "Average Episode Reward: 313.782\n",
      "i: 38000, Loss: 0.6229811906814575\n",
      "i: 38200, Loss: 0.6229641437530518\n",
      "i: 38400, Loss: 0.6229491829872131\n",
      "i: 38600, Loss: 0.622932493686676\n",
      "i: 38800, Loss: 0.622916042804718\n",
      "Average Episode Reward: 362.555\n",
      "i: 39000, Loss: 0.6229001879692078\n",
      "i: 39200, Loss: 0.6228837966918945\n",
      "i: 39400, Loss: 0.6228675246238708\n",
      "i: 39600, Loss: 0.6228504180908203\n",
      "i: 39800, Loss: 0.622834324836731\n",
      "Average Episode Reward: 329.683\n",
      "i: 40000, Loss: 0.6228179335594177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 40200, Loss: 0.6228013634681702\n",
      "i: 40400, Loss: 0.6227853894233704\n",
      "i: 40600, Loss: 0.6227695941925049\n",
      "i: 40800, Loss: 0.6227543950080872\n",
      "Average Episode Reward: 318.292\n",
      "i: 41000, Loss: 0.6227377653121948\n",
      "i: 41200, Loss: 0.6227222084999084\n",
      "i: 41400, Loss: 0.6227061748504639\n",
      "i: 41600, Loss: 0.6226914525032043\n",
      "i: 41800, Loss: 0.6226764917373657\n",
      "Average Episode Reward: 268.188\n",
      "i: 42000, Loss: 0.6226606965065002\n",
      "i: 42200, Loss: 0.6226471662521362\n",
      "i: 42400, Loss: 0.6226323843002319\n",
      "i: 42600, Loss: 0.6226170063018799\n",
      "i: 42800, Loss: 0.6226010322570801\n",
      "Average Episode Reward: 379.893\n",
      "i: 43000, Loss: 0.6225854754447937\n",
      "i: 43200, Loss: 0.6225701570510864\n",
      "i: 43400, Loss: 0.6225555539131165\n",
      "i: 43600, Loss: 0.6225407719612122\n",
      "i: 43800, Loss: 0.6225256323814392\n",
      "Average Episode Reward: 314.976\n",
      "i: 44000, Loss: 0.6225089430809021\n",
      "i: 44200, Loss: 0.6224940419197083\n",
      "i: 44400, Loss: 0.6224792003631592\n",
      "i: 44600, Loss: 0.6224632859230042\n",
      "i: 44800, Loss: 0.6224494576454163\n",
      "Average Episode Reward: 298.757\n",
      "i: 45000, Loss: 0.6224346160888672\n",
      "i: 45200, Loss: 0.6224189400672913\n",
      "i: 45400, Loss: 0.6224036812782288\n",
      "i: 45600, Loss: 0.6223890781402588\n",
      "i: 45800, Loss: 0.6223755478858948\n",
      "Average Episode Reward: 330.458\n",
      "i: 46000, Loss: 0.6223607659339905\n",
      "i: 46200, Loss: 0.6223462224006653\n",
      "i: 46400, Loss: 0.6223310232162476\n",
      "i: 46600, Loss: 0.6223183870315552\n",
      "i: 46800, Loss: 0.6223018169403076\n",
      "Average Episode Reward: 364.716\n",
      "i: 47000, Loss: 0.6222873330116272\n",
      "i: 47200, Loss: 0.6222726106643677\n",
      "i: 47400, Loss: 0.6222565770149231\n",
      "i: 47600, Loss: 0.6222425699234009\n",
      "i: 47800, Loss: 0.6222282648086548\n",
      "Average Episode Reward: 252.215\n",
      "i: 48000, Loss: 0.6222130656242371\n",
      "i: 48200, Loss: 0.6221985816955566\n",
      "i: 48400, Loss: 0.6221835613250732\n",
      "i: 48600, Loss: 0.6221694350242615\n",
      "i: 48800, Loss: 0.6221558451652527\n",
      "Average Episode Reward: 336.113\n",
      "i: 49000, Loss: 0.6221407651901245\n",
      "i: 49200, Loss: 0.6221253871917725\n",
      "i: 49400, Loss: 0.6221128106117249\n",
      "i: 49600, Loss: 0.6220989227294922\n",
      "i: 49800, Loss: 0.6220834851264954\n",
      "Average Episode Reward: 389.776\n",
      "i: 50000, Loss: 0.622069776058197\n",
      "i: 50200, Loss: 0.6220542788505554\n",
      "i: 50400, Loss: 0.6220405697822571\n",
      "i: 50600, Loss: 0.6220270991325378\n",
      "i: 50800, Loss: 0.6220122575759888\n",
      "Average Episode Reward: 375.274\n",
      "i: 51000, Loss: 0.6219977736473083\n",
      "i: 51200, Loss: 0.6219834089279175\n",
      "i: 51400, Loss: 0.6219700574874878\n",
      "i: 51600, Loss: 0.6219562888145447\n",
      "i: 51800, Loss: 0.6219419240951538\n",
      "Average Episode Reward: 250.582\n",
      "i: 52000, Loss: 0.6219271421432495\n",
      "i: 52200, Loss: 0.6219136118888855\n",
      "i: 52400, Loss: 0.6218982338905334\n",
      "i: 52600, Loss: 0.6218842267990112\n",
      "i: 52800, Loss: 0.6218702793121338\n",
      "Average Episode Reward: 345.141\n",
      "i: 53000, Loss: 0.6218550801277161\n",
      "i: 53200, Loss: 0.6218406558036804\n",
      "i: 53400, Loss: 0.6218267679214478\n",
      "i: 53600, Loss: 0.6218132972717285\n",
      "i: 53800, Loss: 0.6217997074127197\n",
      "Average Episode Reward: 315.349\n",
      "i: 54000, Loss: 0.6217859983444214\n",
      "i: 54200, Loss: 0.6217725276947021\n",
      "i: 54400, Loss: 0.6217582821846008\n",
      "i: 54600, Loss: 0.6217452883720398\n",
      "i: 54800, Loss: 0.6217309236526489\n",
      "Average Episode Reward: 369.763\n",
      "i: 55000, Loss: 0.6217172741889954\n",
      "i: 55200, Loss: 0.6217037439346313\n",
      "i: 55400, Loss: 0.6216897964477539\n",
      "i: 55600, Loss: 0.6216747760772705\n",
      "i: 55800, Loss: 0.6216606497764587\n",
      "Average Episode Reward: 209.517\n",
      "i: 56000, Loss: 0.6216479539871216\n",
      "i: 56200, Loss: 0.6216333508491516\n",
      "i: 56400, Loss: 0.6216204166412354\n",
      "i: 56600, Loss: 0.6216060519218445\n",
      "i: 56800, Loss: 0.6215925216674805\n",
      "Average Episode Reward: 183.987\n",
      "i: 57000, Loss: 0.6215786337852478\n",
      "i: 57200, Loss: 0.6215659976005554\n",
      "i: 57400, Loss: 0.6215521693229675\n",
      "i: 57600, Loss: 0.6215401291847229\n",
      "i: 57800, Loss: 0.6215266585350037\n",
      "Average Episode Reward: 304.838\n",
      "i: 58000, Loss: 0.6215130090713501\n",
      "i: 58200, Loss: 0.6214993000030518\n",
      "i: 58400, Loss: 0.6214869022369385\n",
      "i: 58600, Loss: 0.6214739084243774\n",
      "i: 58800, Loss: 0.6214605569839478\n",
      "Average Episode Reward: 261.461\n",
      "i: 59000, Loss: 0.6214463710784912\n",
      "i: 59200, Loss: 0.6214333176612854\n",
      "i: 59400, Loss: 0.6214194297790527\n",
      "i: 59600, Loss: 0.6214061379432678\n",
      "i: 59800, Loss: 0.6213918328285217\n",
      "Average Episode Reward: 401.268\n",
      "i: 60000, Loss: 0.6213780641555786\n",
      "i: 60200, Loss: 0.6213648915290833\n",
      "i: 60400, Loss: 0.6213517189025879\n",
      "i: 60600, Loss: 0.6213390231132507\n",
      "i: 60800, Loss: 0.6213254332542419\n",
      "Average Episode Reward: 263.826\n",
      "i: 61000, Loss: 0.6213109493255615\n",
      "i: 61200, Loss: 0.6212975382804871\n",
      "i: 61400, Loss: 0.6212850213050842\n",
      "i: 61600, Loss: 0.6212725639343262\n",
      "i: 61800, Loss: 0.6212608814239502\n",
      "Average Episode Reward: 325.756\n",
      "i: 62000, Loss: 0.6212477087974548\n",
      "i: 62200, Loss: 0.6212339997291565\n",
      "i: 62400, Loss: 0.6212209463119507\n",
      "i: 62600, Loss: 0.621207058429718\n",
      "i: 62800, Loss: 0.6211948394775391\n",
      "Average Episode Reward: 402.455\n",
      "i: 63000, Loss: 0.621182382106781\n",
      "i: 63200, Loss: 0.6211701035499573\n",
      "i: 63400, Loss: 0.6211583018302917\n",
      "i: 63600, Loss: 0.6211448311805725\n",
      "i: 63800, Loss: 0.6211321949958801\n",
      "Average Episode Reward: 369.443\n",
      "i: 64000, Loss: 0.6211204528808594\n",
      "i: 64200, Loss: 0.6211082339286804\n",
      "i: 64400, Loss: 0.6210957765579224\n",
      "i: 64600, Loss: 0.6210820078849792\n",
      "i: 64800, Loss: 0.6210697293281555\n",
      "Average Episode Reward: 299.892\n",
      "i: 65000, Loss: 0.621056318283081\n",
      "i: 65200, Loss: 0.6210434436798096\n",
      "i: 65400, Loss: 0.6210301518440247\n",
      "i: 65600, Loss: 0.6210177540779114\n",
      "i: 65800, Loss: 0.621006190776825\n",
      "Average Episode Reward: 228.012\n",
      "i: 66000, Loss: 0.6209931373596191\n",
      "i: 66200, Loss: 0.620980978012085\n",
      "i: 66400, Loss: 0.6209682822227478\n",
      "i: 66600, Loss: 0.6209568381309509\n",
      "i: 66800, Loss: 0.6209437847137451\n",
      "Average Episode Reward: 313.605\n",
      "i: 67000, Loss: 0.6209311485290527\n",
      "i: 67200, Loss: 0.6209177374839783\n",
      "i: 67400, Loss: 0.6209059357643127\n",
      "i: 67600, Loss: 0.6208930611610413\n",
      "i: 67800, Loss: 0.6208826303482056\n",
      "Average Episode Reward: 392.516\n",
      "i: 68000, Loss: 0.6208706498146057\n",
      "i: 68200, Loss: 0.6208575963973999\n",
      "i: 68400, Loss: 0.6208451390266418\n",
      "i: 68600, Loss: 0.6208334565162659\n",
      "i: 68800, Loss: 0.6208214163780212\n",
      "Average Episode Reward: 203.922\n",
      "i: 69000, Loss: 0.6208089590072632\n",
      "i: 69200, Loss: 0.6207959055900574\n",
      "i: 69400, Loss: 0.6207845211029053\n",
      "i: 69600, Loss: 0.6207733154296875\n",
      "i: 69800, Loss: 0.6207610368728638\n",
      "Average Episode Reward: 275.242\n",
      "i: 70000, Loss: 0.6207475662231445\n",
      "i: 70200, Loss: 0.6207368969917297\n",
      "i: 70400, Loss: 0.6207244396209717\n",
      "i: 70600, Loss: 0.6207116842269897\n",
      "i: 70800, Loss: 0.6206992268562317\n",
      "Average Episode Reward: 408.367\n",
      "i: 71000, Loss: 0.6206860542297363\n",
      "i: 71200, Loss: 0.6206732392311096\n",
      "i: 71400, Loss: 0.6206592917442322\n",
      "i: 71600, Loss: 0.6206464767456055\n",
      "i: 71800, Loss: 0.6206340789794922\n",
      "Average Episode Reward: 309.108\n",
      "i: 72000, Loss: 0.6206221580505371\n",
      "i: 72200, Loss: 0.6206107139587402\n",
      "i: 72400, Loss: 0.6205981373786926\n",
      "i: 72600, Loss: 0.620586097240448\n",
      "i: 72800, Loss: 0.6205750107765198\n",
      "Average Episode Reward: 348.103\n",
      "i: 73000, Loss: 0.6205639243125916\n",
      "i: 73200, Loss: 0.6205529570579529\n",
      "i: 73400, Loss: 0.6205422282218933\n",
      "i: 73600, Loss: 0.6205303072929382\n",
      "i: 73800, Loss: 0.6205180287361145\n",
      "Average Episode Reward: 429.507\n",
      "i: 74000, Loss: 0.6205072999000549\n",
      "i: 74200, Loss: 0.6204946637153625\n",
      "i: 74400, Loss: 0.6204822063446045\n",
      "i: 74600, Loss: 0.6204715967178345\n",
      "i: 74800, Loss: 0.620459258556366\n",
      "Average Episode Reward: 359.522\n",
      "i: 75000, Loss: 0.6204469799995422\n",
      "i: 75200, Loss: 0.6204354166984558\n",
      "i: 75400, Loss: 0.6204219460487366\n",
      "i: 75600, Loss: 0.620410144329071\n",
      "i: 75800, Loss: 0.6203974485397339\n",
      "Average Episode Reward: 219.021\n",
      "i: 76000, Loss: 0.6203863024711609\n",
      "i: 76200, Loss: 0.620374858379364\n",
      "i: 76400, Loss: 0.6203632950782776\n",
      "i: 76600, Loss: 0.6203522682189941\n",
      "i: 76800, Loss: 0.6203415989875793\n",
      "Average Episode Reward: 283.516\n",
      "i: 77000, Loss: 0.6203288435935974\n",
      "i: 77200, Loss: 0.620317280292511\n",
      "i: 77400, Loss: 0.6203059554100037\n",
      "i: 77600, Loss: 0.6202948093414307\n",
      "i: 77800, Loss: 0.6202826499938965\n",
      "Average Episode Reward: 356.936\n",
      "i: 78000, Loss: 0.620272159576416\n",
      "i: 78200, Loss: 0.6202613115310669\n",
      "i: 78400, Loss: 0.6202496886253357\n",
      "i: 78600, Loss: 0.6202371716499329\n",
      "i: 78800, Loss: 0.6202259063720703\n",
      "Average Episode Reward: 380.99\n",
      "i: 79000, Loss: 0.6202127933502197\n",
      "i: 79200, Loss: 0.6202013492584229\n",
      "i: 79400, Loss: 0.6201897263526917\n",
      "i: 79600, Loss: 0.6201786994934082\n",
      "i: 79800, Loss: 0.62016761302948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 393.073\n",
      "i: 80000, Loss: 0.6201555728912354\n",
      "i: 80200, Loss: 0.6201431751251221\n",
      "i: 80400, Loss: 0.6201311945915222\n",
      "i: 80600, Loss: 0.6201194524765015\n",
      "i: 80800, Loss: 0.6201089024543762\n",
      "Average Episode Reward: 407.947\n",
      "i: 81000, Loss: 0.6200975179672241\n",
      "i: 81200, Loss: 0.6200874447822571\n",
      "i: 81400, Loss: 0.6200760006904602\n",
      "i: 81600, Loss: 0.6200655102729797\n",
      "i: 81800, Loss: 0.6200532913208008\n",
      "Average Episode Reward: 320.949\n",
      "i: 82000, Loss: 0.6200428605079651\n",
      "i: 82200, Loss: 0.620033323764801\n",
      "i: 82400, Loss: 0.6200231313705444\n",
      "i: 82600, Loss: 0.6200125813484192\n",
      "i: 82800, Loss: 0.6200008988380432\n",
      "Average Episode Reward: 298.726\n",
      "i: 83000, Loss: 0.619989812374115\n",
      "i: 83200, Loss: 0.6199785470962524\n",
      "i: 83400, Loss: 0.6199671626091003\n",
      "i: 83600, Loss: 0.6199563145637512\n",
      "i: 83800, Loss: 0.6199458837509155\n",
      "Average Episode Reward: 346.167\n",
      "i: 84000, Loss: 0.6199352741241455\n",
      "i: 84200, Loss: 0.6199237108230591\n",
      "i: 84400, Loss: 0.6199122071266174\n",
      "i: 84600, Loss: 0.6199022531509399\n",
      "i: 84800, Loss: 0.6198904514312744\n",
      "Average Episode Reward: 378.278\n",
      "i: 85000, Loss: 0.6198804974555969\n",
      "i: 85200, Loss: 0.6198683977127075\n",
      "i: 85400, Loss: 0.619857907295227\n",
      "i: 85600, Loss: 0.619847297668457\n",
      "i: 85800, Loss: 0.6198367476463318\n",
      "Average Episode Reward: 360.81\n",
      "i: 86000, Loss: 0.6198250651359558\n",
      "i: 86200, Loss: 0.6198140382766724\n",
      "i: 86400, Loss: 0.6198045015335083\n",
      "i: 86600, Loss: 0.6197945475578308\n",
      "i: 86800, Loss: 0.6197842955589294\n",
      "Average Episode Reward: 464.368\n",
      "i: 87000, Loss: 0.6197729706764221\n",
      "i: 87200, Loss: 0.6197632551193237\n",
      "i: 87400, Loss: 0.6197534203529358\n",
      "i: 87600, Loss: 0.6197429299354553\n",
      "i: 87800, Loss: 0.6197322010993958\n",
      "Average Episode Reward: 398.72\n",
      "i: 88000, Loss: 0.619722843170166\n",
      "i: 88200, Loss: 0.6197131872177124\n",
      "i: 88400, Loss: 0.6197026371955872\n",
      "i: 88600, Loss: 0.6196925044059753\n",
      "i: 88800, Loss: 0.6196820139884949\n",
      "Average Episode Reward: 359.25\n",
      "i: 89000, Loss: 0.6196726560592651\n",
      "i: 89200, Loss: 0.6196629405021667\n",
      "i: 89400, Loss: 0.6196534633636475\n",
      "i: 89600, Loss: 0.6196432709693909\n",
      "i: 89800, Loss: 0.6196326017379761\n",
      "Average Episode Reward: 321.92\n",
      "i: 90000, Loss: 0.6196233630180359\n",
      "i: 90200, Loss: 0.6196131706237793\n",
      "i: 90400, Loss: 0.6196029782295227\n",
      "i: 90600, Loss: 0.6195922493934631\n",
      "i: 90800, Loss: 0.6195815205574036\n",
      "Average Episode Reward: 236.152\n",
      "i: 91000, Loss: 0.619571328163147\n",
      "i: 91200, Loss: 0.6195619106292725\n",
      "i: 91400, Loss: 0.6195520162582397\n",
      "i: 91600, Loss: 0.6195406317710876\n",
      "i: 91800, Loss: 0.619529664516449\n",
      "Average Episode Reward: 253.015\n",
      "i: 92000, Loss: 0.6195187568664551\n",
      "i: 92200, Loss: 0.6195085048675537\n",
      "i: 92400, Loss: 0.6194982528686523\n",
      "i: 92600, Loss: 0.6194878220558167\n",
      "i: 92800, Loss: 0.6194777488708496\n",
      "Average Episode Reward: 391.769\n",
      "i: 93000, Loss: 0.6194687485694885\n",
      "i: 93200, Loss: 0.6194571852684021\n",
      "i: 93400, Loss: 0.619446337223053\n",
      "i: 93600, Loss: 0.6194354891777039\n",
      "i: 93800, Loss: 0.6194246411323547\n",
      "Average Episode Reward: 377.311\n",
      "i: 94000, Loss: 0.6194148659706116\n",
      "i: 94200, Loss: 0.6194051504135132\n",
      "i: 94400, Loss: 0.6193949580192566\n",
      "i: 94600, Loss: 0.6193850636482239\n",
      "i: 94800, Loss: 0.61937415599823\n",
      "Average Episode Reward: 298.645\n",
      "i: 95000, Loss: 0.6193634867668152\n",
      "i: 95200, Loss: 0.6193543076515198\n",
      "i: 95400, Loss: 0.6193432807922363\n",
      "i: 95600, Loss: 0.6193335056304932\n",
      "i: 95800, Loss: 0.6193227767944336\n",
      "Average Episode Reward: 368.519\n",
      "i: 96000, Loss: 0.6193135380744934\n",
      "i: 96200, Loss: 0.6193037033081055\n",
      "i: 96400, Loss: 0.6192940473556519\n",
      "i: 96600, Loss: 0.6192852854728699\n",
      "i: 96800, Loss: 0.6192758679389954\n",
      "Average Episode Reward: 327.815\n",
      "i: 97000, Loss: 0.6192660927772522\n",
      "i: 97200, Loss: 0.6192556023597717\n",
      "i: 97400, Loss: 0.6192458868026733\n",
      "i: 97600, Loss: 0.619236409664154\n",
      "i: 97800, Loss: 0.6192269325256348\n",
      "Average Episode Reward: 399.507\n",
      "i: 98000, Loss: 0.6192178726196289\n",
      "i: 98200, Loss: 0.619208812713623\n",
      "i: 98400, Loss: 0.6191983819007874\n",
      "i: 98600, Loss: 0.6191878318786621\n",
      "i: 98800, Loss: 0.6191781759262085\n",
      "Average Episode Reward: 389.555\n",
      "i: 99000, Loss: 0.6191680431365967\n",
      "i: 99200, Loss: 0.6191595792770386\n",
      "i: 99400, Loss: 0.6191499829292297\n",
      "i: 99600, Loss: 0.6191402673721313\n",
      "i: 99800, Loss: 0.6191298365592957\n",
      "Average Episode Reward: 394.11\n",
      "i: 100000, Loss: 0.6191192269325256\n",
      "i: 100200, Loss: 0.6191098690032959\n",
      "i: 100400, Loss: 0.6191004514694214\n",
      "i: 100600, Loss: 0.6190906763076782\n",
      "i: 100800, Loss: 0.6190801858901978\n",
      "Average Episode Reward: 272.538\n",
      "i: 101000, Loss: 0.6190698146820068\n",
      "i: 101200, Loss: 0.6190598607063293\n",
      "i: 101400, Loss: 0.6190495491027832\n",
      "i: 101600, Loss: 0.6190394163131714\n",
      "i: 101800, Loss: 0.6190304756164551\n",
      "Average Episode Reward: 339.287\n",
      "i: 102000, Loss: 0.6190207004547119\n",
      "i: 102200, Loss: 0.6190100312232971\n",
      "i: 102400, Loss: 0.6190003752708435\n",
      "i: 102600, Loss: 0.6189899444580078\n",
      "i: 102800, Loss: 0.6189802885055542\n",
      "Average Episode Reward: 329.802\n",
      "i: 103000, Loss: 0.6189702153205872\n",
      "i: 103200, Loss: 0.618960440158844\n",
      "i: 103400, Loss: 0.6189512014389038\n",
      "i: 103600, Loss: 0.618941068649292\n",
      "i: 103800, Loss: 0.6189311742782593\n",
      "Average Episode Reward: 283.65\n",
      "i: 104000, Loss: 0.6189227104187012\n",
      "i: 104200, Loss: 0.6189132928848267\n",
      "i: 104400, Loss: 0.6189035177230835\n",
      "i: 104600, Loss: 0.6188949942588806\n",
      "i: 104800, Loss: 0.6188850998878479\n",
      "Average Episode Reward: 298.389\n",
      "i: 105000, Loss: 0.6188768148422241\n",
      "i: 105200, Loss: 0.6188666224479675\n",
      "i: 105400, Loss: 0.6188578605651855\n",
      "i: 105600, Loss: 0.6188482046127319\n",
      "i: 105800, Loss: 0.6188387274742126\n",
      "Average Episode Reward: 244.723\n",
      "i: 106000, Loss: 0.6188293695449829\n",
      "i: 106200, Loss: 0.618819534778595\n",
      "i: 106400, Loss: 0.6188100576400757\n",
      "i: 106600, Loss: 0.6188008189201355\n",
      "i: 106800, Loss: 0.6187905669212341\n",
      "Average Episode Reward: 383.307\n",
      "i: 107000, Loss: 0.6187818646430969\n",
      "i: 107200, Loss: 0.618772566318512\n",
      "i: 107400, Loss: 0.6187633872032166\n",
      "i: 107600, Loss: 0.6187548041343689\n",
      "i: 107800, Loss: 0.6187452673912048\n",
      "Average Episode Reward: 340.976\n",
      "i: 108000, Loss: 0.6187366843223572\n",
      "i: 108200, Loss: 0.6187275648117065\n",
      "i: 108400, Loss: 0.6187180876731873\n",
      "i: 108600, Loss: 0.618708610534668\n",
      "i: 108800, Loss: 0.6186991930007935\n",
      "Average Episode Reward: 382.044\n",
      "i: 109000, Loss: 0.6186907887458801\n",
      "i: 109200, Loss: 0.6186814308166504\n",
      "i: 109400, Loss: 0.6186717748641968\n",
      "i: 109600, Loss: 0.6186619400978088\n",
      "i: 109800, Loss: 0.6186536550521851\n",
      "Average Episode Reward: 318.562\n",
      "i: 110000, Loss: 0.6186445951461792\n",
      "i: 110200, Loss: 0.6186354160308838\n",
      "i: 110400, Loss: 0.6186265349388123\n",
      "i: 110600, Loss: 0.6186182498931885\n",
      "i: 110800, Loss: 0.6186102628707886\n",
      "Average Episode Reward: 403.974\n",
      "i: 111000, Loss: 0.618600606918335\n",
      "i: 111200, Loss: 0.618590772151947\n",
      "i: 111400, Loss: 0.6185817122459412\n",
      "i: 111600, Loss: 0.6185734272003174\n",
      "i: 111800, Loss: 0.6185648441314697\n",
      "Average Episode Reward: 387.715\n",
      "i: 112000, Loss: 0.6185563802719116\n",
      "i: 112200, Loss: 0.6185464859008789\n",
      "i: 112400, Loss: 0.6185372471809387\n",
      "i: 112600, Loss: 0.6185284852981567\n",
      "i: 112800, Loss: 0.6185204982757568\n",
      "Average Episode Reward: 313.057\n",
      "i: 113000, Loss: 0.6185118556022644\n",
      "i: 113200, Loss: 0.6185036897659302\n",
      "i: 113400, Loss: 0.6184945702552795\n",
      "i: 113600, Loss: 0.6184859275817871\n",
      "i: 113800, Loss: 0.6184771656990051\n",
      "Average Episode Reward: 334.858\n",
      "i: 114000, Loss: 0.6184680461883545\n",
      "i: 114200, Loss: 0.6184600591659546\n",
      "i: 114400, Loss: 0.6184502840042114\n",
      "i: 114600, Loss: 0.6184413433074951\n",
      "i: 114800, Loss: 0.6184328198432922\n",
      "Average Episode Reward: 348.824\n",
      "i: 115000, Loss: 0.6184240579605103\n",
      "i: 115200, Loss: 0.6184155344963074\n",
      "i: 115400, Loss: 0.6184061765670776\n",
      "i: 115600, Loss: 0.6183982491493225\n",
      "i: 115800, Loss: 0.6183892488479614\n",
      "Average Episode Reward: 348.867\n",
      "i: 116000, Loss: 0.618380606174469\n",
      "i: 116200, Loss: 0.6183716058731079\n",
      "i: 116400, Loss: 0.618363618850708\n",
      "i: 116600, Loss: 0.618354320526123\n",
      "i: 116800, Loss: 0.6183458566665649\n",
      "Average Episode Reward: 402.576\n",
      "i: 117000, Loss: 0.6183373928070068\n",
      "i: 117200, Loss: 0.6183289289474487\n",
      "i: 117400, Loss: 0.6183204054832458\n",
      "i: 117600, Loss: 0.6183109879493713\n",
      "i: 117800, Loss: 0.6183030009269714\n",
      "Average Episode Reward: 378.25\n",
      "i: 118000, Loss: 0.6182942986488342\n",
      "i: 118200, Loss: 0.6182861328125\n",
      "i: 118400, Loss: 0.6182770133018494\n",
      "i: 118600, Loss: 0.6182688474655151\n",
      "i: 118800, Loss: 0.6182600855827332\n",
      "Average Episode Reward: 321.037\n",
      "i: 119000, Loss: 0.6182510852813721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 119200, Loss: 0.6182425618171692\n",
      "i: 119400, Loss: 0.6182339787483215\n",
      "i: 119600, Loss: 0.618225634098053\n",
      "i: 119800, Loss: 0.6182166337966919\n",
      "Average Episode Reward: 340.0\n",
      "i: 120000, Loss: 0.6182078719139099\n",
      "i: 120200, Loss: 0.6181988716125488\n",
      "i: 120400, Loss: 0.6181899905204773\n",
      "i: 120600, Loss: 0.6181817650794983\n",
      "i: 120800, Loss: 0.6181718111038208\n",
      "Average Episode Reward: 430.643\n",
      "i: 121000, Loss: 0.6181629300117493\n",
      "i: 121200, Loss: 0.6181545853614807\n",
      "i: 121400, Loss: 0.6181464791297913\n",
      "i: 121600, Loss: 0.6181380152702332\n",
      "i: 121800, Loss: 0.6181297898292542\n",
      "Average Episode Reward: 335.011\n",
      "i: 122000, Loss: 0.6181210279464722\n",
      "i: 122200, Loss: 0.6181128025054932\n",
      "i: 122400, Loss: 0.6181047558784485\n",
      "i: 122600, Loss: 0.618096649646759\n",
      "i: 122800, Loss: 0.6180891394615173\n",
      "Average Episode Reward: 338.396\n",
      "i: 123000, Loss: 0.6180803179740906\n",
      "i: 123200, Loss: 0.6180713772773743\n",
      "i: 123400, Loss: 0.6180635094642639\n",
      "i: 123600, Loss: 0.6180549263954163\n",
      "i: 123800, Loss: 0.618046760559082\n",
      "Average Episode Reward: 385.633\n",
      "i: 124000, Loss: 0.6180389523506165\n",
      "i: 124200, Loss: 0.6180321574211121\n",
      "i: 124400, Loss: 0.6180243492126465\n",
      "i: 124600, Loss: 0.6180164813995361\n",
      "i: 124800, Loss: 0.6180073618888855\n",
      "Average Episode Reward: 351.081\n",
      "i: 125000, Loss: 0.6179988384246826\n",
      "i: 125200, Loss: 0.6179906129837036\n",
      "i: 125400, Loss: 0.617982804775238\n",
      "i: 125600, Loss: 0.6179738640785217\n",
      "i: 125800, Loss: 0.6179656982421875\n",
      "Average Episode Reward: 293.789\n",
      "i: 126000, Loss: 0.6179569959640503\n",
      "i: 126200, Loss: 0.6179481744766235\n",
      "i: 126400, Loss: 0.6179402470588684\n",
      "i: 126600, Loss: 0.6179325580596924\n",
      "i: 126800, Loss: 0.6179237365722656\n",
      "Average Episode Reward: 361.996\n",
      "i: 127000, Loss: 0.6179153919219971\n",
      "i: 127200, Loss: 0.6179075241088867\n",
      "i: 127400, Loss: 0.6178995370864868\n",
      "i: 127600, Loss: 0.6178916692733765\n",
      "i: 127800, Loss: 0.6178828477859497\n",
      "Average Episode Reward: 387.759\n",
      "i: 128000, Loss: 0.6178751587867737\n",
      "i: 128200, Loss: 0.6178680658340454\n",
      "i: 128400, Loss: 0.6178603172302246\n",
      "i: 128600, Loss: 0.6178527474403381\n",
      "i: 128800, Loss: 0.6178454756736755\n",
      "Average Episode Reward: 356.233\n",
      "i: 129000, Loss: 0.6178375482559204\n",
      "i: 129200, Loss: 0.6178300380706787\n",
      "i: 129400, Loss: 0.6178225874900818\n",
      "i: 129600, Loss: 0.6178139448165894\n",
      "i: 129800, Loss: 0.6178058981895447\n",
      "Average Episode Reward: 258.28\n",
      "i: 130000, Loss: 0.6177984476089478\n",
      "i: 130200, Loss: 0.6177906394004822\n",
      "i: 130400, Loss: 0.6177816390991211\n",
      "i: 130600, Loss: 0.6177739500999451\n",
      "i: 130800, Loss: 0.6177663207054138\n",
      "Average Episode Reward: 410.333\n",
      "i: 131000, Loss: 0.6177579164505005\n",
      "i: 131200, Loss: 0.617750346660614\n",
      "i: 131400, Loss: 0.6177428364753723\n",
      "i: 131600, Loss: 0.6177355647087097\n",
      "i: 131800, Loss: 0.6177274584770203\n",
      "Average Episode Reward: 313.42\n",
      "i: 132000, Loss: 0.6177197098731995\n",
      "i: 132200, Loss: 0.6177113056182861\n",
      "i: 132400, Loss: 0.6177038550376892\n",
      "i: 132600, Loss: 0.6176961064338684\n",
      "i: 132800, Loss: 0.6176874041557312\n",
      "Average Episode Reward: 195.849\n",
      "i: 133000, Loss: 0.6176793575286865\n",
      "i: 133200, Loss: 0.6176726818084717\n",
      "i: 133400, Loss: 0.6176643371582031\n",
      "i: 133600, Loss: 0.6176562905311584\n",
      "i: 133800, Loss: 0.6176484227180481\n",
      "Average Episode Reward: 327.699\n",
      "i: 134000, Loss: 0.6176401376724243\n",
      "i: 134200, Loss: 0.6176323294639587\n",
      "i: 134400, Loss: 0.6176242232322693\n",
      "i: 134600, Loss: 0.6176164746284485\n",
      "i: 134800, Loss: 0.6176087260246277\n",
      "Average Episode Reward: 372.832\n",
      "i: 135000, Loss: 0.617601215839386\n",
      "i: 135200, Loss: 0.6175930500030518\n",
      "i: 135400, Loss: 0.6175858974456787\n",
      "i: 135600, Loss: 0.6175785660743713\n",
      "i: 135800, Loss: 0.6175715327262878\n",
      "Average Episode Reward: 339.191\n",
      "i: 136000, Loss: 0.6175633668899536\n",
      "i: 136200, Loss: 0.6175550818443298\n",
      "i: 136400, Loss: 0.6175479292869568\n",
      "i: 136600, Loss: 0.6175394058227539\n",
      "i: 136800, Loss: 0.6175333857536316\n",
      "Average Episode Reward: 418.144\n",
      "i: 137000, Loss: 0.6175255179405212\n",
      "i: 137200, Loss: 0.6175181865692139\n",
      "i: 137400, Loss: 0.6175099015235901\n",
      "i: 137600, Loss: 0.6175026893615723\n",
      "i: 137800, Loss: 0.6174948215484619\n",
      "Average Episode Reward: 340.027\n",
      "i: 138000, Loss: 0.6174868941307068\n",
      "i: 138200, Loss: 0.617479145526886\n",
      "i: 138400, Loss: 0.61747145652771\n",
      "i: 138600, Loss: 0.6174624562263489\n",
      "i: 138800, Loss: 0.6174541711807251\n",
      "Average Episode Reward: 419.321\n",
      "i: 139000, Loss: 0.6174466013908386\n",
      "i: 139200, Loss: 0.6174388527870178\n",
      "i: 139400, Loss: 0.6174317598342896\n",
      "i: 139600, Loss: 0.6174238920211792\n",
      "i: 139800, Loss: 0.6174162030220032\n",
      "Average Episode Reward: 343.718\n",
      "i: 140000, Loss: 0.6174083352088928\n",
      "i: 140200, Loss: 0.6174010038375854\n",
      "i: 140400, Loss: 0.6173931360244751\n",
      "i: 140600, Loss: 0.6173855662345886\n",
      "i: 140800, Loss: 0.6173789501190186\n",
      "Average Episode Reward: 411.047\n",
      "i: 141000, Loss: 0.6173714399337769\n",
      "i: 141200, Loss: 0.6173633337020874\n",
      "i: 141400, Loss: 0.6173563599586487\n",
      "i: 141600, Loss: 0.6173498034477234\n",
      "i: 141800, Loss: 0.617341160774231\n",
      "Average Episode Reward: 334.752\n",
      "i: 142000, Loss: 0.6173346638679504\n",
      "i: 142200, Loss: 0.6173273921012878\n",
      "i: 142400, Loss: 0.6173206567764282\n",
      "i: 142600, Loss: 0.6173132061958313\n",
      "i: 142800, Loss: 0.6173062324523926\n",
      "Average Episode Reward: 406.77\n",
      "i: 143000, Loss: 0.6172984838485718\n",
      "i: 143200, Loss: 0.6172910332679749\n",
      "i: 143400, Loss: 0.617283284664154\n",
      "i: 143600, Loss: 0.6172760128974915\n",
      "i: 143800, Loss: 0.6172690391540527\n",
      "Average Episode Reward: 354.259\n",
      "i: 144000, Loss: 0.6172617673873901\n",
      "i: 144200, Loss: 0.6172552704811096\n",
      "i: 144400, Loss: 0.6172472238540649\n",
      "i: 144600, Loss: 0.6172392964363098\n",
      "i: 144800, Loss: 0.6172314882278442\n",
      "Average Episode Reward: 277.519\n",
      "i: 145000, Loss: 0.6172242760658264\n",
      "i: 145200, Loss: 0.6172180771827698\n",
      "i: 145400, Loss: 0.617211639881134\n",
      "i: 145600, Loss: 0.6172045469284058\n",
      "i: 145800, Loss: 0.6171979904174805\n",
      "Average Episode Reward: 375.992\n",
      "i: 146000, Loss: 0.6171905398368835\n",
      "i: 146200, Loss: 0.6171833276748657\n",
      "i: 146400, Loss: 0.6171752214431763\n",
      "i: 146600, Loss: 0.6171671152114868\n",
      "i: 146800, Loss: 0.6171598434448242\n",
      "Average Episode Reward: 396.041\n",
      "i: 147000, Loss: 0.6171522736549377\n",
      "i: 147200, Loss: 0.617144763469696\n",
      "i: 147400, Loss: 0.6171365976333618\n",
      "i: 147600, Loss: 0.6171281337738037\n",
      "i: 147800, Loss: 0.6171208620071411\n",
      "Average Episode Reward: 410.079\n",
      "i: 148000, Loss: 0.617114245891571\n",
      "i: 148200, Loss: 0.6171066164970398\n",
      "i: 148400, Loss: 0.6170986890792847\n",
      "i: 148600, Loss: 0.6170917749404907\n",
      "i: 148800, Loss: 0.6170850396156311\n",
      "Average Episode Reward: 393.134\n",
      "i: 149000, Loss: 0.6170777082443237\n",
      "i: 149200, Loss: 0.6170705556869507\n",
      "i: 149400, Loss: 0.6170631647109985\n",
      "i: 149600, Loss: 0.6170555353164673\n",
      "i: 149800, Loss: 0.6170481443405151\n",
      "Average Episode Reward: 348.89\n",
      "i: 150000, Loss: 0.6170409321784973\n",
      "i: 150200, Loss: 0.6170332431793213\n",
      "i: 150400, Loss: 0.6170263290405273\n",
      "i: 150600, Loss: 0.6170191764831543\n",
      "i: 150800, Loss: 0.6170114278793335\n",
      "Average Episode Reward: 467.961\n",
      "i: 151000, Loss: 0.6170040965080261\n",
      "i: 151200, Loss: 0.616996169090271\n",
      "i: 151400, Loss: 0.6169899106025696\n",
      "i: 151600, Loss: 0.6169826984405518\n",
      "i: 151800, Loss: 0.6169754266738892\n",
      "Average Episode Reward: 426.751\n",
      "i: 152000, Loss: 0.6169683337211609\n",
      "i: 152200, Loss: 0.6169605255126953\n",
      "i: 152400, Loss: 0.6169544458389282\n",
      "i: 152600, Loss: 0.6169479489326477\n",
      "i: 152800, Loss: 0.616940975189209\n",
      "Average Episode Reward: 351.309\n",
      "i: 153000, Loss: 0.6169352531433105\n",
      "i: 153200, Loss: 0.6169276237487793\n",
      "i: 153400, Loss: 0.6169208288192749\n",
      "i: 153600, Loss: 0.616913914680481\n",
      "i: 153800, Loss: 0.6169074177742004\n",
      "Average Episode Reward: 390.857\n",
      "i: 154000, Loss: 0.6169009804725647\n",
      "i: 154200, Loss: 0.6168937683105469\n",
      "i: 154400, Loss: 0.6168866753578186\n",
      "i: 154600, Loss: 0.6168802380561829\n",
      "i: 154800, Loss: 0.6168733239173889\n",
      "Average Episode Reward: 386.324\n",
      "i: 155000, Loss: 0.6168659925460815\n",
      "i: 155200, Loss: 0.6168590188026428\n",
      "i: 155400, Loss: 0.6168529987335205\n",
      "i: 155600, Loss: 0.6168462038040161\n",
      "i: 155800, Loss: 0.6168394684791565\n",
      "Average Episode Reward: 373.111\n",
      "i: 156000, Loss: 0.6168328523635864\n",
      "i: 156200, Loss: 0.6168256998062134\n",
      "i: 156400, Loss: 0.6168200969696045\n",
      "i: 156600, Loss: 0.616814136505127\n",
      "i: 156800, Loss: 0.6168068647384644\n",
      "Average Episode Reward: 351.522\n",
      "i: 157000, Loss: 0.6168004274368286\n",
      "i: 157200, Loss: 0.6167927980422974\n",
      "i: 157400, Loss: 0.6167864203453064\n",
      "i: 157600, Loss: 0.6167788505554199\n",
      "i: 157800, Loss: 0.6167720556259155\n",
      "Average Episode Reward: 296.977\n",
      "i: 158000, Loss: 0.6167652606964111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 158200, Loss: 0.6167581677436829\n",
      "i: 158400, Loss: 0.6167512536048889\n",
      "i: 158600, Loss: 0.6167430877685547\n",
      "i: 158800, Loss: 0.6167362928390503\n",
      "Average Episode Reward: 198.089\n",
      "i: 159000, Loss: 0.6167297959327698\n",
      "i: 159200, Loss: 0.6167231202125549\n",
      "i: 159400, Loss: 0.6167165637016296\n",
      "i: 159600, Loss: 0.6167100667953491\n",
      "i: 159800, Loss: 0.6167024374008179\n",
      "Average Episode Reward: 400.372\n",
      "i: 160000, Loss: 0.6166959404945374\n",
      "i: 160200, Loss: 0.6166881918907166\n",
      "i: 160400, Loss: 0.6166815757751465\n",
      "i: 160600, Loss: 0.6166753172874451\n",
      "i: 160800, Loss: 0.6166682839393616\n",
      "Average Episode Reward: 341.861\n",
      "i: 161000, Loss: 0.6166609525680542\n",
      "i: 161200, Loss: 0.616655170917511\n",
      "i: 161400, Loss: 0.6166481375694275\n",
      "i: 161600, Loss: 0.616642415523529\n",
      "i: 161800, Loss: 0.6166349649429321\n",
      "Average Episode Reward: 427.633\n",
      "i: 162000, Loss: 0.6166291832923889\n",
      "i: 162200, Loss: 0.6166225671768188\n",
      "i: 162400, Loss: 0.6166152954101562\n",
      "i: 162600, Loss: 0.6166080236434937\n",
      "i: 162800, Loss: 0.616601824760437\n",
      "Average Episode Reward: 236.722\n",
      "i: 163000, Loss: 0.6165955662727356\n",
      "i: 163200, Loss: 0.6165898442268372\n",
      "i: 163400, Loss: 0.6165834069252014\n",
      "i: 163600, Loss: 0.6165763735771179\n",
      "i: 163800, Loss: 0.6165693998336792\n",
      "Average Episode Reward: 321.141\n",
      "i: 164000, Loss: 0.6165624260902405\n",
      "i: 164200, Loss: 0.6165562868118286\n",
      "i: 164400, Loss: 0.6165503263473511\n",
      "i: 164600, Loss: 0.6165441274642944\n",
      "i: 164800, Loss: 0.61653733253479\n",
      "Average Episode Reward: 398.361\n",
      "i: 165000, Loss: 0.616531252861023\n",
      "i: 165200, Loss: 0.6165245771408081\n",
      "i: 165400, Loss: 0.6165183186531067\n",
      "i: 165600, Loss: 0.6165114045143127\n",
      "i: 165800, Loss: 0.6165046095848083\n",
      "Average Episode Reward: 224.625\n",
      "i: 166000, Loss: 0.6164987683296204\n",
      "i: 166200, Loss: 0.6164928078651428\n",
      "i: 166400, Loss: 0.6164867877960205\n",
      "i: 166600, Loss: 0.6164805293083191\n",
      "i: 166800, Loss: 0.6164743900299072\n",
      "Average Episode Reward: 425.204\n",
      "i: 167000, Loss: 0.6164676547050476\n",
      "i: 167200, Loss: 0.6164616942405701\n",
      "i: 167400, Loss: 0.6164553761482239\n",
      "i: 167600, Loss: 0.6164495944976807\n",
      "i: 167800, Loss: 0.6164440512657166\n",
      "Average Episode Reward: 362.682\n",
      "i: 168000, Loss: 0.6164383888244629\n",
      "i: 168200, Loss: 0.6164314150810242\n",
      "i: 168400, Loss: 0.6164258718490601\n",
      "i: 168600, Loss: 0.6164199709892273\n",
      "i: 168800, Loss: 0.6164141893386841\n",
      "Average Episode Reward: 386.606\n",
      "i: 169000, Loss: 0.6164076924324036\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-1c1464591f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegments_to_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6d185aca0f97>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mtrajectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def to_training(s, dr, dh):\n",
    "    l = s.tolist()\n",
    "    l.append(dr)\n",
    "    l.append(dh)\n",
    "    return l\n",
    "\n",
    "def segments_to_training(segments):\n",
    "    x = []\n",
    "    y = []\n",
    "    for (s, dr, dh), action in segments:\n",
    "        l = to_training(s, dr, dh)\n",
    "        x.append(l)\n",
    "        y.append(action)\n",
    "        \n",
    "    x = torch.tensor(x).float().to(device)\n",
    "    y = torch.tensor(y).float().to(device)\n",
    "    \n",
    "    return x, y\n",
    "        \n",
    "# accuracy_m = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    #print(predictions, targets)\n",
    "    \n",
    "    loss = loss_object(predictions, targets.unsqueeze(1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def generate_episode(cmd):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    \n",
    "    t = Trajectory()\n",
    "    while not done:\n",
    "        (dh, dr) = cmd\n",
    "        inputs = torch.tensor([to_training(s, dr, dh)]).float().to(device)\n",
    "        \n",
    "        action_probs = model(inputs)\n",
    "        action_probs = F.sigmoid(action_probs) #, dim=-1)\n",
    "        \n",
    "        m = torch.distributions.bernoulli.Bernoulli(probs=action_probs) #categorical.Categorical(probs=action_probs)\n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "        \n",
    "        # env.render()\n",
    "        s_old = s\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        t.add(s_old, action, reward, s)\n",
    "        \n",
    "        ep_reward += reward\n",
    "        dh = dh - 1\n",
    "        dr = dr - reward\n",
    "        cmd = (dh, dr)\n",
    "    \n",
    "    # print(f'Episode reward: {ep_reward}')\n",
    "    rb.add(t)\n",
    "    return ep_reward\n",
    "    \n",
    "    \n",
    "epochs = 1000000\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    segments = rb.sample(batch_size)\n",
    "    segments = np.array(segments)\n",
    "    x, y = segments_to_training(segments)\n",
    "    loss = train_step(x, y)\n",
    "    loss_sum += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    #if i % 1000 == 0:\n",
    "    lr_scheduler.step()\n",
    "    #print(lr_scheduler.get_lr())\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        rewards = [] \n",
    "        for e in range(1000):\n",
    "            cmd = rb.sample_command()\n",
    "            rewards.append(generate_episode(cmd))\n",
    "        \n",
    "        print(f\"Average Episode Reward: {np.mean(rewards)}\")\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(f'i: {i}, Loss: {loss_sum/loss_count}') #'\\t Accuracy: {accuracy_m.result()}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 500.0)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A1:6 Generate episodes using Alg 2 and add to replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (500, 500) #rb.sample_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 372.187\n"
     ]
    }
   ],
   "source": [
    "avg_rewards = []\n",
    "\n",
    "def test(cmd):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        (dh, dr) = cmd\n",
    "        inputs = torch.tensor([to_training(s, dr, dh)]).float().to(device)\n",
    "        \n",
    "        action_probs = model(inputs)\n",
    "        action_probs = F.sigmoid(action_probs) #, dim=-1)\n",
    "        \n",
    "        m = torch.distributions.bernoulli.Bernoulli(probs=action_probs) #torch.distributions.categorical.Categorical(probs=action_probs)\n",
    "        action = int(m.sample().squeeze().cpu().numpy())\n",
    "        \n",
    "        #env.render()\n",
    "        s_old = s\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        \n",
    "        ep_reward += reward\n",
    "        dh = dh - 1\n",
    "        dr = dr - reward\n",
    "        cmd = (dh, dr)\n",
    "    \n",
    "    # print(f'Episode reward: {ep_reward}')\n",
    "    return ep_reward\n",
    "\n",
    "rewards = [] \n",
    "for e in range(1000):\n",
    "    rewards.append(test(cmd))\n",
    "\n",
    "# env.close()\n",
    "print(f\"Average Episode Reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
